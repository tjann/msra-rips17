CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/08/02 07:32:38

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/08/02 07:32:38

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/08/02 07:32:38

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/08/02 07:32:38

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
--------------------------------------------------------------------------
[[41682,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: Multiversohk

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (2) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (0) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (3) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (1) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
[Multiversohk:19361] 3 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[Multiversohk:19361] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].

Model has 209 nodes. Using GPU 2.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.


Model has 209 nodes. Using GPU 1.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.


Model has 209 nodes. Using GPU 3.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.


Model has 209 nodes. Using GPU 0.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.

Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.08-seconds latency this time; accumulated time on sync point = 2.08 seconds , average latency = 2.08 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 8.07-seconds latency this time; accumulated time on sync point = 8.07 seconds , average latency = 8.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 8.41-seconds latency this time; accumulated time on sync point = 8.41 seconds , average latency = 8.41 seconds
Finished Epoch[ 1 of 160]: [Training] ce = 1.99699621 * 50000; errs = 74.340% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=24.1259s
Finished Epoch[ 1 of 160]: [Training] ce = 1.99699621 * 50000; errs = 74.340% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=24.1259s
Finished Epoch[ 1 of 160]: [Training] Finished Epoch[ 1 of 160]: [Training] ce = 1.99699621 * 50000; errs = 74.340% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=24.1259s
ce = 1.99699621 * 50000; errs = 74.340% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=24.1259s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.54-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.49-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 1.49 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 2 of 160]: [Training] ce = 1.75326922 * 50000; errs = 66.694% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=9.25126s
Finished Epoch[ 2 of 160]: [Training] ce = 1.75326922 * 50000; errs = 66.694% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=9.25084s
Finished Epoch[ 2 of 160]: [Training] ce = 1.75326922 * 50000; errs = 66.694% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=9.25084s
Finished Epoch[ 2 of 160]: [Training] ce = 1.75326922 * 50000; errs = 66.694% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=9.25102s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.24-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.24 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
Finished Epoch[ 3 of 160]: [Training] ce = 1.70259836 * 50000; errs = Finished Epoch[ 3 of 160]: [Training] Finished Epoch[ 3 of 160]: [Training] ce = 1.70259836 * 50000; errs = 63.252% * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=9.46806s
Finished Epoch[ 3 of 160]: [Training] ce = 1.70259836 * 50000; errs = 63.252%63.252% * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=9.46806s
 * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=9.46808s
ce = 1.70259836 * 50000; errs = 63.252% * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=9.46807s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.48-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 1.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 4 of 160]: [Training] Finished Epoch[ 4 of 160]: [Training] ce = 1.54441082 * 50000; errs = 56.710% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.49669s
Finished Epoch[ 4 of 160]: [Training] ce = 1.54441082 * 50000; errs = 56.710% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.49669s
Finished Epoch[ 4 of 160]: [Training] ce = 1.54441082 * 50000; errs = 56.710% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.49694s
ce = 1.54441082 * 50000; errs = 56.710% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.49668s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.05-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 1.05 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.18-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.18 seconds
Finished Epoch[ 5 of 160]: [Training] ce = 1.44355887 * 50000; errs = 52.626% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=9.3975s
Finished Epoch[ 5 of 160]: [Training] Finished Epoch[ 5 of 160]: [Training] ce = 1.44355887 * 50000; errs = 52.626% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=9.39748s
Finished Epoch[ 5 of 160]: [Training] ce = 1.44355887 * 50000; errs = 52.626% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=9.39749s
ce = 1.44355887 * 50000; errs = 52.626% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=9.39749s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.22-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 1.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.40-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 1.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
Finished Epoch[ 6 of 160]: [Training] Finished Epoch[ 6 of 160]: [Training] ce = 1.38606730 * 50000; errs = 50.342% * 50000; totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=8.83506s
Finished Epoch[ 6 of 160]: [Training] ce = 1.38606730 * 50000; errs = 50.342% * 50000; totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=8.83507s
Finished Epoch[ 6 of 160]: [Training] ce = 1.38606730 * 50000; errs = 50.342% * 50000; totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=8.83505s
ce = 1.38606730 * 50000; errs = 50.342% * 50000; totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=8.83479s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.02-seconds latency this time; accumulated time on sync point = 3.02 seconds , average latency = 3.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.74-seconds latency this time; accumulated time on sync point = 1.74 seconds , average latency = 1.74 seconds
Finished Epoch[ 7 of 160]: [Training] Finished Epoch[ 7 of 160]: [Training] ce = 1.27669586 * 50000; errs = 46.566% * 50000; totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=10.4848s
Finished Epoch[ 7 of 160]: [Training] ce = 1.27669586 * 50000; errs = 46.566% * 50000; totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=10.4848s
Finished Epoch[ 7 of 160]: [Training] ce = 1.27669586 * 50000; errs = 46.566% * 50000; totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=10.4847s
ce = 1.27669586 * 50000; errs = 46.566% * 50000; totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=10.4847s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 8 of 160]: [Training] ce = 1.21203510 * 50000; errs = 44.024% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.54925s
Finished Epoch[ 8 of 160]: [Training] Finished Epoch[ 8 of 160]: [Training] ce = 1.21203510 * 50000; errs = 44.024% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.54919s
Finished Epoch[ 8 of 160]: [Training] ce = 1.21203510 * 50000; errs = 44.024% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.54924s
ce = 1.21203510 * 50000; errs = 44.024% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.54232s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.55-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 1.55 seconds
Finished Epoch[ 9 of 160]: [Training] ce = 1.15297678 * 50000; errs = 41.586% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.18526s
Finished Epoch[ 9 of 160]: [Training] ce = 1.15297678 * 50000; errs = 41.586% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.18526s
Finished Epoch[ 9 of 160]: [Training] ce = 1.15297678 * 50000; errs = 41.586% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.18524s
Finished Epoch[ 9 of 160]: [Training] ce = 1.15297678 * 50000; errs = 41.586% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.18524s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.13-seconds latency this time; accumulated time on sync point = 2.13 seconds , average latency = 2.13 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.57-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 1.57 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
Finished Epoch[10 of 160]: [Training] ce = 1.08496885 * 50000; errs = 39.156% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=9.81081s
Finished Epoch[10 of 160]: [Training] ce = 1.08496885 * 50000; errs = 39.156% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=9.7887s
Finished Epoch[10 of 160]: [Training] ce = 1.08496885 * 50000; errs = 39.156% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=9.8108s
Finished Epoch[10 of 160]: [Training] ce = 1.08496885 * 50000; errs = 39.156% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=9.8108s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.60-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 1.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.09 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[11 of 160]: [Training] ce = 1.04734260 * 50000; errs = 37.732% * 50000; Finished Epoch[11 of 160]: [Training] ce = 1.04734260 * 50000; errs = 37.732% * 50000; totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.58251s
Finished Epoch[11 of 160]: [Training] ce = 1.04734260 * 50000; errs = 37.732% * 50000; totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.5825s
Finished Epoch[11 of 160]: [Training] totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.58251s
ce = 1.04734260 * 50000; errs = 37.732% * 50000; totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.5825s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.47-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 1.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.88-seconds latency this time; accumulated time on sync point = 1.88 seconds , average latency = 1.88 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.50-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.50 seconds
Finished Epoch[12 of 160]: [Training] ce = 0.99096287 * 50000; errs = 35.198%Finished Epoch[12 of 160]: [Training] ce = 0.99096287 * 50000; errs = 35.198% * 50000; Finished Epoch[12 of 160]: [Training] Finished Epoch[12 of 160]: [Training] ce = 0.99096287 * 50000; errs = 35.198% * 50000; totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.43751s
 * 50000; totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.43751s
ce = 0.99096287 * 50000; totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.43751s
errs = 35.198% * 50000; totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.43751s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.06-seconds latency this time; accumulated time on sync point = 2.06 seconds , average latency = 2.06 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.53-seconds latency this time; accumulated time on sync point = 0.53 seconds , average latency = 0.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.03-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 2.03 seconds
Finished Epoch[13 of 160]: [Training] Finished Epoch[13 of 160]: [Training] ce = 0.93085699 * 50000; errs = 33.128% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.80296s
Finished Epoch[13 of 160]: [Training] ce = 0.93085699 * 50000; errs = 33.128% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.80999s
Finished Epoch[13 of 160]: [Training] ce = 0.93085699 * 50000; errs = 33.128% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.81002s
ce = 0.93085699 * 50000; errs = 33.128% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.80999s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.58-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.67-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 1.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.27-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 1.27 seconds
Finished Epoch[14 of 160]: [Training] ce = 0.89541652 * 50000; errs = 31.722% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.29772s
Finished Epoch[14 of 160]: [Training] Finished Epoch[14 of 160]: [Training] ce = 0.89541652 * 50000; errs = 31.722% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.29772s
Finished Epoch[14 of 160]: [Training] ce = 0.89541652 * 50000; errs = 31.722% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.2973s
ce = 0.89541652 * 50000; errs = 31.722% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.29729s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.44-seconds latency this time; accumulated time on sync point = 1.44 seconds , average latency = 1.44 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.75-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.57-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 1.57 seconds
Finished Epoch[15 of 160]: [Training] ce = 0.85213383 * 50000; errs = 30.104% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.20912s
Finished Epoch[15 of 160]: [Training] Finished Epoch[15 of 160]: [Training] ce = 0.85213383 * 50000; errs = 30.104% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.20914s
Finished Epoch[15 of 160]: [Training] ce = ce = 0.85213383 * 50000; errs = 30.104% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.2092s
0.85213383 * 50000; errs = 30.104% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.20913s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.37-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 1.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
Finished Epoch[16 of 160]: [Training] Finished Epoch[16 of 160]: [Training] ce = 0.82189689 * 50000; errs = 28.990%Finished Epoch[16 of 160]: [Training] ce = 0.82189689 * 50000; errs = 28.990% * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=9.72965s
Finished Epoch[16 of 160]: [Training] ce = 0.82189689 * 50000; errs = 28.990% * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=9.72966s
 * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=9.72967s
ce = 0.82189689 * 50000; errs = 28.990% * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=9.72965s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.16-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 1.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.17 seconds
Finished Epoch[17 of 160]: [Training] ce = 0.78666979 * 50000Finished Epoch[17 of 160]: [Training] ce = 0.78666979 * 50000; errs = Finished Epoch[17 of 160]: [Training] ce = 0.78666979 * 50000; errs = 27.398%Finished Epoch[17 of 160]: [Training] ; errs = 27.398% * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=8.93034s
27.398% * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=8.93034s
 * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=8.93034s
ce = 0.78666979 * 50000; errs = 27.398% * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=8.93033s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.95-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.95 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.20-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 1.20 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.22-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.22 seconds
Finished Epoch[18 of 160]: [Training] ce = 0.76454955 * 50000; errs = 26.670% * 50000; totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=8.88469s
Finished Epoch[18 of 160]: [Training] Finished Epoch[18 of 160]: [Training] ce = 0.76454955 * 50000; errs = 26.670% * 50000; totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=8.88469s
Finished Epoch[18 of 160]: [Training] ce = 0.76454955 * 50000; errs = 26.670% * 50000; totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=8.88496s
ce = 0.76454955 * 50000; errs = 26.670% * 50000; totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=8.88494s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.48-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 1.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.24-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.24 seconds
Finished Epoch[19 of 160]: [Training] Finished Epoch[19 of 160]: [Training] ce = 0.71875115 * 50000; errs = 24.970% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=9.39132s
Finished Epoch[19 of 160]: [Training] ce = 0.71875115 * 50000; errs = 24.970% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=9.39134s
Finished Epoch[19 of 160]: [Training] ce = 0.71875115 * 50000; errs = 24.970% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=9.39132s
ce = 0.71875115 * 50000; errs = 24.970% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=9.39133s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.18-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 1.18 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.34-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.34 seconds
Finished Epoch[20 of 160]: [Training] ce = 0.70284658 * 50000; errs = 24.372% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.2724s
Finished Epoch[20 of 160]: [Training] Finished Epoch[20 of 160]: [Training] ce = 0.70284658 * 50000; errs = 24.372% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.2724s
Finished Epoch[20 of 160]: [Training] ce = 0.70284658 * 50000; errs = 24.372% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.2724s
ce = 0.70284658 * 50000; errs = 24.372% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.26363s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.80-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 1.80 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.36-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.36 seconds
Finished Epoch[21 of 160]: [Training] ce = 0.67073215 * 50000; errs = 23.340% * 50000; Finished Epoch[21 of 160]: [Training] Finished Epoch[21 of 160]: [Training] ce = 0.67073215 * 50000; errs = 23.340% * 50000; totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.56727s
totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.56726s
Finished Epoch[21 of 160]: [Training] ce = 0.67073215 * 50000; errs = 23.340% * 50000; totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.56731s
ce = 0.67073215 * 50000; errs = 23.340% * 50000; totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.5673s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.82-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 1.82 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.81-seconds latency this time; accumulated time on sync point = 1.81 seconds , average latency = 1.81 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.03-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 1.03 seconds
Finished Epoch[22 of 160]: [Training] ce = 0.66246768 * 50000; errs = 22.908% * 50000; totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.57196s
Finished Epoch[22 of 160]: [Training] ce = 0.66246768 * 50000; errs = 22.908% * 50000; totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.57197s
Finished Epoch[22 of 160]: [Training] ce = 0.66246768 * 50000; errs = 22.908% * 50000; totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.57195s
Finished Epoch[22 of 160]: [Training] ce = 0.66246768 * 50000; errs = 22.908% * 50000; totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.57196s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.36-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 1.36 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.86-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.86 seconds
Finished Epoch[23 of 160]: [Training] ce = 0.63656068 * 50000; errs = 21.946% * 50000; totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=8.8937s
Finished Epoch[23 of 160]: [Training] ce = 0.63656068 * 50000; errs = 21.946% * 50000; totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=8.8937s
Finished Epoch[23 of 160]: [Training] ce = 0.63656068 * 50000; errs = 21.946% * 50000; totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=8.89376s
Finished Epoch[23 of 160]: [Training] ce = 0.63656068 * 50000; errs = 21.946% * 50000; totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=8.89374s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.17-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 1.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.33-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.33 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[24 of 160]: [Training] ce = 0.63306218 * 50000; errs = 21.920% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.05258s
Finished Epoch[24 of 160]: [Training] ce = 0.63306218 * 50000; errs = 21.920% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.05259s
Finished Epoch[24 of 160]: [Training] ce = 0.63306218 * 50000; errs = 21.920% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.05259s
Finished Epoch[24 of 160]: [Training] ce = 0.63306218 * 50000; errs = 21.920% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.05257s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.73-seconds latency this time; accumulated time on sync point = 1.73 seconds , average latency = 1.73 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.58-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.89-seconds latency this time; accumulated time on sync point = 1.89 seconds , average latency = 1.89 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[25 of 160]: [Training] ce = 0.60300558 * 50000; errs = 20.856% * 50000; Finished Epoch[25 of 160]: [Training] ce = 0.60300558 * 50000; errs = 20.856% * 50000; totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.42764s
Finished Epoch[25 of 160]: [Training] totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.42764s
Finished Epoch[25 of 160]: [Training] ce = 0.60300558 * 50000; errs = 20.856% * 50000; totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.42807s
ce = 0.60300558 * 50000; errs = 20.856% * 50000; totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.4278s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.83-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 1.83 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.62-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 1.62 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.26-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.26 seconds
Finished Epoch[26 of 160]: [Training] ce = 0.58962290Finished Epoch[26 of 160]: [Training] ce = 0.58962290 * 50000; errs = 20.314% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.44325s
Finished Epoch[26 of 160]: [Training] Finished Epoch[26 of 160]: [Training] ce = 0.58962290 * 50000; errs = 20.314% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.44322s
 * 50000; errs = 20.314% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.44322s
ce = 0.58962290 * 50000; errs = 20.314% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.44325s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.52-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.83-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 1.83 seconds
Finished Epoch[27 of 160]: [Training] ce = 0.57878017 * 50000; errs = Finished Epoch[27 of 160]: [Training] Finished Epoch[27 of 160]: [Training] ce = Finished Epoch[27 of 160]: [Training] ce = 0.57878017 * 50000; errs = 19.992% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.58626s
19.992% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.58625s
0.57878017 * 50000; errs = 19.992% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.58627s
ce = 0.57878017 * 50000; errs = 19.992% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.58625s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.80-seconds latency this time; accumulated time on sync point = 0.80 seconds , average latency = 0.80 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.06-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 1.06 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.17 seconds
Finished Epoch[28 of 160]: [Training] ce = 0.56553287 * 50000; errs = 19.458% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=8.84309s
Finished Epoch[28 of 160]: [Training] ce = 0.56553287 * 50000; errs = 19.458% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=8.84309s
Finished Epoch[28 of 160]: [Training] Finished Epoch[28 of 160]: [Training] ce = 0.56553287 * 50000; errs = 19.458% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=8.84309s
ce = 0.56553287 * 50000; errs = 19.458% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=8.84309s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.37-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.02-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 1.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.95-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.95 seconds
Finished Epoch[29 of 160]: [Training] Finished Epoch[29 of 160]: [Training] ce = 0.56330145 * 50000; errs = Finished Epoch[29 of 160]: [Training] ce = 0.56330145 * 50000; errs = Finished Epoch[29 of 160]: [Training] ce = 0.56330145 * 50000; errs = 19.308% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=8.97152s
19.308% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=8.97152s
19.308% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=8.97153s
ce = 0.56330145 * 50000; errs = 19.308% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=8.97151s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.46-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 1.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
Finished Epoch[30 of 160]: [Training] ce = 0.54458107 * 50000; errs = 18.874% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=9.3117s
Finished Epoch[30 of 160]: [Training] ce = 0.54458107 * 50000; errs = 18.874% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=9.31169s
Finished Epoch[30 of 160]: [Training] ce = 0.54458107 * 50000; errs = 18.874% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=9.31169s
Finished Epoch[30 of 160]: [Training] ce = 0.54458107 * 50000; errs = 18.874% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=9.3117s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.31-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 1.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
Finished Epoch[31 of 160]: [Training] ce = 0.52883147 * 50000; errs = 18.506% * 50000; totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=9.47422s
Finished Epoch[31 of 160]: [Training] ce = 0.52883147 * 50000; errs = 18.506% * 50000; totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=9.47423s
Finished Epoch[31 of 160]: [Training] ce = 0.52883147 * 50000; errs = 18.506% * 50000; totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=9.47423s
Finished Epoch[31 of 160]: [Training] ce = 0.52883147 * 50000; errs = 18.506% * 50000; totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=9.47448s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.14 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.67-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 1.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.84-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 1.84 seconds
Finished Epoch[32 of 160]: [Training] ce = 0.52299903 * 50000; errs = 18.072% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=9.33966s
Finished Epoch[32 of 160]: [Training] Finished Epoch[32 of 160]: [Training] ce = 0.52299903 * 50000; errs = 18.072% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=9.33964s
Finished Epoch[32 of 160]: [Training] ce = 0.52299903 * 50000; errs = 18.072% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=9.33967s
ce = 0.52299903 * 50000; errs = 18.072% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=9.33964s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.27-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.31-seconds latency this time; accumulated time on sync point = 2.31 seconds , average latency = 2.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.05-seconds latency this time; accumulated time on sync point = 2.05 seconds , average latency = 2.05 seconds
Finished Epoch[33 of 160]: [Training] ce = 0.51851030 * 50000; errs = 17.872% * 50000; totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=10.3602s
Finished Epoch[33 of 160]: [Training] ce = 0.51851030 * 50000; errs = 17.872% * 50000; totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=10.3602s
Finished Epoch[33 of 160]: [Training] ce = 0.51851030 * 50000; errs = 17.872% * 50000; totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=10.3602s
Finished Epoch[33 of 160]: [Training] ce = 0.51851030 * 50000; errs = 17.872% * 50000; totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=10.3602s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.12-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 1.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.14 seconds
Finished Epoch[34 of 160]: [Training] Finished Epoch[34 of 160]: [Training] ce = 0.51321296 * 50000; errs = 17.776% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.09014s
Finished Epoch[34 of 160]: [Training] ce = 0.51321296 * 50000; errs = 17.776% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.09014s
Finished Epoch[34 of 160]: [Training] ce = 0.51321296 * 50000; errs = 17.776% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.09015s
ce = 0.51321296 * 50000; errs = 17.776% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.09014s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.59-seconds latency this time; accumulated time on sync point = 0.59 seconds , average latency = 0.59 seconds
Finished Epoch[35 of 160]: [Training] Finished Epoch[35 of 160]: [Training] ce = 0.51566883 * 50000; errs = 17.696% * 50000; totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.27786s
Finished Epoch[35 of 160]: [Training] ce = 0.51566883 * 50000; errs = 17.696% * 50000; Finished Epoch[35 of 160]: [Training] ce = 0.51566883 * 50000; errs = 17.696% * 50000; totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.27781s
totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.27781s
ce = 0.51566883 * 50000; errs = 17.696% * 50000; totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.27786s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.28-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.28 seconds
Finished Epoch[36 of 160]: [Training] ce = 0.49127354 * 50000; errs = 16.592% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=9.40935s
Finished Epoch[36 of 160]: [Training] ce = 0.49127354 * 50000; errs = 16.592% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=9.40934s
Finished Epoch[36 of 160]: [Training] ce = 0.49127354 * 50000; errs = 16.592% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=9.40936s
Finished Epoch[36 of 160]: [Training] ce = 0.49127354 * 50000; errs = 16.592% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=9.40936s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.11-seconds latency this time; accumulated time on sync point = 1.11 seconds , average latency = 1.11 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.14 seconds
Finished Epoch[37 of 160]: [Training] Finished Epoch[37 of 160]: [Training] ce = 0.49508406 * 50000; errs = 17.164% * 50000; totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=8.97051s
Finished Epoch[37 of 160]: [Training] ce = 0.49508406 * 50000; errs = 17.164% * 50000; totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=8.9705s
Finished Epoch[37 of 160]: [Training] ce = 0.49508406 * 50000; errs = 17.164% * 50000; totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=8.97057s
ce = 0.49508406 * 50000; errs = 17.164% * 50000; totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=8.9705s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.36-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 1.36 seconds
Finished Epoch[38 of 160]: [Training] Finished Epoch[38 of 160]: [Training] ce = 0.49208336 * 50000; errs = 17.042% * 50000; totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.20659s
Finished Epoch[38 of 160]: [Training] ce = 0.49208336 * 50000; errs = 17.042% * 50000; totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.20659s
Finished Epoch[38 of 160]: [Training] ce = 0.49208336 * 50000; errs = 17.042% * 50000; totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.20658s
ce = 0.49208336 * 50000; errs = 17.042% * 50000; totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.20658s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.48-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[39 of 160]: [Training] ce = 0.48916428 * 50000; errs = 16.838% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.20645s
Finished Epoch[39 of 160]: [Training] ce = 0.48916428 * 50000; errs = 16.838% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.20645s
Finished Epoch[39 of 160]: [Training] Finished Epoch[39 of 160]: [Training] ce = 0.48916428 * 50000; errs = 16.838% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.20808s
ce = 0.48916428 * 50000; errs = 16.838% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.20808s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.78-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 1.78 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.50-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 1.50 seconds
Finished Epoch[40 of 160]: [Training] Finished Epoch[40 of 160]: [Training] ce = 0.48618792 * 50000; errs = 16.774% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.25217s
Finished Epoch[40 of 160]: [Training] ce = 0.48618792 * 50000; errs = 16.774% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.25218s
Finished Epoch[40 of 160]: [Training] ce = 0.48618792 * 50000; errs = 16.774% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.25221s
ce = 0.48618792 * 50000; errs = 16.774% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.25218s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.89-seconds latency this time; accumulated time on sync point = 1.89 seconds , average latency = 1.89 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.18-seconds latency this time; accumulated time on sync point = 2.18 seconds , average latency = 2.18 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.09 seconds
Finished Epoch[41 of 160]: [Training] ce = 0.46797565 * 50000; errs = 16.072% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.88559s
Finished Epoch[41 of 160]: [Training] Finished Epoch[41 of 160]: [Training] ce = 0.46797565 * 50000; errs = 16.072% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.88558s
Finished Epoch[41 of 160]: [Training] ce = 0.46797565 * 50000; errs = 16.072% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.8856s
ce = 0.46797565 * 50000; errs = 16.072% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.8856s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.06-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 1.06 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.20-seconds latency this time; accumulated time on sync point = 2.20 seconds , average latency = 2.20 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.14-seconds latency this time; accumulated time on sync point = 2.14 seconds , average latency = 2.14 seconds
Finished Epoch[42 of 160]: [Training] ce = 0.47774199 * 50000; errs = 16.422% * 50000; totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.78165s
Finished Epoch[42 of 160]: [Training] ce = 0.47774199Finished Epoch[42 of 160]: [Training] ce = 0.47774199 * 50000; errs = 16.422% * 50000; totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.78189s
Finished Epoch[42 of 160]: [Training] ce = 0.47774199 * 50000; errs = 16.422% * 50000; totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.78162s
 * 50000; errs = 16.422% * 50000; totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.78164s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.85-seconds latency this time; accumulated time on sync point = 1.85 seconds , average latency = 1.85 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.82-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 1.82 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
Finished Epoch[43 of 160]: [Training] ce = 0.46355680 * 50000; errs = 15.858% * 50000; totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=9.6065s
Finished Epoch[43 of 160]: [Training] ce = 0.46355680 * 50000; errs = 15.858% * 50000; totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=9.60651s
Finished Epoch[43 of 160]: [Training] ce = 0.46355680 * 50000; errs = 15.858% * 50000; totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=9.60651s
Finished Epoch[43 of 160]: [Training] ce = 0.46355680 * 50000; errs = 15.858% * 50000; totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=9.60652s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.76-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.76 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.00-seconds latency this time; accumulated time on sync point = 2.00 seconds , average latency = 2.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.95-seconds latency this time; accumulated time on sync point = 1.95 seconds , average latency = 1.95 seconds
Finished Epoch[44 of 160]: [Training] Finished Epoch[44 of 160]: [Training] ce = 0.45682190 * 50000; errs = 15.788% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.96785s
Finished Epoch[44 of 160]: [Training] ce = 0.45682190 * 50000; errs = 15.788% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.96785s
Finished Epoch[44 of 160]: [Training] ce = 0.45682190 * 50000; errs = 15.788% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.96785s
ce = 0.45682190 * 50000; errs = 15.788% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.96785s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.88-seconds latency this time; accumulated time on sync point = 1.88 seconds , average latency = 1.88 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.40-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 1.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.74-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.74 seconds
Finished Epoch[45 of 160]: [Training] Finished Epoch[45 of 160]: [Training] ce = 0.46163896 * 50000; errs = 15.956% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.46932s
Finished Epoch[45 of 160]: [Training] ce = 0.46163896 * 50000; errs = 15.956% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.46932s
Finished Epoch[45 of 160]: [Training] ce = 0.46163896 * 50000; errs = 15.956% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.46932s
ce = 0.46163896 * 50000; errs = 15.956% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.46774s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.35-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.13-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 1.13 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
Finished Epoch[46 of 160]: [Training] ce = Finished Epoch[46 of 160]: [Training] 0.46305649 * 50000; errs = Finished Epoch[46 of 160]: [Training] ce = 0.46305649 * 50000; errs = Finished Epoch[46 of 160]: [Training] ce = 15.882% * 50000; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=8.92674s
15.882% * 50000; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=8.92675s
0.46305649 * 50000; errs = 15.882% * 50000; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=8.92702s
ce = 0.46305649 * 50000; errs = 15.882% * 50000; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=8.92674s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.09-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 1.09 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.31-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.31 seconds
Finished Epoch[47 of 160]: [Training] ce = 0.45292148 * 50000; errs = 15.628% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=8.94121s
Finished Epoch[47 of 160]: [Training] Finished Epoch[47 of 160]: [Training] ce = 0.45292148 * 50000; errs = Finished Epoch[47 of 160]: [Training] ce = 0.45292148 * 50000; errs = 15.628% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=8.94121s
15.628% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=8.94123s
ce = 0.45292148 * 50000; errs = 15.628% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=8.94092s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.45-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 1.45 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.23-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 1.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
Finished Epoch[48 of 160]: [Training] ce = 0.44566121 * 50000; errs = 15.248% * 50000; totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.36194s
Finished Epoch[48 of 160]: [Training] ce = 0.44566121 * 50000; errs = Finished Epoch[48 of 160]: [Training] ce = 0.44566121 * 50000; errs = 15.248% * 50000; totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.36194s
15.248% * 50000; totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.36171s
Finished Epoch[48 of 160]: [Training] ce = 0.44566121 * 50000; errs = 15.248% * 50000; totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.36222s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.41-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 1.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.46-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 1.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.61-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.61 seconds
Finished Epoch[49 of 160]: [Training] ce = 0.44505764 * 50000; errs = 15.418% * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=9.32453s
Finished Epoch[49 of 160]: [Training] ce = 0.44505764 * 50000; errs = 15.418% * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=9.32455s
Finished Epoch[49 of 160]: [Training] ce = 0.44505764 * 50000; errs = 15.418% * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=9.32454s
Finished Epoch[49 of 160]: [Training] ce = 0.44505764 * 50000; errs = 15.418% * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=9.32454s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.22-seconds latency this time; accumulated time on sync point = 2.22 seconds , average latency = 2.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.02-seconds latency this time; accumulated time on sync point = 2.02 seconds , average latency = 2.02 seconds
Finished Epoch[50 of 160]: [Training] ce = 0.44226128 * 50000; errs = 15.146% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=9.97029s
Finished Epoch[50 of 160]: [Training] ce = 0.44226128 * 50000; errs = 15.146% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=9.9703s
Finished Epoch[50 of 160]: [Training] ce = 0.44226128 * 50000; errs = 15.146% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=9.97029s
Finished Epoch[50 of 160]: [Training] ce = 0.44226128 * 50000; errs = 15.146% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=9.9703s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.76-seconds latency this time; accumulated time on sync point = 1.76 seconds , average latency = 1.76 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.71-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 1.71 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.62-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.62 seconds
Finished Epoch[51 of 160]: [Training] Finished Epoch[51 of 160]: [Training] ce = 0.44689992 * 50000; errs = 15.304% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=9.3713s
Finished Epoch[51 of 160]: [Training] ce = 0.44689992 * 50000; errs = Finished Epoch[51 of 160]: [Training] ce = 0.44689992 * 50000; errs = 15.304% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=9.37132s
15.304% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=9.37132s
ce = 0.44689992 * 50000; errs = 15.304% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=9.3713s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.79-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 1.79 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.44-seconds latency this time; accumulated time on sync point = 1.44 seconds , average latency = 1.44 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
Finished Epoch[52 of 160]: [Training] ce = 0.42776828 * 50000; errs = 14.740% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.30438s
Finished Epoch[52 of 160]: [Training] ce = 0.42776828 * 50000; errs = 14.740% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.30431s
Finished Epoch[52 of 160]: [Training] ce = 0.42776828 * 50000; errs = 14.740% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.30431s
Finished Epoch[52 of 160]: [Training] ce = 0.42776828 * 50000; errs = 14.740% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.30437s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.16-seconds latency this time; accumulated time on sync point = 2.16 seconds , average latency = 2.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.81-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.81 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.26-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.26 seconds
Finished Epoch[53 of 160]: [Training] ce = 0.43371615 * 50000; errs = 14.892% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.79048s
Finished Epoch[53 of 160]: [Training] ce = 0.43371615 * 50000; errs = 14.892% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.79048s
Finished Epoch[53 of 160]: [Training] Finished Epoch[53 of 160]: [Training] ce = 0.43371615 * 50000; errs = 14.892% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.79048s
ce = 0.43371615 * 50000; errs = 14.892% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.79049s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.64-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.03-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 2.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.30-seconds latency this time; accumulated time on sync point = 2.30 seconds , average latency = 2.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[54 of 160]: [Training] ce = Finished Epoch[54 of 160]: [Training] ce = 0.43229665 * 50000; errs = 14.808% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.90117s
Finished Epoch[54 of 160]: [Training] ce = 0.43229665 * 50000; errs = 14.808% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.90116s
0.43229665 * 50000; errs = 14.808% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.90117s
Finished Epoch[54 of 160]: [Training] ce = 0.43229665 * 50000; errs = 14.808% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.90142s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.22-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.15-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 1.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
Finished Epoch[55 of 160]: [Training] ce = 0.43442840 * 50000; errs = 14.824% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.23152s
Finished Epoch[55 of 160]: [Training] Finished Epoch[55 of 160]: [Training] ce = 0.43442840 * 50000; errs = 14.824% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.23144s
ce = 0.43442840 * 50000; Finished Epoch[55 of 160]: [Training] ce = 0.43442840 * 50000; errs = 14.824% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.23143s
errs = 14.824% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.23469s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.74-seconds latency this time; accumulated time on sync point = 1.74 seconds , average latency = 1.74 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.43-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.43 seconds
Finished Epoch[56 of 160]: [Training] ce = 0.42015964 * 50000; errs = 14.496% * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.49929s
Finished Epoch[56 of 160]: [Training] Finished Epoch[56 of 160]: [Training] ce = 0.42015964 * 50000; errs = 14.496% * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.49928s
Finished Epoch[56 of 160]: [Training] ce = 0.42015964 * 50000; errs = 14.496% * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.4993s
ce = 0.42015964 * 50000; errs = 14.496% * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.48984s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.60-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.07-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 1.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.87-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.87 seconds
Finished Epoch[57 of 160]: [Training] ce = 0.43589904 * 50000; errs = 14.754% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=8.79498s
Finished Epoch[57 of 160]: [Training] ce = 0.43589904 * 50000; errs = 14.754% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=8.79493s
Finished Epoch[57 of 160]: [Training] ce = 0.43589904 * 50000; errs = 14.754% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=8.79497s
Finished Epoch[57 of 160]: [Training] ce = 0.43589904 * 50000; errs = 14.754% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=8.79493s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.47-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.20-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 1.20 seconds
Finished Epoch[58 of 160]: [Training] ce = 0.42643521 * 50000; errs = 14.626% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=9.22083s
Finished Epoch[58 of 160]: [Training] ce = 0.42643521 * 50000; errs = 14.626% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=9.22083s
Finished Epoch[58 of 160]: [Training] ce = 0.42643521 * 50000; errs = 14.626% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=9.22084s
Finished Epoch[58 of 160]: [Training] ce = 0.42643521 * 50000; errs = 14.626% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=9.22082s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.35-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.51-seconds latency this time; accumulated time on sync point = 2.51 seconds , average latency = 2.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.31-seconds latency this time; accumulated time on sync point = 2.31 seconds , average latency = 2.31 seconds
Finished Epoch[59 of 160]: [Training] Finished Epoch[59 of 160]: [Training] ce = 0.42351355 * 50000; errs = Finished Epoch[59 of 160]: [Training] ce = 0.42351355 * 50000; errs = 14.670% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=10.2305s
14.670% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=10.2305s
Finished Epoch[59 of 160]: [Training] ce = 0.42351355 * 50000; errs = 14.670% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=10.2306s
ce = 0.42351355 * 50000; errs = 14.670% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=10.2312s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.06-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 1.06 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.19 seconds
Finished Epoch[60 of 160]: [Training] ce = 0.41804946 * 50000; errs = 14.348% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=8.83403s
Finished Epoch[60 of 160]: [Training] ce = 0.41804946 * 50000; errs = 14.348% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=8.83394s
Finished Epoch[60 of 160]: [Training] ce = 0.41804946 * 50000; errs = 14.348% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=8.83393s
Finished Epoch[60 of 160]: [Training] ce = 0.41804946 * 50000; errs = 14.348% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=8.83404s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.47-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 1.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.31-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 1.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
Finished Epoch[61 of 160]: [Training] ce = 0.41947498 * 50000; errs = 14.488% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=9.27453s
Finished Epoch[61 of 160]: [Training] ce = 0.41947498 * 50000; errs = 14.488% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=9.27361s
Finished Epoch[61 of 160]: [Training] Finished Epoch[61 of 160]: [Training] ce = 0.41947498 * 50000; errs = 14.488% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=9.27361s
ce = 0.41947498 * 50000; errs = 14.488% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=9.27454s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.93-seconds latency this time; accumulated time on sync point = 1.93 seconds , average latency = 1.93 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.93-seconds latency this time; accumulated time on sync point = 1.93 seconds , average latency = 1.93 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
Finished Epoch[62 of 160]: [Training] ce = 0.42530761 * 50000; errs = 14.730% * 50000; totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.81495s
Finished Epoch[62 of 160]: [Training] ce = 0.42530761 * 50000; errs = 14.730% * 50000; totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.81496s
Finished Epoch[62 of 160]: [Training] ce = 0.42530761 * 50000; errs = 14.730% * 50000; totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.81494s
Finished Epoch[62 of 160]: [Training] ce = 0.42530761 * 50000; errs = 14.730% * 50000; totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.81368s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.94-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.94 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.31-seconds latency this time; accumulated time on sync point = 2.31 seconds , average latency = 2.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.19-seconds latency this time; accumulated time on sync point = 2.19 seconds , average latency = 2.19 seconds
Finished Epoch[63 of 160]: [Training] ce = 0.40554725 * 50000; Finished Epoch[63 of 160]: [Training] Finished Epoch[63 of 160]: [Training] ce = 0.40554725 * 50000; errs = 14.050%Finished Epoch[63 of 160]: [Training] ce = 0.40554725 * 50000; errs = 14.050% * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=9.87626s
errs = 14.050% * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=9.87627s
 * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=9.87627s
ce = 0.40554725 * 50000; errs = 14.050% * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=9.87626s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.47-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.62-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.62 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.76-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.76 seconds
Finished Epoch[64 of 160]: [Training] ce = 0.40751771 * 50000; errs = 13.982% * 50000; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=9.53146s
Finished Epoch[64 of 160]: [Training] ce = 0.40751771 * 50000; errs = 13.982% * 50000; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=9.53149s
Finished Epoch[64 of 160]: [Training] ce = 0.40751771 * 50000; errs = 13.982% * 50000; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=9.53149s
Finished Epoch[64 of 160]: [Training] ce = 0.40751771 * 50000; errs = 13.982% * 50000; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=9.53147s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.86-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 1.86 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.14 seconds
Finished Epoch[65 of 160]: [Training] Finished Epoch[65 of 160]: [Training] ce = 0.41362887 * 50000; errs = 14.196% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.14624s
Finished Epoch[65 of 160]: [Training] ce = 0.41362887 * 50000; errs = 14.196% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.14625s
Finished Epoch[65 of 160]: [Training] ce = 0.41362887 * 50000; errs = 14.196% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.14629s
ce = 0.41362887 * 50000; errs = 14.196% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.14626s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.57-seconds latency this time; accumulated time on sync point = 0.57 seconds , average latency = 0.57 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.48-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 1.48 seconds
Finished Epoch[66 of 160]: [Training] Finished Epoch[66 of 160]: [Training] ce = 0.41011107Finished Epoch[66 of 160]: [Training] ce = 0.41011107 * 50000; errs = 14.188% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.30884s
ce = 0.41011107 * 50000; errs = 14.188% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.30858s
 * 50000Finished Epoch[66 of 160]: [Training] ce = 0.41011107 * 50000; errs = 14.188% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.30895s
; errs = 14.188% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.30858s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.32-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 1.32 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.20-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.20 seconds
Finished Epoch[67 of 160]: [Training] Finished Epoch[67 of 160]: [Training] ce = 0.40071823 * 50000; errs = 13.876% * 50000; totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.35526s
Finished Epoch[67 of 160]: [Training] ce = 0.40071823 * 50000; errs = 13.876% * 50000; totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.3555s
Finished Epoch[67 of 160]: [Training] ce = 0.40071823 * 50000; errs = 13.876% * 50000; totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.35551s
ce = 0.40071823 * 50000; errs = 13.876% * 50000; totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.35525s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.36-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 1.36 seconds
Finished Epoch[68 of 160]: [Training] Finished Epoch[68 of 160]: [Training] ce = 0.40978529 * 50000; errs = 14.050% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.45796s
Finished Epoch[68 of 160]: [Training] Finished Epoch[68 of 160]: [Training] ce = 0.40978529 * 50000; errs = 14.050% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.45789s
ce = 0.40978529 * 50000; errs = 14.050% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.4579s
ce = 0.40978529 * 50000; errs = 14.050% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.45794s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.86-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.86 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.81-seconds latency this time; accumulated time on sync point = 1.81 seconds , average latency = 1.81 seconds
Finished Epoch[69 of 160]: [Training] ce = 0.40171441 * 50000; errs = 13.928% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.6326s
Finished Epoch[69 of 160]: [Training] ce = 0.40171441 * 50000; errs = 13.928% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.63202s
Finished Epoch[69 of 160]: [Training] ce = 0.40171441 * 50000; errs = 13.928% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.63202s
Finished Epoch[69 of 160]: [Training] ce = 0.40171441 * 50000; errs = 13.928% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.63296s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.43-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.02-seconds latency this time; accumulated time on sync point = 2.02 seconds , average latency = 2.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.70-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 1.70 seconds
Finished Epoch[70 of 160]: [Training] ce = 0.41104041 * 50000; errs = 14.048% * 50000; totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.86328s
Finished Epoch[70 of 160]: [Training] Finished Epoch[70 of 160]: [Training] ce = 0.41104041 * 50000; errs = 14.048% * 50000; Finished Epoch[70 of 160]: [Training] ce = 0.41104041 * 50000; errs = 14.048% * 50000; totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.8633s
totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.86329s
ce = 0.41104041 * 50000; errs = 14.048% * 50000; totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.86328s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.13-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 1.13 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.48-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 1.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
Finished Epoch[71 of 160]: [Training] ce = 0.40422468 * 50000; errs = 13.822% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.75602s
Finished Epoch[71 of 160]: [Training] Finished Epoch[71 of 160]: [Training] ce = 0.40422468 * 50000; errs = 13.822% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.75602s
Finished Epoch[71 of 160]: [Training] ce = 0.40422468 * 50000; errs = 13.822% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.75603s
ce = 0.40422468 * 50000; errs = 13.822% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.75602s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.17-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 1.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.32-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 1.32 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
Finished Epoch[72 of 160]: [Training] ce = 0.39299973 * 50000; errs = 13.572% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=9.16311s
Finished Epoch[72 of 160]: [Training] ce = 0.39299973 * 50000; errs = 13.572% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=9.1631s
Finished Epoch[72 of 160]: [Training] ce = 0.39299973 * 50000; errs = 13.572% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=9.16311s
Finished Epoch[72 of 160]: [Training] ce = 0.39299973 * 50000; errs = 13.572% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=9.16311s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.85-seconds latency this time; accumulated time on sync point = 0.85 seconds , average latency = 0.85 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.10-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 1.10 seconds
Finished Epoch[73 of 160]: [Training] ce = 0.39727130 * 50000; errs = 13.550% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.15687s
Finished Epoch[73 of 160]: [Training] ce = 0.39727130 * 50000; errs = 13.550% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.15686s
Finished Epoch[73 of 160]: [Training] ce = 0.39727130 * 50000; errs = 13.550% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.15686s
Finished Epoch[73 of 160]: [Training] ce = 0.39727130 * 50000; errs = 13.550% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.15686s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.71-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 1.71 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.14-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 1.14 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.54-seconds latency this time; accumulated time on sync point = 0.54 seconds , average latency = 0.54 seconds
Finished Epoch[74 of 160]: [Training] ce = 0.39355035 * 50000; errs = 13.538% * 50000; totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=9.11984s
Finished Epoch[74 of 160]: [Training] ce = 0.39355035 * 50000Finished Epoch[74 of 160]: [Training] ce = 0.39355035 * 50000; errs = 13.538% * 50000; totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=9.12014s
Finished Epoch[74 of 160]: [Training] ce = 0.39355035 * 50000; errs = 13.538% * 50000; totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=9.12014s
; errs = 13.538% * 50000; totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=9.11984s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.65-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 1.65 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.77-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 1.77 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.82-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.82 seconds
Finished Epoch[75 of 160]: [Training] Finished Epoch[75 of 160]: [Training] ce = 0.39464120 * 50000; errs = 13.332% * 50000; Finished Epoch[75 of 160]: [Training] ce = 0.39464120 * 50000; errs = 13.332% * 50000; totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=9.7038s
Finished Epoch[75 of 160]: [Training] ce = 0.39464120 * 50000; errs = 13.332% * 50000; totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=9.70379s
totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=9.7038s
ce = 0.39464120 * 50000; errs = 13.332% * 50000; totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=9.69942s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.35-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.78-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 1.78 seconds
Finished Epoch[76 of 160]: [Training] ce = 0.39768419 * 50000; errs = 13.634% * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=9.84404s
Finished Epoch[76 of 160]: [Training] ce = 0.39768419 * 50000; errs = 13.634% * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=9.84404s
Finished Epoch[76 of 160]: [Training] Finished Epoch[76 of 160]: [Training] ce = 0.39768419 * 50000; errs = 13.634% * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=9.84403s
ce = 0.39768419 * 50000; errs = 13.634% * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=9.84403s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.69-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 1.69 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.45-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 1.45 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
Finished Epoch[77 of 160]: [Training] ce = 0.39633979 * 50000; errs = Finished Epoch[77 of 160]: [Training] Finished Epoch[77 of 160]: [Training] ce = 0.39633979 * 50000; errs = 13.616% * 50000; totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=9.49466s
Finished Epoch[77 of 160]: [Training] ce = 0.39633979 * 50000; errs = 13.616% * 50000; totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=9.49466s
13.616% * 50000; totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=9.49467s
ce = 0.39633979 * 50000; errs = 13.616% * 50000; totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=9.49466s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.53-seconds latency this time; accumulated time on sync point = 0.53 seconds , average latency = 0.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.65-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 1.65 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.73-seconds latency this time; accumulated time on sync point = 1.73 seconds , average latency = 1.73 seconds
Finished Epoch[78 of 160]: [Training] ce = 0.38812970 * 50000; errs = 13.354% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=9.42839s
Finished Epoch[78 of 160]: [Training] Finished Epoch[78 of 160]: [Training] ce = 0.38812970 * 50000; errs = 13.354% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=9.4284s
Finished Epoch[78 of 160]: [Training] ce = 0.38812970 * 50000; errs = 13.354% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=9.42844s
ce = 0.38812970 * 50000; errs = 13.354% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=9.42844s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.29-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.22-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 1.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.28-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 1.28 seconds
Finished Epoch[79 of 160]: [Training] ce = 0.39873286 * 50000; errs = 13.852% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.33842s
Finished Epoch[79 of 160]: [Training] ce = 0.39873286 * 50000; errs = 13.852% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.33841s
Finished Epoch[79 of 160]: [Training] ce = 0.39873286 * 50000; errs = 13.852% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.33843s
Finished Epoch[79 of 160]: [Training] ce = 0.39873286 * 50000; errs = 13.852% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.33841s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.44-seconds latency this time; accumulated time on sync point = 1.44 seconds , average latency = 1.44 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
Finished Epoch[80 of 160]: [Training] ce = 0.38707142 * 50000; errs = 13.214% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=9.38199s
Finished Epoch[80 of 160]: [Training] Finished Epoch[80 of 160]: [Training] ce = 0.38707142 * 50000; errs = 13.214% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=9.382s
Finished Epoch[80 of 160]: [Training] ce = 0.38707142 * 50000; errs = 13.214% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=9.38199s
ce = 0.38707142 * 50000; errs = 13.214% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=9.38199s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.97-seconds latency this time; accumulated time on sync point = 1.97 seconds , average latency = 1.97 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.26-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.25-seconds latency this time; accumulated time on sync point = 2.25 seconds , average latency = 2.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[81 of 160]: [Training] ce = 0.29973630 * 50000; errs = 10.284% * 50000; Finished Epoch[81 of 160]: [Training] Finished Epoch[81 of 160]: [Training] ce = 0.29973630 * 50000; errs = 10.284% * 50000; totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.85692s
Finished Epoch[81 of 160]: [Training] ce = 0.29973630 * 50000; errs = 10.284% * 50000; totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.85691s
totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.85692s
ce = 0.29973630 * 50000; errs = 10.284% * 50000; totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.85691s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.26-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 1.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.00-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 1.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.61-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.61 seconds
Finished Epoch[82 of 160]: [Training] ce = 0.34268994 * 50000; errs = 11.720% * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=10.0545s
Finished Epoch[82 of 160]: [Training] ce = 0.34268994 * 50000; errs = 11.720% * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=10.0545s
Finished Epoch[82 of 160]: [Training] ce = 0.34268994 * 50000; errs = 11.720% * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=10.0545s
Finished Epoch[82 of 160]: [Training] ce = 0.34268994 * 50000; errs = 11.720% * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=10.0546s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.27-seconds latency this time; accumulated time on sync point = 2.27 seconds , average latency = 2.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.88-seconds latency this time; accumulated time on sync point = 3.88 seconds , average latency = 3.88 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.63-seconds latency this time; accumulated time on sync point = 3.63 seconds , average latency = 3.63 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[83 of 160]: [Training] ce = 0.36033570 * 50000; errs = 12.412% * 50000; totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=11.7187s
Finished Epoch[83 of 160]: [Training] ce = 0.36033570 * 50000; errs = 12.412%Finished Epoch[83 of 160]: [Training] Finished Epoch[83 of 160]: [Training] ce = 0.36033570 * 50000; errs = 12.412% * 50000; totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=11.7187s
 * 50000; totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=11.7187s
ce = 0.36033570 * 50000; errs = 12.412% * 50000; totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=11.7187s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.62-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 1.62 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.26-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.26 seconds
Finished Epoch[84 of 160]: [Training] ce = 0.34601744 * 50000; errs = 11.986% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=9.57736s
Finished Epoch[84 of 160]: [Training] ce = 0.34601744 * 50000; errs = 11.986% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=9.57729s
Finished Epoch[84 of 160]: [Training] ce = 0.34601744 * 50000; errs = 11.986% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=9.57726s
Finished Epoch[84 of 160]: [Training] ce = 0.34601744 * 50000; errs = 11.986% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=9.5776s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.89-seconds latency this time; accumulated time on sync point = 1.89 seconds , average latency = 1.89 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[85 of 160]: [Training] ce = 0.32354447 * 50000; errs = 11.246% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.50604s
Finished Epoch[85 of 160]: [Training] ce = 0.32354447 * 50000; errs = 11.246% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.50605s
Finished Epoch[85 of 160]: [Training] ce = 0.32354447 * 50000; errs = 11.246% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.50604s
Finished Epoch[85 of 160]: [Training] ce = 0.32354447 * 50000; errs = 11.246% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.4938s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.05-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 1.05 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.34-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.34 seconds
Finished Epoch[86 of 160]: [Training] ce = 0.30493024 * 50000; errs = 10.582% * 50000; totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.06944s
Finished Epoch[86 of 160]: [Training] ce = 0.30493024 * 50000; errs = Finished Epoch[86 of 160]: [Training] ce = 0.30493024 * 50000; errs = 10.582% * 50000; totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.06944s
Finished Epoch[86 of 160]: [Training] ce = 0.30493024 * 50000; errs = 10.582% * 50000; totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.06946s
10.582% * 50000; totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.06944s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.70-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 1.70 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.51-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.51 seconds
Finished Epoch[87 of 160]: [Training] ce = 0.28779376 * 50000; errs = 10.004% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.6855s
Finished Epoch[87 of 160]: [Training] ce = 0.28779376 * 50000; errs = 10.004% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.68552s
Finished Epoch[87 of 160]: [Training] ce = 0.28779376 * 50000; errs = 10.004% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.68551s
Finished Epoch[87 of 160]: [Training] ce = 0.28779376 * 50000; errs = 10.004% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.68551s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.21 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.49-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 1.49 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[88 of 160]: [Training] ce = 0.27219527 * 50000; Finished Epoch[88 of 160]: [Training] ce = 0.27219527 * 50000; errs = 9.524% * 50000; totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=9.29803s
Finished Epoch[88 of 160]: [Training] Finished Epoch[88 of 160]: [Training] ce = 0.27219527 * 50000; errs = 9.524% * 50000; totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=9.29827s
errs = 9.524% * 50000; totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=9.29802s
ce = 0.27219527 * 50000; errs = 9.524% * 50000; totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=9.29485s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.77-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 1.77 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
Finished Epoch[89 of 160]: [Training] ce = 0.25787168 * 50000; errs = 9.002% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=9.84191s
Finished Epoch[89 of 160]: [Training] ce = 0.25787168 * 50000; errs = 9.002% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=9.84191s
Finished Epoch[89 of 160]: [Training] ce = 0.25787168 * 50000; errs = 9.002% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=9.84191s
Finished Epoch[89 of 160]: [Training] ce = 0.25787168 * 50000; errs = 9.002% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=9.84191s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.92-seconds latency this time; accumulated time on sync point = 1.92 seconds , average latency = 1.92 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.60-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 1.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.44-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.44 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[90 of 160]: [Training] ce = 0.24351661 * 50000; errs = 8.570% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.60499s
Finished Epoch[90 of 160]: [Training] ce = 0.24351661 * 50000; errs = 8.570% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.60499s
Finished Epoch[90 of 160]: [Training] ce = 0.24351661 * 50000; errs = 8.570% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.60499s
Finished Epoch[90 of 160]: [Training] ce = 0.24351661 * 50000; errs = 8.570% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.60498s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.87-seconds latency this time; accumulated time on sync point = 1.87 seconds , average latency = 1.87 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.93-seconds latency this time; accumulated time on sync point = 1.93 seconds , average latency = 1.93 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
Finished Epoch[91 of 160]: [Training] ce = 0.23146672 * 50000; errs = Finished Epoch[91 of 160]: [Training] Finished Epoch[91 of 160]: [Training] ce = 0.23146672 * 50000; errs = 7.954% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.86405s
Finished Epoch[91 of 160]: [Training] ce = 0.23146672 * 50000; errs = 7.954% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.86407s
7.954% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.86405s
ce = 0.23146672 * 50000; errs = 7.954% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.86405s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.30-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.25-seconds latency this time; accumulated time on sync point = 1.25 seconds , average latency = 1.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[92 of 160]: [Training] ce = 0.22445846 * 50000; errs = 7.856% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.27835s
Finished Epoch[92 of 160]: [Training] ce = 0.22445846 * 50000; errs = 7.856% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.27834s
Finished Epoch[92 of 160]: [Training] Finished Epoch[92 of 160]: [Training] ce = 0.22445846 * 50000; errs = 7.856% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.2786s
ce = 0.22445846 * 50000; errs = 7.856% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.27811s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.39-seconds latency this time; accumulated time on sync point = 2.39 seconds , average latency = 2.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.87-seconds latency this time; accumulated time on sync point = 3.87 seconds , average latency = 3.87 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.71-seconds latency this time; accumulated time on sync point = 3.71 seconds , average latency = 3.71 seconds
Finished Epoch[93 of 160]: [Training] ce = 0.21282902 * 50000; errs = 7.350% * 50000; totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=11.2105s
Finished Epoch[93 of 160]: [Training] ce = 0.21282902 * 50000; errs = 7.350% * 50000; totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=11.2105s
Finished Epoch[93 of 160]: [Training] ce = 0.21282902 * 50000; errs = 7.350% * 50000; totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=11.2105s
Finished Epoch[93 of 160]: [Training] ce = 0.21282902 * 50000; errs = 7.350% * 50000; totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=11.2078s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.97-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.97 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.82-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.82 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.22-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.22 seconds
Finished Epoch[94 of 160]: [Training] ce = 0.20560997 * 50000; errs = 7.094% * 50000; Finished Epoch[94 of 160]: [Training] ce = 0.20560997 * 50000; errs = 7.094% * 50000; totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.262s
Finished Epoch[94 of 160]: [Training] ce = 0.20560997 * 50000; errs = 7.094% * 50000; totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.27014s
Finished Epoch[94 of 160]: [Training] totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.27014s
ce = 0.20560997 * 50000; errs = 7.094% * 50000; totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.27014s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.49-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 1.49 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.69-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 1.69 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[95 of 160]: [Training] ce = 0.19798695 * 50000; errs = 7.002% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.32916s
Finished Epoch[95 of 160]: [Training] ce = 0.19798695 * 50000; errs = 7.002% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.32916s
Finished Epoch[95 of 160]: [Training] ce = 0.19798695 * 50000; errs = 7.002% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.32915s
Finished Epoch[95 of 160]: [Training] ce = 0.19798695 * 50000; errs = 7.002% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.32917s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.79-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 1.79 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.10-seconds latency this time; accumulated time on sync point = 2.10 seconds , average latency = 2.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.72-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.72 seconds
Finished Epoch[96 of 160]: [Training] ce = 0.18961233 * 50000; Finished Epoch[96 of 160]: [Training] ce = 0.18961233 * 50000; errs = 6.546% * 50000; Finished Epoch[96 of 160]: [Training] totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=10.0267s
Finished Epoch[96 of 160]: [Training] ce = 0.18961233 * 50000; errs = 6.546% * 50000; totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=10.0272s
errs = 6.546% * 50000; totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=10.0275s
ce = 0.18961233 * 50000; errs = 6.546% * 50000; totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=10.0275s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.32-seconds latency this time; accumulated time on sync point = 2.32 seconds , average latency = 2.32 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.15-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 1.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.52-seconds latency this time; accumulated time on sync point = 2.52 seconds , average latency = 2.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[97 of 160]: [Training] Finished Epoch[97 of 160]: [Training] ce = 0.18371092 * 50000; errs = 6.368% * 50000; Finished Epoch[97 of 160]: [Training] ce = 0.18371092 * 50000; errs = 6.368% * 50000; totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=10.2326s
Finished Epoch[97 of 160]: [Training] ce = 0.18371092 * 50000; errs = totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=10.2302s
6.368% * 50000; totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=10.2327s
ce = 0.18371092 * 50000; errs = 6.368% * 50000; totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=10.2302s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.19-seconds latency this time; accumulated time on sync point = 1.19 seconds , average latency = 1.19 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[98 of 160]: [Training] ce = 0.17814203 * 50000; errs = 6.288% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=9.39129s
Finished Epoch[98 of 160]: [Training] ce = 0.17814203 * 50000; errs = 6.288% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=9.39129s
Finished Epoch[98 of 160]: [Training] ce = 0.17814203 * 50000; errs = 6.288% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=9.39129s
Finished Epoch[98 of 160]: [Training] ce = 0.17814203 * 50000; errs = 6.288% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=9.39129s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.49-seconds latency this time; accumulated time on sync point = 2.49 seconds , average latency = 2.49 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.70-seconds latency this time; accumulated time on sync point = 2.70 seconds , average latency = 2.70 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.39-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.39 seconds
Finished Epoch[99 of 160]: [Training] Finished Epoch[99 of 160]: [Training] ce = 0.17431009 * 50000; errs = 6.046% * 50000; Finished Epoch[99 of 160]: [Training] ce = 0.17431009 * 50000; errs = Finished Epoch[99 of 160]: [Training] ce = totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=10.2597s
0.17431009 * 50000; errs = 6.046% * 50000; totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=10.2597s
6.046% * 50000; totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=10.2556s
ce = 0.17431009 * 50000; errs = 6.046% * 50000; totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=10.2597s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.71-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 1.71 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.55-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 1.55 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.37-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.37 seconds
Finished Epoch[100 of 160]: [Training] ce = 0.16809448 * 50000; errs = 5.824% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.91163s
Finished Epoch[100 of 160]: [Training] ce = 0.16809448 * 50000; errs = 5.824% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.91162s
Finished Epoch[100 of 160]: [Training] Finished Epoch[100 of 160]: [Training] ce = 0.16809448 * 50000; errs = 5.824% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.91164s
ce = 0.16809448 * 50000; errs = 5.824% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.91162s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.97-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.97 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.00-seconds latency this time; accumulated time on sync point = 2.00 seconds , average latency = 2.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.95-seconds latency this time; accumulated time on sync point = 1.95 seconds , average latency = 1.95 seconds
Finished Epoch[101 of 160]: [Training] ce = 0.16763810 * 50000; errs = 5.936% * 50000; totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=9.62284s
Finished Epoch[101 of 160]: [Training] Finished Epoch[101 of 160]: [Training] ce = 0.16763810 * 50000; errs = 5.936% * 50000; totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=9.6229s
Finished Epoch[101 of 160]: [Training] ce = 0.16763810 * 50000; errs = 5.936% * 50000; totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=9.62289s
ce = 0.16763810 * 50000; errs = 5.936% * 50000; totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=9.62285s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.83-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 1.83 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.65-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.65 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.71-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 1.71 seconds
Finished Epoch[102 of 160]: [Training] ce = 0.16429224 * 50000; errs = 5.758%Finished Epoch[102 of 160]: [Training] Finished Epoch[102 of 160]: [Training] ce = 0.16429224 * 50000; errs = 5.758% * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.67984s
 * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.67984s
ce = 0.16429224 * 50000; errs = Finished Epoch[102 of 160]: [Training] ce = 0.164292245.758% * 50000; errs = 5.758% * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.68044s
 * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.67983s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.21-seconds latency this time; accumulated time on sync point = 2.21 seconds , average latency = 2.21 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.25-seconds latency this time; accumulated time on sync point = 2.25 seconds , average latency = 2.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
Finished Epoch[103 of 160]: [Training] Finished Epoch[103 of 160]: [Training] ce = 0.15715094 * 50000; errs = 5.584% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.80477s
Finished Epoch[103 of 160]: [Training] ce = 0.15715094 * 50000; errs = 5.584% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.80449s
Finished Epoch[103 of 160]: [Training] ce = 0.15715094 * 50000; errs = 5.584% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.80479s
ce = 0.15715094 * 50000; errs = 5.584% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.80451s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.03-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 1.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.87-seconds latency this time; accumulated time on sync point = 1.87 seconds , average latency = 1.87 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.36-seconds latency this time; accumulated time on sync point = 2.36 seconds , average latency = 2.36 seconds
Finished Epoch[104 of 160]: [Training] ce = 0.15330093 * 50000; errs = 5.508%Finished Epoch[104 of 160]: [Training] Finished Epoch[104 of 160]: [Training] ce = 0.15330093 * 50000; errs = 5.508% * 50000;  * 50000; totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=10.2997s
totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=10.2997s
ce = Finished Epoch[104 of 160]: [Training] ce = 0.15330093 * 50000; errs = 5.508% * 50000; 0.15330093 * 50000; errs = 5.508% * 50000; totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=10.2997s
totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=10.3004s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.36-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 1.36 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.30-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 1.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
Finished Epoch[105 of 160]: [Training] ce = 0.14953846 * 50000; errs = 5.262% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.50648s
Finished Epoch[105 of 160]: [Training] Finished Epoch[105 of 160]: [Training] ce = 0.14953846 * 50000; errs = 5.262% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.50648s
Finished Epoch[105 of 160]: [Training] ce = 0.14953846 * 50000; errs = 5.262% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.50651s
ce = 0.14953846 * 50000; errs = 5.262% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.50623s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.38-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.91-seconds latency this time; accumulated time on sync point = 1.91 seconds , average latency = 1.91 seconds
Finished Epoch[106 of 160]: [Training] ce = 0.14472618 * 50000Finished Epoch[106 of 160]: [Training] ; errs = 5.166% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.31718s
Finished Epoch[106 of 160]: [Training] ce = 0.14472618 * 50000; errs = 5.166% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.31725s
ce = 0.14472618 * 50000; errs = 5.166% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.31716s
Finished Epoch[106 of 160]: [Training] ce = 0.14472618 * 50000; errs = 5.166% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.31724s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.07-seconds latency this time; accumulated time on sync point = 2.07 seconds , average latency = 2.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.24-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.24 seconds
Finished Epoch[107 of 160]: [Training] ce = 0.14250590 * 50000; errs = 5.102% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.95239s
Finished Epoch[107 of 160]: [Training] ce = 0.14250590 * 50000; errs = 5.102% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.95239s
Finished Epoch[107 of 160]: [Training] ce = 0.14250590 * 50000; errs = 5.102% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.95239s
Finished Epoch[107 of 160]: [Training] ce = 0.14250590 * 50000; errs = 5.102% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.95273s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.21 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.45-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 1.45 seconds
Finished Epoch[108 of 160]: [Training] ce = 0.14504892 * 50000; errs = 5.128% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.21618s
Finished Epoch[108 of 160]: [Training] ce = 0.14504892 * 50000; errs = 5.128% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.21611s
Finished Epoch[108 of 160]: [Training] Finished Epoch[108 of 160]: [Training] ce = 0.14504892 * 50000; errs = 5.128% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.21611s
ce = 0.14504892 * 50000; errs = 5.128% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.21619s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.67-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 1.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.86-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 1.86 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
Finished Epoch[109 of 160]: [Training] ce = 0.13538396 * 50000; errs = 4.848% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.95353s
Finished Epoch[109 of 160]: [Training] ce = 0.13538396 * 50000; errs = 4.848% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.95354s
Finished Epoch[109 of 160]: [Training] ce = 0.13538396 * 50000; errs = 4.848% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.95356s
Finished Epoch[109 of 160]: [Training] ce = 0.13538396 * 50000; errs = 4.848% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.95369s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.30-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 1.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.72-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 1.72 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.37-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.37 seconds
Finished Epoch[110 of 160]: [Training] ce = 0.13720527 * 50000; errs = 4.878% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.42256s
Finished Epoch[110 of 160]: [Training] ce = 0.13720527 * 50000; errs = 4.878% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.42254s
Finished Epoch[110 of 160]: [Training] Finished Epoch[110 of 160]: [Training] ce = 0.13720527 * 50000; errs = 4.878% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.42255s
ce = 0.13720527 * 50000; errs = 4.878% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.42255s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.55-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.55 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.63-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 1.63 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.86-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 1.86 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[111 of 160]: [Training] Finished Epoch[111 of 160]: [Training] ce = 0.13380218 * 50000; errs = Finished Epoch[111 of 160]: [Training] ce = 0.13380218 * 50000; errs = 4.740% * 50000; 4.740% * 50000; totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=10.0434s
totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=10.0434s
Finished Epoch[111 of 160]: [Training] ce = 0.13380218 * 50000; errs = 4.740% * 50000; totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=10.0434s
ce = 0.13380218 * 50000; errs = 4.740% * 50000; totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=10.0437s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.08-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 1.08 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.45-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.45 seconds
Finished Epoch[112 of 160]: [Training] ce = 0.13067301 * 50000; errs = 4.630% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.26049s
Finished Epoch[112 of 160]: [Training] ce = 0.13067301 * 50000; errs = 4.630% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.26049s
Finished Epoch[112 of 160]: [Training] ce = 0.13067301 * 50000; errs = 4.630% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.26022s
Finished Epoch[112 of 160]: [Training] ce = 0.13067301 * 50000; errs = 4.630% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.26022s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.17-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 1.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.92-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.92 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.19 seconds
Finished Epoch[113 of 160]: [Training] ce = 0.12948388 * 50000; errs = 4.664% * 50000; Finished Epoch[113 of 160]: [Training] ce = 0.12948388 * 50000; errs = Finished Epoch[113 of 160]: [Training] Finished Epoch[113 of 160]: [Training] ce = 0.12948388 * 50000; errs = 4.664% * 50000; totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=8.64961s
4.664% * 50000; totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=8.64963s
totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=8.64847s
ce = 0.12948388 * 50000; errs = 4.664% * 50000; totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=8.64969s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.65-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 1.65 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.65-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.65 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.80-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 1.80 seconds
Finished Epoch[114 of 160]: [Training] ce = 0.12869815 * 50000; errs = 4.632% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.81433s
Finished Epoch[114 of 160]: [Training] Finished Epoch[114 of 160]: [Training] ce = 0.12869815 * 50000; errs = 4.632% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.81433s
Finished Epoch[114 of 160]: [Training] ce = 0.12869815 * 50000; errs = 4.632% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.8144s
ce = 0.12869815 * 50000; errs = 4.632% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.81436s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.95-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.95 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.30-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 1.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[115 of 160]: [Training] ce = 0.12757309 * 50000; errs = 4.482% * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=8.75808s
Finished Epoch[115 of 160]: [Training] Finished Epoch[115 of 160]: [Training] ce = 0.12757309 * 50000; errs = 4.482% * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=8.75809s
Finished Epoch[115 of 160]: [Training] ce = 0.12757309 * 50000; errs = 4.482% * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=8.75809s
ce = 0.12757309 * 50000; errs = 4.482% * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=8.75807s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.54-seconds latency this time; accumulated time on sync point = 2.54 seconds , average latency = 2.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.73-seconds latency this time; accumulated time on sync point = 1.73 seconds , average latency = 1.73 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.19 seconds
Finished Epoch[116 of 160]: [Training] ce = 0.12969302 * 50000; errs = 4.666% * 50000; totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.99104s
Finished Epoch[116 of 160]: [Training] Finished Epoch[116 of 160]: [Training] ce = 0.12969302 * 50000; errs = 4.666% * 50000; totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.99105s
Finished Epoch[116 of 160]: [Training] ce = 0.12969302 * 50000; errs = 4.666% * 50000; totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.9911s
ce = 0.12969302 * 50000; errs = 4.666% * 50000; totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.99108s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.60-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.84-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 1.84 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.82-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 1.82 seconds
Finished Epoch[117 of 160]: [Training] ce = 0.12314961 * 50000; errs = 4.310% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.44382s
Finished Epoch[117 of 160]: [Training] ce = 0.12314961 * 50000; errs = 4.310% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.44382s
Finished Epoch[117 of 160]: [Training] ce = 0.12314961 * 50000; errs = 4.310% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.44382s
Finished Epoch[117 of 160]: [Training] ce = 0.12314961 * 50000; errs = 4.310% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.44381s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.77-seconds latency this time; accumulated time on sync point = 0.77 seconds , average latency = 0.77 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.14-seconds latency this time; accumulated time on sync point = 2.14 seconds , average latency = 2.14 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.34-seconds latency this time; accumulated time on sync point = 2.34 seconds , average latency = 2.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[118 of 160]: [Training] ce = 0.12537615 * 50000; errs = 4.506% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=10.2532s
Finished Epoch[118 of 160]: [Training] ce = 0.12537615 * 50000; errs = 4.506% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=10.2532s
Finished Epoch[118 of 160]: [Training] ce = 0.12537615 * 50000; errs = 4.506% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=10.2532s
Finished Epoch[118 of 160]: [Training] ce = 0.12537615 * 50000; errs = 4.506% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=10.2532s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.30-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.86-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 1.86 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.20-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 1.20 seconds
Finished Epoch[119 of 160]: [Training] ce = 0.11829232 * 50000; errs = 4.240% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=9.96788s
Finished Epoch[119 of 160]: [Training] ce = 0.11829232 * 50000; errs = 4.240% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=9.96553s
Finished Epoch[119 of 160]: [Training] ce = 0.11829232 * 50000; errs = 4.240% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=9.96789s
Finished Epoch[119 of 160]: [Training] ce = 0.11829232 * 50000; errs = 4.240% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=9.96788s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
Finished Epoch[120 of 160]: [Training] ce = 0.11878004 * 50000; errs = 4.314% * 50000; totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.32632s
Finished Epoch[120 of 160]: [Training] ce = 0.11878004 * 50000; errs = 4.314% * 50000; totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.32634s
Finished Epoch[120 of 160]: [Training] ce = 0.11878004 * 50000; errs = 4.314% * 50000; totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.32632s
Finished Epoch[120 of 160]: [Training] ce = 0.11878004 * 50000; errs = 4.314% * 50000; totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.32634s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.31-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 1.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.77-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 1.77 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.36-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.36 seconds
Finished Epoch[121 of 160]: [Training] ce = 0.10237281 * 50000; errs = 3.628% * 50000; totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.58406s
Finished Epoch[121 of 160]: [Training] ce = 0.10237281 * 50000; errs = 3.628% * 50000; totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.58405s
Finished Epoch[121 of 160]: [Training] ce = 0.10237281 * 50000; errs = 3.628% * 50000; totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.58405s
Finished Epoch[121 of 160]: [Training] ce = 0.10237281 * 50000; errs = 3.628% * 50000; totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.58406s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.10-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 1.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.20-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 1.20 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.21 seconds
Finished Epoch[122 of 160]: [Training] ce = 0.11544312 * 50000; errs = 4.094% * 50000; totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.05036s
Finished Epoch[122 of 160]: [Training] ce = 0.11544312 * 50000; errs = 4.094% * 50000; totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.05037s
Finished Epoch[122 of 160]: [Training] ce = 0.11544312 * 50000; errs = 4.094% * 50000; totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.04023s
Finished Epoch[122 of 160]: [Training] ce = 0.11544312 * 50000; errs = 4.094% * 50000; totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.05036s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.65-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.65 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.91-seconds latency this time; accumulated time on sync point = 1.91 seconds , average latency = 1.91 seconds
Finished Epoch[123 of 160]: [Training] ce = 0.12258679 * 50000; errs = Finished Epoch[123 of 160]: [Training] ce = 0.12258679 * 50000; errs = 4.324% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=9.54676s
Finished Epoch[123 of 160]: [Training] Finished Epoch[123 of 160]: [Training] ce = 0.12258679 * 50000; errs = 4.324% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=9.54677s
4.324% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=9.54677s
ce = 0.12258679 * 50000; errs = 4.324% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=9.54676s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.86-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.86 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[124 of 160]: [Training] ce = 0.12223915 * 50000; errs = 4.392% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.01211s
Finished Epoch[124 of 160]: [Training] ce = 0.12223915 * 50000; errs = 4.392% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.01211s
Finished Epoch[124 of 160]: [Training] ce = 0.12223915 * 50000; errs = 4.392% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.01211s
Finished Epoch[124 of 160]: [Training] ce = 0.12223915 * 50000; errs = 4.392% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.01187s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.38-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.95-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.95 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
Finished Epoch[125 of 160]: [Training] ce = 0.11582862 * 50000; errs = 4.048% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=9.49736s
Finished Epoch[125 of 160]: [Training] ce = 0.11582862 * 50000; errs = 4.048% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=9.49737s
Finished Epoch[125 of 160]: [Training] ce = 0.11582862 * 50000; errs = 4.048% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=9.49738s
Finished Epoch[125 of 160]: [Training] ce = 0.11582862 * 50000; errs = 4.048% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=9.49738s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.39-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.27-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 1.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
Finished Epoch[126 of 160]: [Training] Finished Epoch[126 of 160]: [Training] ce = 0.10987345 * 50000; errs = 3.878% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.42612s
Finished Epoch[126 of 160]: [Training] ce = 0.10987345 * 50000; errs = 3.878% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.42613s
Finished Epoch[126 of 160]: [Training] ce = 0.10987345 * 50000; errs = 3.878% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.42612s
ce = 0.10987345 * 50000; errs = 3.878% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.42209s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.60-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.27-seconds latency this time; accumulated time on sync point = 2.27 seconds , average latency = 2.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.05-seconds latency this time; accumulated time on sync point = 2.05 seconds , average latency = 2.05 seconds
Finished Epoch[127 of 160]: [Training] ce = 0.10322176 * 50000; Finished Epoch[127 of 160]: [Training] ce = 0.10322176 * 50000; errs = 3.602% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.83461s
Finished Epoch[127 of 160]: [Training] Finished Epoch[127 of 160]: [Training] ce = 0.10322176 * 50000; errs = 3.602% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.83457s
errs = 3.602% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.83457s
ce = 0.10322176 * 50000; errs = 3.602% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.83461s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.94-seconds latency this time; accumulated time on sync point = 2.94 seconds , average latency = 2.94 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.16-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 1.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.25-seconds latency this time; accumulated time on sync point = 2.25 seconds , average latency = 2.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[128 of 160]: [Training] ce = 0.09942921 * 50000; errs = 3.462% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=10.3649s
Finished Epoch[128 of 160]: [Training] ce = 0.09942921 * 50000; errs = 3.462% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=10.3649s
Finished Epoch[128 of 160]: [Training] ce = 0.09942921 * 50000; errs = 3.462% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=10.3649s
Finished Epoch[128 of 160]: [Training] ce = 0.09942921 * 50000; errs = 3.462% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=10.3649s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.37-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.98-seconds latency this time; accumulated time on sync point = 1.98 seconds , average latency = 1.98 seconds
Finished Epoch[129 of 160]: [Training] Finished Epoch[129 of 160]: [Training] ce = 0.09755879 * 50000; errs = 3.398% * 50000; Finished Epoch[129 of 160]: [Training] ce = 0.09755879 * 50000; errs = 3.398% * 50000; totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=10.0517s
totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=10.0517s
Finished Epoch[129 of 160]: [Training] ce = 0.09755879 * 50000; errs = 3.398% * 50000; totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=10.0518s
ce = 0.09755879 * 50000; errs = 3.398% * 50000; totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=10.0517s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.60-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 1.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
Finished Epoch[130 of 160]: [Training] ce = 0.09054681 * 50000; errs = 3.066% * 50000; totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.17046s
Finished Epoch[130 of 160]: [Training] ce = 0.09054681 * 50000; errs = 3.066% * 50000; totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.17039s
Finished Epoch[130 of 160]: [Training] ce = 0.09054681 * 50000; errs = 3.066% * 50000; totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.17046s
Finished Epoch[130 of 160]: [Training] ce = 0.09054681 * 50000; errs = 3.066% * 50000; totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.17046s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
Finished Epoch[131 of 160]: [Training] ce = 0.09081000 * 50000; errs = 3.204% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=8.93173s
Finished Epoch[131 of 160]: [Training] ce = 0.09081000 * 50000; errs = 3.204% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=8.93172s
Finished Epoch[131 of 160]: [Training] ce = 0.09081000 * 50000; errs = 3.204% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=8.93173s
Finished Epoch[131 of 160]: [Training] ce = 0.09081000 * 50000; errs = 3.204% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=8.93173s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.79-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 1.79 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.41-seconds latency this time; accumulated time on sync point = 0.41 seconds , average latency = 0.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
Finished Epoch[132 of 160]: [Training] ce = 0.08601919 * 50000; errs = 3.024% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=9.63592s
Finished Epoch[132 of 160]: [Training] ce = 0.08601919 * 50000; errs = 3.024% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=9.63593s
Finished Epoch[132 of 160]: [Training] ce = 0.08601919 * 50000; errs = 3.024% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=9.63593s
Finished Epoch[132 of 160]: [Training] ce = 0.08601919 * 50000; errs = 3.024% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=9.63592s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.96-seconds latency this time; accumulated time on sync point = 1.96 seconds , average latency = 1.96 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.64-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.08-seconds latency this time; accumulated time on sync point = 2.08 seconds , average latency = 2.08 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[133 of 160]: [Training] ce = 0.08412539 * 50000; errs = 2.896% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=10.059s
Finished Epoch[133 of 160]: [Training] ce = 0.08412539 * 50000; errs = 2.896% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=10.059s
Finished Epoch[133 of 160]: [Training] ce = 0.08412539 * 50000; errs = 2.896% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=10.0593s
Finished Epoch[133 of 160]: [Training] ce = 0.08412539 * 50000; errs = 2.896% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=10.059s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.67-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 1.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.09 seconds
Finished Epoch[134 of 160]: [Training] ce = 0.08160149 * 50000; errs = 2.842% * 50000; totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.75588s
Finished Epoch[134 of 160]: [Training] ce = 0.08160149 * 50000; errs = 2.842% * 50000; totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.74768s
Finished Epoch[134 of 160]: [Training] ce = 0.08160149 * 50000; errs = 2.842% * 50000; totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.75589s
Finished Epoch[134 of 160]: [Training] ce = 0.08160149 * 50000; errs = 2.842% * 50000; totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.75589s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.15-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 1.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.31-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 1.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.34-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.34 seconds
Finished Epoch[135 of 160]: [Training] ce = 0.07924714Finished Epoch[135 of 160]: [Training] Finished Epoch[135 of 160]: [Training] ce = 0.07924714 * 50000; errs = 2.710% * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=8.77369s
 * 50000; errs = 2.710% * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=8.77369s
Finished Epoch[135 of 160]: [Training] ce = 0.07924714 * 50000; errs = 2.710% * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=8.77374s
ce = 0.07924714 * 50000; errs = 2.710% * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=8.77375s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.60-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 1.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.81-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.81 seconds
Finished Epoch[136 of 160]: [Training] ce = 0.07724943 * 50000; errs = 2.652% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.71486s
Finished Epoch[136 of 160]: [Training] ce = 0.07724943 * 50000; errs = 2.652% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.71486s
Finished Epoch[136 of 160]: [Training] ce = 0.07724943 * 50000; errs = 2.652% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.7092s
Finished Epoch[136 of 160]: [Training] ce = 0.07724943 * 50000; errs = 2.652% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.71496s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.46-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 1.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.15-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 1.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.09 seconds
Finished Epoch[137 of 160]: [Training] ce = 0.07709523 * 50000; errs = 2.726% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.2397s
Finished Epoch[137 of 160]: [Training] ce = 0.07709523 * 50000; errs = 2.726% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.23971s
Finished Epoch[137 of 160]: [Training] ce = 0.07709523 * 50000; errs = 2.726% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.2397s
Finished Epoch[137 of 160]: [Training] ce = 0.07709523 * 50000; errs = 2.726% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.23971s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.40-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.30-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.30 seconds
Finished Epoch[138 of 160]: [Training] Finished Epoch[138 of 160]: [Training] ce = 0.07465182 * 50000; errs = 2.546% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=9.34755s
Finished Epoch[138 of 160]: [Training] ce = 0.07465182 * 50000; errs = 2.546% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=9.3506s
Finished Epoch[138 of 160]: [Training] ce = 0.07465182 * 50000; errs = 2.546% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=9.34742s
ce = 0.07465182 * 50000; errs = 2.546% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=9.3475s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.94-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.94 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.44-seconds latency this time; accumulated time on sync point = 1.44 seconds , average latency = 1.44 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[139 of 160]: [Training] ce = 0.07393620 * 50000; errs = 2.454% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.6105s
Finished Epoch[139 of 160]: [Training] ce = 0.07393620 * 50000; errs = 2.454% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.6105s
Finished Epoch[139 of 160]: [Training] ce = 0.07393620 * 50000; errs = 2.454% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.61049s
Finished Epoch[139 of 160]: [Training] ce = 0.07393620 * 50000; errs = 2.454% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.61049s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.10-seconds latency this time; accumulated time on sync point = 2.10 seconds , average latency = 2.10 seconds
Finished Epoch[140 of 160]: [Training] ce = 0.07104652 * 50000; errs = 2.432% * 50000; totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.70805s
Finished Epoch[140 of 160]: [Training] ce = 0.07104652 * 50000; errs = 2.432% * 50000; totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.70805s
Finished Epoch[140 of 160]: [Training] ce = 0.07104652 * 50000; errs = 2.432% * 50000; totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.70804s
Finished Epoch[140 of 160]: [Training] ce = 0.07104652 * 50000; errs = 2.432% * 50000; totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.70804s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.39-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.59-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 1.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.67-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 1.67 seconds
Finished Epoch[141 of 160]: [Training] ce = 0.07187997 * 50000; errs = 2.524% * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.22035s
Finished Epoch[141 of 160]: [Training] ce = 0.07187997 * 50000; errs = 2.524% * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.22035s
Finished Epoch[141 of 160]: [Training] Finished Epoch[141 of 160]: [Training] ce = 0.07187997 * 50000; errs = 2.524% * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.22035s
ce = 0.07187997 * 50000; errs = 2.524% * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.22035s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.18-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 1.18 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.24-seconds latency this time; accumulated time on sync point = 2.24 seconds , average latency = 2.24 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.84-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 1.84 seconds
Finished Epoch[142 of 160]: [Training] ce = 0.06812436 * 50000; errs = 2.278% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=9.46919s
Finished Epoch[142 of 160]: [Training] ce = 0.06812436 * 50000; errs = 2.278% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=9.46902s
Finished Epoch[142 of 160]: [Training] ce = 0.06812436 * 50000; errs = 2.278% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=9.46917s
Finished Epoch[142 of 160]: [Training] ce = 0.06812436 * 50000; errs = 2.278% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=9.46918s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.45-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 1.45 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.39-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.39 seconds
Finished Epoch[143 of 160]: [Training] ce = 0.06895635 * 50000; errs = 2.360% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.56963s
Finished Epoch[143 of 160]: [Training] ce = 0.06895635 * 50000; errs = 2.360% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.56962s
Finished Epoch[143 of 160]: [Training] ce = 0.06895635 * 50000; errs = 2.360% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.56988s
Finished Epoch[143 of 160]: [Training] ce = 0.06895635 * 50000; errs = 2.360% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.56987s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.88-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.88 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.02-seconds latency this time; accumulated time on sync point = 2.02 seconds , average latency = 2.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.71-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 1.71 seconds
Finished Epoch[144 of 160]: [Training] ce = 0.07075926 * 50000; errs = 2.390% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.53578s
Finished Epoch[144 of 160]: [Training] ce = 0.07075926 * 50000; errs = 2.390% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.53578s
Finished Epoch[144 of 160]: [Training] ce = 0.07075926 * 50000; errs = 2.390% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.53578s
Finished Epoch[144 of 160]: [Training] ce = 0.07075926 * 50000; errs = 2.390% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.53577s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.76-seconds latency this time; accumulated time on sync point = 1.76 seconds , average latency = 1.76 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.55-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.55 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.05-seconds latency this time; accumulated time on sync point = 2.05 seconds , average latency = 2.05 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[145 of 160]: [Training] ce = 0.06666984 * 50000; errs = 2.162% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.58217s
Finished Epoch[145 of 160]: [Training] ce = 0.06666984 * 50000; errs = 2.162% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.58215s
Finished Epoch[145 of 160]: [Training] ce = 0.06666984 * 50000; errs = 2.162% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.58216s
Finished Epoch[145 of 160]: [Training] ce = 0.06666984 * 50000; errs = 2.162% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.58217s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.04-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 1.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.99-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 1.99 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.02-seconds latency this time; accumulated time on sync point = 2.02 seconds , average latency = 2.02 seconds
Finished Epoch[146 of 160]: [Training] ce = 0.06744009 * 50000; errs = 2.288% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.56937s
Finished Epoch[146 of 160]: [Training] ce = 0.06744009 * 50000; errs = 2.288% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.56937s
Finished Epoch[146 of 160]: [Training] ce = 0.06744009 * 50000; errs = 2.288% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.56936s
Finished Epoch[146 of 160]: [Training] ce = 0.06744009 * 50000; errs = 2.288% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.56937s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.78-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 1.78 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.34-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.34 seconds
Finished Epoch[147 of 160]: [Training] Finished Epoch[147 of 160]: [Training] ce = 0.06817643 * 50000; errs = Finished Epoch[147 of 160]: [Training] ce = 0.06817643 * 50000; errs = 2.340% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.55523s
2.340% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.55512s
Finished Epoch[147 of 160]: [Training] ce = 0.06817643 * 50000; errs = 2.340% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.55514s
ce = 0.06817643 * 50000; errs = 2.340% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.54673s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.47-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.14-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 1.14 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.23-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 1.23 seconds
Finished Epoch[148 of 160]: [Training] ce = 0.06616562 * 50000; errs = 2.200% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.09938s
Finished Epoch[148 of 160]: [Training] ce = 0.06616562 * 50000; errs = 2.200% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.10545s
Finished Epoch[148 of 160]: [Training] ce = 0.06616562 * 50000; errs = 2.200% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.10536s
Finished Epoch[148 of 160]: [Training] ce = 0.06616562 * 50000; errs = 2.200% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.10535s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.88-seconds latency this time; accumulated time on sync point = 1.88 seconds , average latency = 1.88 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.03-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 1.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.04-seconds latency this time; accumulated time on sync point = 2.04 seconds , average latency = 2.04 seconds
Finished Epoch[149 of 160]: [Training] ce = 0.06630611 * 50000; errs = 2.170% * 50000Finished Epoch[149 of 160]: [Training] ce = 0.06630611 * 50000; errs = 2.170% * 50000; totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=9.69452s
Finished Epoch[149 of 160]: [Training] Finished Epoch[149 of 160]: [Training] ce = 0.06630611 * 50000; errs = 2.170% * 50000; totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=9.69449s
; totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=9.69445s
ce = 0.06630611 * 50000; errs = 2.170% * 50000; totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=9.69764s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.01-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 1.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.68-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.19-seconds latency this time; accumulated time on sync point = 1.19 seconds , average latency = 1.19 seconds
Finished Epoch[150 of 160]: [Training] ce = 0.06513781 * 50000; errs = 2.188% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.02752s
Finished Epoch[150 of 160]: [Training] ce = 0.06513781 * 50000; errs = 2.188% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.02753s
Finished Epoch[150 of 160]: [Training] ce = 0.06513781 * 50000; errs = 2.188% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.02754s
Finished Epoch[150 of 160]: [Training] ce = 0.06513781 * 50000; errs = 2.188% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.02755s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.19 seconds
Finished Epoch[151 of 160]: [Training] ce = 0.06309516 * 50000; errs = 2.068% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.2796s
Finished Epoch[151 of 160]: [Training] ce = 0.06309516 * 50000; errs = 2.068% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.27961s
Finished Epoch[151 of 160]: [Training] Finished Epoch[151 of 160]: [Training] ce = 0.06309516 * 50000; errs = 2.068% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.27961s
ce = 0.06309516 * 50000; errs = 2.068% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.27959s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.62-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 1.62 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.39-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[152 of 160]: [Training] Finished Epoch[152 of 160]: [Training] ce = 0.06470110 * 50000; errs = Finished Epoch[152 of 160]: [Training] ce = 0.06470110 * 50000; errs = 2.188% * 50000; Finished Epoch[152 of 160]: [Training] ce = 0.06470110 * 50000; errs = 2.188% * 50000; 2.188% * 50000; totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=9.17708s
totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=9.17706s
totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=9.17707s
ce = 0.06470110 * 50000; errs = 2.188% * 50000; totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=9.17706s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.64-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.55-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 1.55 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.46-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 1.46 seconds
Finished Epoch[153 of 160]: [Training] ce = 0.06470836 * 50000; errs = 2.218% * 50000; totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.03249s
Finished Epoch[153 of 160]: [Training] ce = 0.06470836 * 50000; Finished Epoch[153 of 160]: [Training] ce = 0.06470836 * 50000; errs = 2.218% * 50000; totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.03249s
errs = 2.218% * 50000; totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.03249s
Finished Epoch[153 of 160]: [Training] ce = 0.06470836 * 50000; errs = 2.218% * 50000; totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.03292s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.59-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 1.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.30-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.30 seconds
Finished Epoch[154 of 160]: [Training] ce = 0.06393859 * 50000; errs = 2.156% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.66529s
Finished Epoch[154 of 160]: [Training] ce = 0.06393859Finished Epoch[154 of 160]: [Training] Finished Epoch[154 of 160]: [Training] ce = 0.06393859 * 50000; errs = 2.156% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.6653s
ce =  * 50000; 0.06393859 * 50000; errs = 2.156% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.66527s
errs = 2.156% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.66531s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.78-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 1.78 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.19 seconds
Finished Epoch[155 of 160]: [Training] ce = Finished Epoch[155 of 160]: [Training] Finished Epoch[155 of 160]: [Training] ce = 0.06308756 * 50000; errs = 2.162% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=9.41049s
Finished Epoch[155 of 160]: [Training] ce = 0.06308756 * 50000; errs = 2.162% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=9.41051s
0.06308756ce = 0.06308756 * 50000; errs = 2.162% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=9.41026s
 * 50000; errs = 2.162% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=9.41027s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.45-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.45 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.41-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 1.41 seconds
Finished Epoch[156 of 160]: [Training] ce = 0.06169251 * 50000; errs = 2.104% * 50000; totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.68673s
Finished Epoch[156 of 160]: [Training] Finished Epoch[156 of 160]: [Training] ce = 0.06169251 * 50000; errs = 2.104% * 50000; totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.68609s
Finished Epoch[156 of 160]: [Training] ce = 0.06169251 * 50000; errs = 2.104% * 50000; totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.68601s
ce = 0.06169251 * 50000; errs = 2.104% * 50000; totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.68673s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.88-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.88 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.57-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 1.57 seconds
Finished Epoch[157 of 160]: [Training] ce = 0.06223990 * 50000; errs = 2.120% * 50000Finished Epoch[157 of 160]: [Training] ce = 0.06223990 * 50000; errs = 2.120% * 50000; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.35818s
Finished Epoch[157 of 160]: [Training] Finished Epoch[157 of 160]: [Training] ce = 0.06223990 * 50000; errs = 2.120% * 50000; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.35826s
; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.35826s
ce = 0.06223990 * 50000; errs = 2.120% * 50000; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.35818s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.56-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.80-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 1.80 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.80-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 1.80 seconds
Finished Epoch[158 of 160]: [Training] ce = 0.06221441 * 50000; errs = 2.062% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=10.2962s
Finished Epoch[158 of 160]: [Training] Finished Epoch[158 of 160]: [Training] ce = 0.06221441 * 50000; errs = 2.062% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=10.2962s
Finished Epoch[158 of 160]: [Training] ce = 0.06221441 * 50000; errs = 2.062% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=10.2962s
ce = 0.06221441 * 50000; errs = 2.062% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=10.2962s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.99-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 1.99 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.77-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 1.77 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.36-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.36 seconds
Finished Epoch[159 of 160]: [Training] Finished Epoch[159 of 160]: [Training] ce = 0.06022586 * 50000; errs = 2.042% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=9.71553s
Finished Epoch[159 of 160]: [Training] ce = Finished Epoch[159 of 160]: [Training] ce = 0.06022586 * 50000; errs = 2.042% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=9.71558s
0.06022586 * 50000; errs = 2.042% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=9.71553s
ce = 0.06022586 * 50000; errs = 2.042% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=9.71557s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.59-seconds latency this time; accumulated time on sync point = 0.59 seconds , average latency = 0.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.73-seconds latency this time; accumulated time on sync point = 1.73 seconds , average latency = 1.73 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.08-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 1.08 seconds
Finished Epoch[160 of 160]: [Training] ce = 0.06054025 * 50000; errs = 2.016% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.80625s
Finished Epoch[160 of 160]: [Training] Finished Epoch[160 of 160]: [Training] ce = 0.06054025 * 50000; errs = 2.016% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.80625s
Finished Epoch[160 of 160]: [Training] ce = 0.06054025 * 50000; errs = 2.016% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.80626s
ce = 0.06054025 * 50000; errs = 2.016% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.80624s





##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################


##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################


##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################


##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################

Final Results: Minibatch[1-21]: errs = 8.880% * 10000; top5Errs = 0.230% * 10000
Final Results: Minibatch[1-21]: errs = 8.880% * 10000; top5Errs = 0.230% * 10000
Final Results: Minibatch[1-21]: errs = 8.880% * 10000; top5Errs = 0.230% * 10000
Final Results: Minibatch[1-21]: errs = 8.880% * 10000; top5Errs = 0.230% * 10000




COMPLETED.
COMPLETED.
COMPLETED.
COMPLETED.
