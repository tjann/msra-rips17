CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/14 09:58:02

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/14 09:58:02

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/14 09:58:02

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/14 09:58:02

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
--------------------------------------------------------------------------
[[65473,1],1]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: Multiversohk

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (2) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (0) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (1) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (3) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11436 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].
[Multiversohk:05810] 3 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[Multiversohk:05810] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].

Model has 209 nodes. Using GPU 1.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.


Model has 209 nodes. Using GPU 2.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.


Model has 209 nodes. Using GPU 0.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.


Model has 209 nodes. Using GPU 3.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.

Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 5.28-seconds latency this time; accumulated time on sync point = 5.28 seconds , average latency = 5.28 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 7.30-seconds latency this time; accumulated time on sync point = 7.30 seconds , average latency = 7.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 9.33-seconds latency this time; accumulated time on sync point = 9.33 seconds , average latency = 9.33 seconds
Finished Epoch[ 1 of 160]: [Training] ce = 2.02438734 * 50000; errs = 75.408% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=23.9521s
Finished Epoch[ 1 of 160]: [Training] ce = 2.02438734 * 50000; errs = 75.408% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=23.9521s
Finished Epoch[ 1 of 160]: [Training] ce = 2.02438734 * 50000; errs = 75.408% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=23.9521s
Finished Epoch[ 1 of 160]: [Training] ce = 2.02438734 * 50000; errs = 75.408% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=23.9521s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.19-seconds latency this time; accumulated time on sync point = 1.19 seconds , average latency = 1.19 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.46-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.28-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 1.28 seconds
Finished Epoch[ 2 of 160]: [Training] ce = 1.84810926 * 50000; errs = 70.010% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=9.08609s
Finished Epoch[ 2 of 160]: [Training] Finished Epoch[ 2 of 160]: [Training] ce = 1.84810926 * 50000; errs = 70.010% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=9.08612s
Finished Epoch[ 2 of 160]: [Training] ce = 1.84810926 * 50000; errs = 70.010% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=9.08612s
ce = 1.84810926 * 50000; errs = 70.010% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=9.07701s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.12-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 1.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.10-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 1.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.38-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.38 seconds
Finished Epoch[ 3 of 160]: [Training] ce = 1.75992730 * 50000Finished Epoch[ 3 of 160]: [Training] ce = 1.75992730 * 50000; errs = 65.780% * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=8.93095s
Finished Epoch[ 3 of 160]: [Training] Finished Epoch[ 3 of 160]: [Training] ce = 1.75992730 * 50000; errs = 65.780% * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=8.93095s
; errs = 65.780% * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=8.93095s
ce = 1.75992730 * 50000; errs = 65.780% * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=8.92917s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.77-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 1.77 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.76-seconds latency this time; accumulated time on sync point = 1.76 seconds , average latency = 1.76 seconds
Finished Epoch[ 4 of 160]: [Training] ce = 1.61254660 * 50000; errs = 59.900% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.55394s
Finished Epoch[ 4 of 160]: [Training] Finished Epoch[ 4 of 160]: [Training] ce = 1.61254660 * 50000; errs = 59.900% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.55394s
Finished Epoch[ 4 of 160]: [Training] ce = 1.61254660 * 50000; errs = 59.900% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.55395s
ce = 1.61254660 * 50000; errs = 59.900% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.55393s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.12-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 1.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.04-seconds latency this time; accumulated time on sync point = 2.04 seconds , average latency = 2.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.63-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 1.63 seconds
Finished Epoch[ 5 of 160]: [Training] Finished Epoch[ 5 of 160]: [Training] ce = 1.51390113 * 50000; errs = 55.732% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=9.91522s
Finished Epoch[ 5 of 160]: [Training] ce = 1.51390113 * 50000; errs = 55.732% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=9.91524s
ce = 1.51390113 * 50000; Finished Epoch[ 5 of 160]: [Training] ce = 1.51390113 * 50000; errs = 55.732% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=9.91561s
errs = 55.732% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=9.91522s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.65-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.65 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
Finished Epoch[ 6 of 160]: [Training] Finished Epoch[ 6 of 160]: [Training] ce = 1.45675449 * 50000; errs = 53.156% * 50000; totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=8.96639s
ce = 1.45675449Finished Epoch[ 6 of 160]: [Training] Finished Epoch[ 6 of 160]: [Training] ce = 1.45675449 * 50000; errs = 53.156% * 50000; totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=8.96651s
ce = 1.45675449 * 50000; errs = 53.156% * 50000;  * 50000; errs = 53.156% * 50000; totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=8.96531s
totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=8.96651s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.26-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.93-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.93 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.16-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 1.16 seconds
Finished Epoch[ 7 of 160]: [Training] ce = 1.36523395 * 50000; errs = Finished Epoch[ 7 of 160]: [Training] ce = 1.36523395 * 50000; errs = 50.012% * 50000; Finished Epoch[ 7 of 160]: [Training] 50.012% * 50000; totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=8.82968s
totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=8.82968s
Finished Epoch[ 7 of 160]: [Training] ce = 1.36523395 * 50000; errs = 50.012% * 50000; totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=8.82977s
ce = 1.36523395 * 50000; errs = 50.012% * 50000; totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=8.82975s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.44-seconds latency this time; accumulated time on sync point = 1.44 seconds , average latency = 1.44 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.46-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 8 of 160]: [Training] Finished Epoch[ 8 of 160]: [Training] ce = Finished Epoch[ 8 of 160]: [Training] ce = 1.28300699 * 50000; errs = 46.514% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.36982s
Finished Epoch[ 8 of 160]: [Training] ce = 1.28300699 * 50000; errs = 46.514% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.36981s
1.28300699 * 50000; errs = 46.514% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.3698s
ce = 1.28300699 * 50000; errs = 46.514% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.36978s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.42-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.92-seconds latency this time; accumulated time on sync point = 1.92 seconds , average latency = 1.92 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.77-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 1.77 seconds
Finished Epoch[ 9 of 160]: [Training] Finished Epoch[ 9 of 160]: [Training] ce = 1.20723148 * 50000; errs = 43.542% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.40996s
Finished Epoch[ 9 of 160]: [Training] ce = 1.20723148 * 50000; errs = 43.542% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.40997s
Finished Epoch[ 9 of 160]: [Training] ce = 1.20723148 * 50000; errs = 43.542% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.40998s
ce = 1.20723148 * 50000; errs = 43.542% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.40997s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.95-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.95 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.97-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.97 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.16 seconds
Finished Epoch[10 of 160]: [Training] ce = 1.14563744 * 50000; errs = 41.330% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=8.87746s
Finished Epoch[10 of 160]: [Training] ce = 1.14563744 * 50000; errs = 41.330% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=8.87745s
Finished Epoch[10 of 160]: [Training] Finished Epoch[10 of 160]: [Training] ce = 1.14563744 * 50000; errs = 41.330% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=8.8706s
ce = 1.14563744 * 50000; errs = 41.330% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=8.87745s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.47-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.30-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 1.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.41-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 1.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[11 of 160]: [Training] ce = 1.09178990 * 50000; errs = 39.232%Finished Epoch[11 of 160]: [Training] ce = 1.09178990 * 50000; errs = 39.232% * 50000; totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.24662s
Finished Epoch[11 of 160]: [Training]  * 50000; totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.24654s
Finished Epoch[11 of 160]: [Training] ce = 1.09178990 * 50000; errs = 39.232% * 50000; totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.24655s
ce = 1.09178990 * 50000; errs = 39.232% * 50000; totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.24662s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.03-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 2.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.72-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.72 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.71-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 1.71 seconds
Finished Epoch[12 of 160]: [Training] Finished Epoch[12 of 160]: [Training] ce = 1.03021033 * 50000; errs = Finished Epoch[12 of 160]: [Training] ce = 1.03021033 * 50000; errs = 37.134% * 50000; 37.134% * 50000; totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.38238s
Finished Epoch[12 of 160]: [Training] ce = 1.03021033 * 50000; errs = 37.134% * 50000; totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.38239s
totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.3824s
ce = 1.03021033 * 50000; errs = 37.134% * 50000; totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.38237s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.82-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 1.82 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.88-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.88 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
Finished Epoch[13 of 160]: [Training] ce = 0.98836596 * 50000; errs = 35.302% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.63046s
Finished Epoch[13 of 160]: [Training] ce = 0.98836596 * 50000; errs = 35.302% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.63044s
Finished Epoch[13 of 160]: [Training] ce = 0.98836596 * 50000; errs = 35.302% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.63046s
Finished Epoch[13 of 160]: [Training] ce = 0.98836596 * 50000; errs = 35.302% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.63044s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.46-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 1.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.98-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.98 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.32-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.32 seconds
Finished Epoch[14 of 160]: [Training] ce = 0.95205705 * 50000; errs = 33.728% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.06114s
Finished Epoch[14 of 160]: [Training] ce = 0.95205705 * 50000; errs = 33.728% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.06113s
Finished Epoch[14 of 160]: [Training] Finished Epoch[14 of 160]: [Training] ce = 0.95205705 * 50000; errs = 33.728% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.06138s
ce = 0.95205705 * 50000; errs = 33.728% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.06074s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.64-seconds latency this time; accumulated time on sync point = 0.64 seconds , average latency = 0.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.31-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 1.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
Finished Epoch[15 of 160]: [Training] ce = 0.92169592 * 50000; errs = 32.732% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.4297s
Finished Epoch[15 of 160]: [Training] Finished Epoch[15 of 160]: [Training] ce = 0.92169592 * 50000; errs = 32.732% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.42971s
Finished Epoch[15 of 160]: [Training] ce = 0.92169592 * 50000; errs = 32.732% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.4297s
ce = 0.92169592 * 50000; errs = 32.732% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.42969s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.21 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.36-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 1.36 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.18-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 1.18 seconds
Finished Epoch[16 of 160]: [Training] ce = 0.87784264 * 50000; errs = 31.322% * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=9.09505s
Finished Epoch[16 of 160]: [Training] Finished Epoch[16 of 160]: [Training] ce = 0.87784264 * 50000; errs = 31.322% * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=9.09505s
Finished Epoch[16 of 160]: [Training] ce = 0.87784264 * 50000; errs = 31.322% * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=9.09506s
ce = 0.87784264 * 50000; errs = 31.322% * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=9.09505s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.69-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 1.69 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.49-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.49 seconds
Finished Epoch[17 of 160]: [Training] Finished Epoch[17 of 160]: [Training] ce = 0.85036395 * 50000; errs = 30.078% * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=9.38217s
Finished Epoch[17 of 160]: [Training] ce = 0.85036395 * 50000; errs = 30.078% * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=9.38218s
Finished Epoch[17 of 160]: [Training] ce = 0.85036395 * 50000; errs = 30.078% * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=9.38243s
ce = 0.85036395 * 50000; errs = 30.078% * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=9.38216s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.41-seconds latency this time; accumulated time on sync point = 0.41 seconds , average latency = 0.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.61-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 1.61 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
Finished Epoch[18 of 160]: [Training] ce = 0.82566918 * 50000; errs = 28.992% * 50000; Finished Epoch[18 of 160]: [Training] ce = 0.82566918 * 50000; errs = 28.992% * 50000; totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=9.24847s
Finished Epoch[18 of 160]: [Training] ce = 0.82566918 * 50000; errs = 28.992% * 50000; totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=9.24847s
Finished Epoch[18 of 160]: [Training] totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=9.24872s
ce = 0.82566918 * 50000; errs = 28.992% * 50000; totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=9.24871s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.08-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 1.08 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.02-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 1.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
Finished Epoch[19 of 160]: [Training] ce = 0.77879354 * 50000; errs = 27.250% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=8.87137s
Finished Epoch[19 of 160]: [Training] Finished Epoch[19 of 160]: [Training] ce = 0.77879354 * 50000; errs = 27.250% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=8.87131s
Finished Epoch[19 of 160]: [Training] ce = 0.77879354 * 50000; errs = 27.250% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=8.8674s
ce = 0.77879354 * 50000; errs = 27.250% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=8.87131s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.98-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.98 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
Finished Epoch[20 of 160]: [Training] ce = 0.75979611 * 50000; Finished Epoch[20 of 160]: [Training] ce = 0.75979611 * 50000; errs = 26.778% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.07223s
Finished Epoch[20 of 160]: [Training] ce = 0.75979611 * 50000; errs = 26.778% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.07224s
Finished Epoch[20 of 160]: [Training] ce = 0.75979611 * 50000; errs = 26.778% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.07225s
errs = 26.778% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.07225s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.63-seconds latency this time; accumulated time on sync point = 0.63 seconds , average latency = 0.63 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.83-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 1.83 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
Finished Epoch[21 of 160]: [Training] ce = 0.72272002 * 50000; errs = 25.426% * 50000; totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.4524s
Finished Epoch[21 of 160]: [Training] ce = 0.72272002 * 50000; errs = 25.426% * 50000; totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.4524s
Finished Epoch[21 of 160]: [Training] ce = 0.72272002 * 50000; errs = 25.426% * 50000; totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.4524s
Finished Epoch[21 of 160]: [Training] ce = 0.72272002 * 50000; errs = 25.426% * 50000; totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.45243s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.48-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 1.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.18-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.18 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.46-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 1.46 seconds
Finished Epoch[22 of 160]: [Training] Finished Epoch[22 of 160]: [Training] ce = 0.69604859 * 50000; errs = 24.230% * 50000; totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.16709s
Finished Epoch[22 of 160]: [Training] ce = 0.69604859 * 50000; errs = 24.230% * 50000; totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.1671s
Finished Epoch[22 of 160]: [Training] ce = 0.69604859 * 50000; errs = 24.230% * 50000; totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.16709s
ce = 0.69604859 * 50000; errs = 24.230% * 50000; totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.1671s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.72-seconds latency this time; accumulated time on sync point = 0.72 seconds , average latency = 0.72 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.76-seconds latency this time; accumulated time on sync point = 0.76 seconds , average latency = 0.76 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.07 seconds
Finished Epoch[23 of 160]: [Training] Finished Epoch[23 of 160]: [Training] ce = 0.67073750 * 50000; errs = 23.298% * 50000Finished Epoch[23 of 160]: [Training] ce = 0.67073750 * 50000; errs = 23.298% * 50000; totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=9.01995s
Finished Epoch[23 of 160]: [Training] ce = 0.67073750 * 50000; errs = 23.298% * 50000; totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=9.01996s
; totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=9.01997s
ce = 0.67073750 * 50000; errs = 23.298% * 50000; totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=9.01995s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.60-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.84-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 1.84 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[24 of 160]: [Training] ce = 0.64302000 * 50000; errs = 22.402% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.51515s
Finished Epoch[24 of 160]: [Training] Finished Epoch[24 of 160]: [Training] ce = 0.64302000 * 50000; errs = 22.402% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.51513s
Finished Epoch[24 of 160]: [Training] ce = 0.64302000 * 50000; errs = 22.402% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.51513s
ce = 0.64302000 * 50000; errs = 22.402% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.51514s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.06 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.61-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 1.61 seconds
Finished Epoch[25 of 160]: [Training] ce = 0.63317252 * 50000; errs = 22.012% * 50000; totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.28964s
Finished Epoch[25 of 160]: [Training] ce = 0.63317252 * 50000; errs = 22.012% * 50000; totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.29382s
Finished Epoch[25 of 160]: [Training] ce = 0.63317252 * 50000; errs = 22.012% * 50000; totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.2938s
Finished Epoch[25 of 160]: [Training] ce = 0.63317252 * 50000; errs = 22.012% * 50000; totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.2938s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.25-seconds latency this time; accumulated time on sync point = 1.25 seconds , average latency = 1.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
Finished Epoch[26 of 160]: [Training] ce = 0.60778654 * 50000; errs = 21.170% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.02776s
Finished Epoch[26 of 160]: [Training] ce = 0.60778654 * 50000; errs = 21.170% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.02776s
Finished Epoch[26 of 160]: [Training] ce = 0.60778654 * 50000; errs = 21.170% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.02777s
Finished Epoch[26 of 160]: [Training] ce = 0.60778654 * 50000; errs = 21.170% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.02777s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.48-seconds latency this time; accumulated time on sync point = 0.48 seconds , average latency = 0.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.00-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 1.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[27 of 160]: [Training] Finished Epoch[27 of 160]: [Training] ce = 0.59609184 * 50000; errs = Finished Epoch[27 of 160]: [Training] ce = 0.59609184 * 50000; errs = 20.818% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.2343s
Finished Epoch[27 of 160]: [Training] ce = 0.59609184 * 50000; errs = 20.818% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.23432s
20.818% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.23432s
ce = 0.59609184 * 50000; errs = 20.818% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.23404s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.18-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.18 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.09-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 1.09 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.21-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 1.21 seconds
Finished Epoch[28 of 160]: [Training] ce = 0.57798681 * 50000; errs = 20.060% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=8.7651s
Finished Epoch[28 of 160]: [Training] ce = 0.57798681 * 50000; errs = 20.060% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=8.76509s
Finished Epoch[28 of 160]: [Training] ce = 0.57798681 * 50000; errs = 20.060% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=8.7651s
Finished Epoch[28 of 160]: [Training] ce = 0.57798681 * 50000; errs = 20.060% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=8.7651s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.36-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 1.36 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.36-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.36 seconds
Finished Epoch[29 of 160]: [Training] ce = 0.56277573 * 50000; errs = 19.452% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=9.3054s
Finished Epoch[29 of 160]: [Training] ce = 0.56277573 * 50000; errs = 19.452% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=9.3054s
Finished Epoch[29 of 160]: [Training] ce = 0.56277573 * 50000; errs = 19.452% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=9.3054s
Finished Epoch[29 of 160]: [Training] ce = 0.56277573 * 50000; errs = 19.452% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=9.3054s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.51-seconds latency this time; accumulated time on sync point = 2.51 seconds , average latency = 2.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.37-seconds latency this time; accumulated time on sync point = 2.37 seconds , average latency = 2.37 seconds
Finished Epoch[30 of 160]: [Training] Finished Epoch[30 of 160]: [Training] ce = 0.54892129 * 50000; errs = 18.872% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=10.4013s
Finished Epoch[30 of 160]: [Training] ce = 0.54892129 * 50000; errs = 18.872% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=10.4016s
Finished Epoch[30 of 160]: [Training] ce = 0.54892129 * 50000; errs = 18.872% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=10.3985s
ce = 0.54892129 * 50000; errs = 18.872% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=10.4013s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.73-seconds latency this time; accumulated time on sync point = 0.73 seconds , average latency = 0.73 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.96-seconds latency this time; accumulated time on sync point = 0.96 seconds , average latency = 0.96 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
Finished Epoch[31 of 160]: [Training] ce = Finished Epoch[31 of 160]: [Training] Finished Epoch[31 of 160]: [Training] ce = 0.54905002 * 50000; errs = 18.852% * 50000; Finished Epoch[31 of 160]: [Training] ce = 0.54905002 * 50000; errs = totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=8.83819s
0.54905002 * 50000; errs = 18.852% * 50000; totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=8.83457s
18.852% * 50000; totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=8.83821s
ce = 0.54905002 * 50000; errs = 18.852% * 50000; totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=8.83819s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.41-seconds latency this time; accumulated time on sync point = 2.41 seconds , average latency = 2.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.21-seconds latency this time; accumulated time on sync point = 2.21 seconds , average latency = 2.21 seconds
Finished Epoch[32 of 160]: [Training] ce = 0.52543085 * 50000; errs = 18.142% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=10.1753s
Finished Epoch[32 of 160]: [Training] Finished Epoch[32 of 160]: [Training] ce = 0.52543085 * 50000; errs = 18.142% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=10.1753s
Finished Epoch[32 of 160]: [Training] ce = 0.52543085 * 50000; errs = 18.142% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=10.1753s
ce = 0.52543085 * 50000; errs = 18.142% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=10.1753s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.30-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 1.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.21 seconds
Finished Epoch[33 of 160]: [Training] ce = 0.52283410 * 50000; errs = 17.944% * 50000; totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=9.27498s
Finished Epoch[33 of 160]: [Training] ce = Finished Epoch[33 of 160]: [Training] ce = 0.52283410 * 50000; errs = 17.944% * 50000; totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=9.27311s
Finished Epoch[33 of 160]: [Training] ce = 0.52283410 * 50000; errs = 17.944% * 50000; totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=9.27524s
0.52283410 * 50000; errs = 17.944% * 50000; totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=9.27498s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.45-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 1.45 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
Finished Epoch[34 of 160]: [Training] ce = 0.50950482 * 50000; errs = 17.340% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.31125s
Finished Epoch[34 of 160]: [Training] Finished Epoch[34 of 160]: [Training] ce = 0.50950482 * 50000; errs = 17.340% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.31126s
Finished Epoch[34 of 160]: [Training] ce = 0.50950482 * 50000; errs = 17.340% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.31111s
ce = 0.50950482 * 50000; errs = 17.340% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.31127s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.25-seconds latency this time; accumulated time on sync point = 1.25 seconds , average latency = 1.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.12 seconds
Finished Epoch[35 of 160]: [Training] ce = 0.51734069 * 50000; errs = 17.642% * 50000; totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.06106s
Finished Epoch[35 of 160]: [Training] Finished Epoch[35 of 160]: [Training] ce = 0.51734069 * 50000; errs = 17.642% * 50000; totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.06199s
Finished Epoch[35 of 160]: [Training] ce = 0.51734069 * 50000; errs = 17.642% * 50000; totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.06199s
ce = 0.51734069 * 50000; errs = 17.642% * 50000; totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.06198s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.26-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 1.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.20-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 1.20 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.22-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.22 seconds
Finished Epoch[36 of 160]: [Training] ce = 0.50269246 * 50000; errs = 17.414% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=8.97869s
Finished Epoch[36 of 160]: [Training] ce = 0.50269246 * 50000; errs = 17.414% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=8.97903s
Finished Epoch[36 of 160]: [Training] ce = 0.50269246 * 50000; errs = 17.414% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=8.97869s
Finished Epoch[36 of 160]: [Training] ce = 0.50269246 * 50000; errs = 17.414% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=8.97903s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.59-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 1.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.21 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.57-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 1.57 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[37 of 160]: [Training] ce = 0.48741950 * 50000; Finished Epoch[37 of 160]: [Training] ce = 0.48741950 * 50000; errs = 16.656% * 50000; Finished Epoch[37 of 160]: [Training] ce = 0.48741950 * 50000; errs = Finished Epoch[37 of 160]: [Training] errs = 16.656% * 50000; totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=9.48242s
16.656% * 50000; totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=9.48242s
totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=9.48252s
ce = 0.48741950 * 50000; errs = 16.656% * 50000; totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=9.48251s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.26-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
Finished Epoch[38 of 160]: [Training] ce = 0.49089235 * 50000; errs = Finished Epoch[38 of 160]: [Training] ce = 0.49089235 * 50000; errs = 16.946% * 50000; Finished Epoch[38 of 160]: [Training] Finished Epoch[38 of 160]: [Training] ce = 0.49089235 * 50000; errs = 16.946% * 50000; totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.44501s
totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.44504s
16.946% * 50000; totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.4451s
ce = 0.49089235 * 50000; errs = 16.946% * 50000; totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.44508s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.10-seconds latency this time; accumulated time on sync point = 2.10 seconds , average latency = 2.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.84-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.84 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.13-seconds latency this time; accumulated time on sync point = 2.13 seconds , average latency = 2.13 seconds
Finished Epoch[39 of 160]: [Training] Finished Epoch[39 of 160]: [Training] ce = 0.48336061 * 50000; errs = 16.604% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.82251s
Finished Epoch[39 of 160]: [Training] ce = 0.48336061 * 50000; errs = 16.604% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.82252s
Finished Epoch[39 of 160]: [Training] ce = 0.48336061 * 50000; errs = 16.604% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.82194s
ce = 0.48336061 * 50000; errs = 16.604% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.82251s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.59-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 1.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
Finished Epoch[40 of 160]: [Training] ce = 0.48409194 * 50000; errs = 16.508% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.2738s
Finished Epoch[40 of 160]: [Training] ce = 0.48409194 * 50000; errs = 16.508% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.27474s
Finished Epoch[40 of 160]: [Training] ce = 0.48409194 * 50000; errs = 16.508% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.27625s
Finished Epoch[40 of 160]: [Training] ce = 0.48409194 * 50000; errs = 16.508% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.27382s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.49-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.49 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.50-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 1.50 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[41 of 160]: [Training] ce = 0.47036326 * 50000; errs = Finished Epoch[41 of 160]: [Training] ce = 0.47036326 * 50000; errs = 16.290% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.23125s
Finished Epoch[41 of 160]: [Training] ce = 0.47036326 * 50000; errs = 16.290% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.23126s
Finished Epoch[41 of 160]: [Training] ce = 0.47036326 * 50000; errs = 16.290% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.2312s
16.290% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.23122s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.93-seconds latency this time; accumulated time on sync point = 1.93 seconds , average latency = 1.93 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.78-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 1.78 seconds
Finished Epoch[42 of 160]: [Training] Finished Epoch[42 of 160]: [Training] ce = 0.47093538 * 50000; errs = Finished Epoch[42 of 160]: [Training] ce = 0.47093538 * 50000; errs = 16.114% * 50000; Finished Epoch[42 of 160]: [Training] ce = 0.47093538 * 50000; errs = 16.114% * 50000; totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.67811s
16.114% * 50000; totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.67812s
totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.67812s
ce = 0.47093538 * 50000; errs = 16.114% * 50000; totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.67811s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.72-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 1.72 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.20-seconds latency this time; accumulated time on sync point = 2.20 seconds , average latency = 2.20 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.77-seconds latency this time; accumulated time on sync point = 0.77 seconds , average latency = 0.77 seconds
Finished Epoch[43 of 160]: [Training] Finished Epoch[43 of 160]: [Training] ce = 0.45851984 * 50000; errs = 15.824% * 50000; totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=9.47452s
ce = 0.45851984 * 50000; errs = 15.824% * 50000; Finished Epoch[43 of 160]: [Training] ce = 0.45851984 * 50000; errs = 15.824% * 50000; totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=9.47491s
Finished Epoch[43 of 160]: [Training] ce = 0.45851984 * 50000; errs = 15.824% * 50000; totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=9.47491s
totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=9.47445s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.41-seconds latency this time; accumulated time on sync point = 0.41 seconds , average latency = 0.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
Finished Epoch[44 of 160]: [Training] Finished Epoch[44 of 160]: [Training] ce = 0.46117155 * 50000; errs = 15.732% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.49264s
Finished Epoch[44 of 160]: [Training] ce = 0.46117155 * 50000; errs = 15.732% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.49264s
Finished Epoch[44 of 160]: [Training] ce = 0.46117155 * 50000; errs = 15.732% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.49266s
ce = 0.46117155 * 50000; errs = 15.732% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.49263s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.57-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 1.57 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.77-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 1.77 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
Finished Epoch[45 of 160]: [Training] ce = 0.45481985 * 50000; errs = 15.726% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.63734s
Finished Epoch[45 of 160]: [Training] Finished Epoch[45 of 160]: [Training] ce = 0.45481985 * 50000; errs = 15.726% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.63735s
ce = 0.45481985 * 50000; errs = Finished Epoch[45 of 160]: [Training] ce = 0.45481985 * 50000; errs = 15.726% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.62907s
15.726% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.63735s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.02-seconds latency this time; accumulated time on sync point = 2.02 seconds , average latency = 2.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.71-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 1.71 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.06 seconds
Finished Epoch[46 of 160]: [Training] ce = 0.44085640 * 50000; errs = 15.262% * 50000; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=9.61854s
Finished Epoch[46 of 160]: [Training] ce = 0.44085640 * 50000; errs = 15.262% * 50000Finished Epoch[46 of 160]: [Training] ce = 0.44085640 * 50000; errs = 15.262% * 50000; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=9.61853s
Finished Epoch[46 of 160]: [Training] ; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=9.61854s
ce = 0.44085640 * 50000; errs = 15.262% * 50000; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=9.61828s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.52-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.46-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 1.46 seconds
Finished Epoch[47 of 160]: [Training] Finished Epoch[47 of 160]: [Training] ce = 0.44735759 * 50000; errs = 15.554% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=9.27778s
Finished Epoch[47 of 160]: [Training] ce = 0.44735759 * 50000; errs = 15.554% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=9.27774s
ce = 0.44735759Finished Epoch[47 of 160]: [Training] ce = 0.44735759 * 50000; errs = 15.554% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=9.27772s
 * 50000; errs = 15.554% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=9.27778s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.30-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.26-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 1.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.23-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 1.23 seconds
Finished Epoch[48 of 160]: [Training] Finished Epoch[48 of 160]: [Training] ce = 0.43819791 * 50000; errs = 15.200% * 50000; Finished Epoch[48 of 160]: [Training] ce = 0.43819791 * 50000; errs = 15.200% * 50000; totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.00787s
totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.00302s
Finished Epoch[48 of 160]: [Training] ce = 0.43819791 * 50000; errs = 15.200% * 50000; totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.00303s
ce = 0.43819791 * 50000; errs = 15.200% * 50000; totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.00703s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.22-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 1.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
Finished Epoch[49 of 160]: [Training] Finished Epoch[49 of 160]: [Training] ce = 0.43759698 * 50000; errs = Finished Epoch[49 of 160]: [Training] ce = 0.43759698 * 50000; errs = 15.096% * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=8.97955s
ce = 0.4375969815.096% * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=8.97955s
 * 50000; errs = 15.096% * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=8.97952s
Finished Epoch[49 of 160]: [Training] ce = 0.43759698 * 50000; errs = 15.096% * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=8.99553s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.07-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 1.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.85-seconds latency this time; accumulated time on sync point = 0.85 seconds , average latency = 0.85 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[50 of 160]: [Training] Finished Epoch[50 of 160]: [Training] ce = 0.43158610 * 50000; errs = 14.608% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=8.81802s
Finished Epoch[50 of 160]: [Training] ce = 0.43158610 * 50000; errs = 14.608% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=8.81802s
Finished Epoch[50 of 160]: [Training] ce = 0.43158610 * 50000; errs = 14.608% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=8.81802s
ce = 0.43158610 * 50000; errs = 14.608% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=8.81802s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.27-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.45-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 1.45 seconds
Finished Epoch[51 of 160]: [Training] ce = 0.42808714 * 50000; errs = Finished Epoch[51 of 160]: [Training] ce = 0.42808714 * 50000; errs = 14.718% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=9.14879s
Finished Epoch[51 of 160]: [Training] ce = 0.42808714 * 50000; errs = 14.718% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=9.14879s
14.718% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=9.14881s
Finished Epoch[51 of 160]: [Training] ce = 0.42808714 * 50000; errs = 14.718% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=9.14876s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.87-seconds latency this time; accumulated time on sync point = 1.87 seconds , average latency = 1.87 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.39-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[52 of 160]: [Training] ce = 0.42484000 * 50000; errs = 14.724% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.18319s
Finished Epoch[52 of 160]: [Training] ce = 0.42484000 * 50000; errs = 14.724% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.18319s
Finished Epoch[52 of 160]: [Training] ce = 0.42484000 * 50000; errs = 14.724% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.18319s
Finished Epoch[52 of 160]: [Training] ce = 0.42484000 * 50000; errs = 14.724% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.18319s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.28-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.28 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.94-seconds latency this time; accumulated time on sync point = 1.94 seconds , average latency = 1.94 seconds
Finished Epoch[53 of 160]: [Training] ce = 0.41548050 * 50000; errs = 14.252% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.48894s
Finished Epoch[53 of 160]: [Training] Finished Epoch[53 of 160]: [Training] ce = 0.41548050 * 50000; errs = 14.252% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.48893s
Finished Epoch[53 of 160]: [Training] ce = 0.41548050 * 50000; errs = 14.252% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.48754s
ce = 0.41548050 * 50000; errs = 14.252% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.48754s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.32-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 1.32 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.37-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.37 seconds
Finished Epoch[54 of 160]: [Training] ce = 0.42545105 * 50000; errs = 14.744% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.40732s
Finished Epoch[54 of 160]: [Training] Finished Epoch[54 of 160]: [Training] ce = 0.42545105 * 50000; errs = 14.744% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.40732s
Finished Epoch[54 of 160]: [Training] ce = 0.42545105 * 50000; errs = 14.744% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.40733s
ce = 0.42545105 * 50000; errs = 14.744% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.40733s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.79-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 1.79 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.98-seconds latency this time; accumulated time on sync point = 1.98 seconds , average latency = 1.98 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.34-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.34 seconds
Finished Epoch[55 of 160]: [Training] ce = 0.41628179 * 50000; errs = 14.256% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.45918s
Finished Epoch[55 of 160]: [Training] ce = 0.41628179 * 50000; errs = 14.256% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.46205s
Finished Epoch[55 of 160]: [Training] Finished Epoch[55 of 160]: [Training] ce = 0.41628179 * 50000; errs = 14.256% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.46205s
ce = 0.41628179 * 50000; errs = 14.256% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.46205s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.83-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 1.83 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.65-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 1.65 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.22-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.22 seconds
Finished Epoch[56 of 160]: [Training] Finished Epoch[56 of 160]: [Training] ce = Finished Epoch[56 of 160]: [Training] ce = 0.41846971 * 50000; errs = 0.41846971 * 50000; errs = 14.422% * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.55699s
14.422% * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.55698s
ce = Finished Epoch[56 of 160]: [Training] ce = 0.41846971 * 50000; errs = 14.422% * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.55734s
0.41846971 * 50000; errs = 14.422% * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.55696s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.09-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 1.09 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.60-seconds latency this time; accumulated time on sync point = 0.60 seconds , average latency = 0.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.21-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 1.21 seconds
Finished Epoch[57 of 160]: [Training] ce = 0.41658362 * 50000; errs = 14.294% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=9.07959s
Finished Epoch[57 of 160]: [Training] ce = 0.41658362 * 50000; errs = 14.294% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=9.0796s
Finished Epoch[57 of 160]: [Training] ce = 0.41658362 * 50000; errs = 14.294% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=9.07959s
Finished Epoch[57 of 160]: [Training] ce = 0.41658362 * 50000; errs = 14.294% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=9.07959s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.19 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.04-seconds latency this time; accumulated time on sync point = 2.04 seconds , average latency = 2.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[58 of 160]: [Training] ce = 0.41528815 * 50000; errs = 14.322% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=9.41058s
Finished Epoch[58 of 160]: [Training] ce = 0.41528815 * 50000; errs = 14.322% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=9.41057s
Finished Epoch[58 of 160]: [Training] ce = 0.41528815 * 50000; errs = 14.322% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=9.41058s
Finished Epoch[58 of 160]: [Training] ce = 0.41528815 * 50000; errs = 14.322% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=9.41056s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.17-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 1.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.21 seconds
Finished Epoch[59 of 160]: [Training] ce = 0.41838905 * 50000; errs = 14.448% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=9.14995s
Finished Epoch[59 of 160]: [Training] ce = 0.41838905 * 50000; errs = 14.448% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=9.14995s
Finished Epoch[59 of 160]: [Training] ce = 0.41838905 * 50000; errs = 14.448% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=9.14995s
Finished Epoch[59 of 160]: [Training] ce = 0.41838905 * 50000; errs = 14.448% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=9.14994s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.37-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.24-seconds latency this time; accumulated time on sync point = 1.24 seconds , average latency = 1.24 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.28-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 1.28 seconds
Finished Epoch[60 of 160]: [Training] ce = 0.40896017 * 50000; errs = 13.994% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=8.83879s
Finished Epoch[60 of 160]: [Training] ce = 0.40896017Finished Epoch[60 of 160]: [Training] ce = 0.40896017 * 50000; errs = 13.994% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=8.83903s
Finished Epoch[60 of 160]: [Training]  * 50000; errs = 13.994% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=8.83878s
ce = 0.40896017 * 50000; errs = 13.994% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=8.83904s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.28-seconds latency this time; accumulated time on sync point = 1.28 seconds , average latency = 1.28 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.37-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 1.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
Finished Epoch[61 of 160]: [Training] Finished Epoch[61 of 160]: [Training] ce = 0.39911421 * 50000; errs = 13.762% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=8.82415s
Finished Epoch[61 of 160]: [Training] ce = 0.39911421 * 50000; errs = 13.762% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=8.82415s
Finished Epoch[61 of 160]: [Training] ce = 0.39911421 * 50000; errs = 13.762% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=8.82415s
ce = 0.39911421 * 50000; errs = 13.762% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=8.82415s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.78-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 1.78 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.04-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 1.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.42-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.42 seconds
Finished Epoch[62 of 160]: [Training] ce = 0.40659047 * 50000; errs = 14.062% * 50000; totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.22594s
Finished Epoch[62 of 160]: [Training] ce = 0.40659047 * 50000; errs = 14.062% * 50000; totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.22601s
Finished Epoch[62 of 160]: [Training] ce = 0.40659047 * 50000; errs = 14.062% * 50000; totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.22594s
Finished Epoch[62 of 160]: [Training] ce = 0.40659047 * 50000; errs = 14.062% * 50000; totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.22093s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.69-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 1.69 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.59-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 1.59 seconds
Finished Epoch[63 of 160]: [Training] ce = 0.38867254 * 50000; errs = 13.470% * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=9.23743s
Finished Epoch[63 of 160]: [Training] ce = 0.38867254 * 50000; errs = 13.470% * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=9.23744s
Finished Epoch[63 of 160]: [Training] ce = 0.38867254 * 50000; errs = 13.470% * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=9.23743s
Finished Epoch[63 of 160]: [Training] ce = 0.38867254 * 50000; errs = 13.470% * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=9.23742s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.20-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 1.20 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.22-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 1.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.47-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.47 seconds
Finished Epoch[64 of 160]: [Training] ce = 0.40543470 * 50000; errs = 14.118% * 50000; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=8.79119s
Finished Epoch[64 of 160]: [Training] ce = 0.40543470 * 50000; errs = 14.118% * 50000; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=8.7912s
Finished Epoch[64 of 160]: [Training] ce = 0.40543470 * 50000; errs = 14.118% * 50000; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=8.7912s
Finished Epoch[64 of 160]: [Training] ce = 0.40543470 * 50000; errs = 14.118% * 50000; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=8.7912s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.33-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 1.33 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.33-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 1.33 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.46-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[65 of 160]: [Training] ce = 0.39203314 * 50000; errs = Finished Epoch[65 of 160]: [Training] ce = 0.39203314 * 50000; errs = 13.440% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.14328s
Finished Epoch[65 of 160]: [Training] Finished Epoch[65 of 160]: [Training] ce = 0.39203314 * 50000; errs = 13.440% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.14329s
13.440% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.14329s
ce = 0.39203314 * 50000; errs = 13.440% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.14328s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.67-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 1.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.24-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.24 seconds
Finished Epoch[66 of 160]: [Training] Finished Epoch[66 of 160]: [Training] ce = 0.39543007 * 50000; errs = 13.484% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.39643s
ce = 0.39543007 * 50000Finished Epoch[66 of 160]: [Training] ce = 0.39543007 * 50000; errs = 13.484% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.39646s
Finished Epoch[66 of 160]: [Training] ce = 0.39543007 * 50000; errs = 13.484% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.3967s
; errs = 13.484% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.39641s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.80-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 1.80 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.15-seconds latency this time; accumulated time on sync point = 2.15 seconds , average latency = 2.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.52-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.52 seconds
Finished Epoch[67 of 160]: [Training] Finished Epoch[67 of 160]: [Training] ce = 0.39618468 * 50000; errs = 13.738%Finished Epoch[67 of 160]: [Training] ce = 0.39618468 * 50000; errs = 13.738% * 50000; Finished Epoch[67 of 160]: [Training] ce = 0.39618468 * 50000; errs = 13.738% * 50000; totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.71106s
 * 50000; totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.71107s
totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.71107s
ce = 0.39618468 * 50000; errs = 13.738% * 50000; totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.71106s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.49-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.49 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.67-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 1.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.21-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 1.21 seconds
Finished Epoch[68 of 160]: [Training] Finished Epoch[68 of 160]: [Training] ce = 0.38729187 * 50000; errs = 13.326% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.20093s
Finished Epoch[68 of 160]: [Training] ce = 0.38729187 * 50000; errs = 13.326% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.20086s
Finished Epoch[68 of 160]: [Training] ce = 0.38729187 * 50000; errs = 13.326% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.20085s
ce = 0.38729187 * 50000; errs = 13.326% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.20087s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.33-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 1.33 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.33-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 1.33 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.41-seconds latency this time; accumulated time on sync point = 0.41 seconds , average latency = 0.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
Finished Epoch[69 of 160]: [Training] ce = 0.39421113 * 50000; errs = Finished Epoch[69 of 160]: [Training] ce = 0.39421113 * 50000; errs = 13.602% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.05215s
Finished Epoch[69 of 160]: [Training] 13.602% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.05215s
Finished Epoch[69 of 160]: [Training] ce = 0.39421113 * 50000; errs = 13.602% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.05216s
ce = 0.39421113 * 50000; errs = 13.602% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.05215s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.16-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 1.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.46-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 1.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
Finished Epoch[70 of 160]: [Training] ce = 0.38533485 * 50000; errs = 13.340% * 50000; totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.04913s
Finished Epoch[70 of 160]: [Training] Finished Epoch[70 of 160]: [Training] ce = 0.38533485 * 50000; errs = 13.340% * 50000; totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.04912s
Finished Epoch[70 of 160]: [Training] ce = 0.38533485 * 50000; errs = ce = 13.340% * 50000; 0.38533485 * 50000; errs = totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.04547s
13.340% * 50000; totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.04916s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.84-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.84 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.60-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 1.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[71 of 160]: [Training] ce = 0.38337367 * 50000; errs = 13.370% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.48608s
Finished Epoch[71 of 160]: [Training] ce = 0.38337367 * 50000; errs = 13.370% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.48603s
Finished Epoch[71 of 160]: [Training] ce = 0.38337367 * 50000; Finished Epoch[71 of 160]: [Training] ce = 0.38337367 * 50000; errs = 13.370% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.48616s
errs = 13.370% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.48603s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[72 of 160]: [Training] Finished Epoch[72 of 160]: [Training] ce = 0.38011852 * 50000; errs = 13.186% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=8.88468s
Finished Epoch[72 of 160]: [Training] ce = 0.38011852 * 50000; errs = 13.186% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=8.88468s
Finished Epoch[72 of 160]: [Training] ce = 0.38011852 * 50000; errs = 13.186% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=8.8847s
ce = 0.38011852 * 50000; errs = 13.186% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=8.88467s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.62-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 1.62 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.67-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 1.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
Finished Epoch[73 of 160]: [Training] ce = 0.38979234 * 50000; errs = 13.398% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.61308s
Finished Epoch[73 of 160]: [Training] ce = 0.38979234 * 50000; errs = 13.398% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.61308s
Finished Epoch[73 of 160]: [Training] ce = 0.38979234 * 50000; errs = 13.398% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.61306s
Finished Epoch[73 of 160]: [Training] ce = 0.38979234 * 50000; errs = 13.398% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.61306s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
Finished Epoch[74 of 160]: [Training] ce = 0.39346795 * 50000; errs = 13.638% * 50000; Finished Epoch[74 of 160]: [Training] ce = 0.39346795 * 50000; errs = 13.638% * 50000; totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=8.99116s
Finished Epoch[74 of 160]: [Training] Finished Epoch[74 of 160]: [Training] ce = 0.39346795 * 50000; errs = 13.638% * 50000; totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=8.99116s
totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=8.99116s
ce = 0.39346795 * 50000; errs = 13.638% * 50000; totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=8.99115s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.19-seconds latency this time; accumulated time on sync point = 1.19 seconds , average latency = 1.19 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.26-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[75 of 160]: [Training] ce = 0.38197807 * 50000; errs = Finished Epoch[75 of 160]: [Training] Finished Epoch[75 of 160]: [Training] ce = 0.38197807 * 50000; errs = 13.094% * 50000; totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=8.85342s
13.094% * 50000; totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=8.85342s
Finished Epoch[75 of 160]: [Training] ce = 0.38197807 * 50000; errs = 13.094% * 50000; totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=8.85345s
ce = 0.38197807 * 50000; errs = 13.094% * 50000; totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=8.85343s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.21-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 1.21 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.34-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.75-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.75 seconds
Finished Epoch[76 of 160]: [Training] ce = 0.37808690 * 50000; errs = 13.120% * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=8.97343s
Finished Epoch[76 of 160]: [Training] ce = 0.37808690 * 50000; errs = 13.120% * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=8.97343s
Finished Epoch[76 of 160]: [Training] ce = 0.37808690 * 50000; errs = 13.120% * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=8.97277s
Finished Epoch[76 of 160]: [Training] ce = 0.37808690 * 50000; errs = 13.120% * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=8.97343s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.89-seconds latency this time; accumulated time on sync point = 1.89 seconds , average latency = 1.89 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.36-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.36 seconds
Finished Epoch[77 of 160]: [Training] ce = 0.38977357 * 50000; errs = 13.506% * 50000; totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=9.43773s
Finished Epoch[77 of 160]: [Training] Finished Epoch[77 of 160]: [Training] ce = 0.38977357 * 50000; errs = 13.506% * 50000; totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=9.43773s
Finished Epoch[77 of 160]: [Training] ce = 0.38977357 * 50000; errs = 13.506% * 50000; totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=9.43783s
ce = 0.38977357 * 50000; errs = 13.506% * 50000; totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=9.43772s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.26-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 1.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.27-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 1.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.32-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.32 seconds
Finished Epoch[78 of 160]: [Training] ce = 0.38149754 * 50000; errs = 13.140% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=8.96951s
Finished Epoch[78 of 160]: [Training] ce = 0.38149754 * 50000; errs = 13.140% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=8.96951s
Finished Epoch[78 of 160]: [Training] ce = 0.38149754 * 50000; errs = 13.140% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=8.96954s
Finished Epoch[78 of 160]: [Training] ce = 0.38149754 * 50000; errs = 13.140% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=8.99016s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.37-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.48-seconds latency this time; accumulated time on sync point = 2.48 seconds , average latency = 2.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.24-seconds latency this time; accumulated time on sync point = 2.24 seconds , average latency = 2.24 seconds
Finished Epoch[79 of 160]: [Training] ce = 0.37604377 * 50000; errs = 12.936% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.98639s
Finished Epoch[79 of 160]: [Training] ce = 0.37604377 * 50000; errs = 12.936% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.98638s
Finished Epoch[79 of 160]: [Training] ce = 0.37604377 * 50000; errs = 12.936% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.98637s
Finished Epoch[79 of 160]: [Training] ce = 0.37604377 * 50000; errs = 12.936% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.9864s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.67-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.78-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.78 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[80 of 160]: [Training] ce = 0.37069771 * 50000; errs = 12.822% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=8.61343s
Finished Epoch[80 of 160]: [Training] ce = 0.37069771 * 50000; errs = 12.822% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=8.61343s
Finished Epoch[80 of 160]: [Training] ce = 0.37069771 * 50000; errs = 12.822% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=8.61348s
Finished Epoch[80 of 160]: [Training] ce = 0.37069771 * 50000; errs = 12.822% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=8.61353s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.50-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 1.50 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.17-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 1.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
Finished Epoch[81 of 160]: [Training] ce = 0.27706417 * 50000; errs = 9.606% * 50000; totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.23477s
Finished Epoch[81 of 160]: [Training] ce = 0.27706417 * 50000; errs = 9.606% * 50000; totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.23478s
Finished Epoch[81 of 160]: [Training] ce = 0.27706417 * 50000; errs = 9.606% * 50000; totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.23478s
Finished Epoch[81 of 160]: [Training] ce = 0.27706417 * 50000; errs = 9.606% * 50000; totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.23477s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.42-seconds latency this time; accumulated time on sync point = 2.42 seconds , average latency = 2.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.78-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 1.78 seconds
Finished Epoch[82 of 160]: [Training] ce = 0.32371641 * 50000; errs = 11.220% * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=9.80174s
Finished Epoch[82 of 160]: [Training] Finished Epoch[82 of 160]: [Training] ce = 0.32371641 * 50000; errs = 11.220% * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=9.80174s
Finished Epoch[82 of 160]: [Training] ce = 0.32371641 * 50000; errs = 11.220% * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=9.80176s
ce = 0.32371641 * 50000; errs = 11.220% * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=9.80174s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.73-seconds latency this time; accumulated time on sync point = 1.73 seconds , average latency = 1.73 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.27-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.27 seconds
Finished Epoch[83 of 160]: [Training] Finished Epoch[83 of 160]: [Training] ce = 0.34816294 * 50000; errs = 11.960% * 50000; totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=9.26639s
Finished Epoch[83 of 160]: [Training] ce = 0.34816294 * 50000; errs = 11.960% * 50000; Finished Epoch[83 of 160]: [Training] ce = 0.34816294 * 50000; errs = 11.960% * 50000; totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=9.26641s
totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=9.26641s
ce = 0.34816294 * 50000; errs = 11.960% * 50000; totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=9.26639s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.92-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.92 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.24-seconds latency this time; accumulated time on sync point = 1.24 seconds , average latency = 1.24 seconds
Finished Epoch[84 of 160]: [Training] ce = 0.33550067 * 50000; errs = 11.672% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=9.04318s
Finished Epoch[84 of 160]: [Training] ce = 0.33550067 * 50000; errs = 11.672% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=9.04319s
Finished Epoch[84 of 160]: [Training] ce = 0.33550067 * 50000; errs = 11.672% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=9.04307s
Finished Epoch[84 of 160]: [Training] ce = 0.33550067 * 50000; errs = 11.672% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=9.04318s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.00-seconds latency this time; accumulated time on sync point = 2.00 seconds , average latency = 2.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.04-seconds latency this time; accumulated time on sync point = 2.04 seconds , average latency = 2.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.95-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.95 seconds
Finished Epoch[85 of 160]: [Training] Finished Epoch[85 of 160]: [Training] ce = 0.31585544 * 50000; errs = 10.964% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.61187s
Finished Epoch[85 of 160]: [Training] ce = 0.31585544 * 50000; errs = 10.964% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.61186s
Finished Epoch[85 of 160]: [Training] ce = 0.31585544 * 50000; errs = 10.964% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.61188s
ce = 0.31585544 * 50000; errs = 10.964% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.61186s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.61-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 1.61 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.70-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 1.70 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.28-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.28 seconds
Finished Epoch[86 of 160]: [Training] ce = 0.29760173 * 50000; errs = 10.338% * 50000; totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.36538s
Finished Epoch[86 of 160]: [Training] ce = 0.29760173 * 50000; errs = 10.338% * 50000; totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.3654s
Finished Epoch[86 of 160]: [Training] ce = Finished Epoch[86 of 160]: [Training] ce = 0.29760173 * 50000; errs = 10.338% * 50000; totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.3655s
0.29760173 * 50000; errs = 10.338% * 50000; totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.36544s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.29-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.45-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 1.45 seconds
Finished Epoch[87 of 160]: [Training] ce = 0.27981699 * 50000; errs = 9.670% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.2341s
Finished Epoch[87 of 160]: [Training] ce = 0.27981699 * 50000; errs = 9.670% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.2341s
Finished Epoch[87 of 160]: [Training] Finished Epoch[87 of 160]: [Training] ce = 0.27981699 * 50000; errs = 9.670% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.2341s
ce = 0.27981699 * 50000; errs = 9.670% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.23409s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.09-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 1.09 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.04-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 1.04 seconds
Finished Epoch[88 of 160]: [Training] ce = 0.26389538 * 50000; errs = 9.180% * 50000; Finished Epoch[88 of 160]: [Training] Finished Epoch[88 of 160]: [Training] ce = 0.26389538 * 50000; errs = 9.180% * 50000; totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=8.7065s
Finished Epoch[88 of 160]: [Training] ce = 0.26389538 * 50000; errs = 9.180% * 50000; totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=8.70651s
totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=8.7065s
ce = 0.26389538 * 50000; errs = 9.180% * 50000; totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=8.7065s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.65-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 1.65 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.57-seconds latency this time; accumulated time on sync point = 0.57 seconds , average latency = 0.57 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[89 of 160]: [Training] Finished Epoch[89 of 160]: [Training] ce = 0.25115402 * 50000; errs = 8.742% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=9.35921s
Finished Epoch[89 of 160]: [Training] ce = 0.25115402 * 50000; errs = 8.742% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=9.35922s
Finished Epoch[89 of 160]: [Training] ce = 0.25115402 * 50000; errs = 8.742% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=9.3592s
ce = 0.25115402 * 50000; errs = 8.742% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=9.35919s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.10-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 1.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
Finished Epoch[90 of 160]: [Training] ce = 0.23924521 * 50000; Finished Epoch[90 of 160]: [Training] ce = 0.23924521 * 50000; errs = 8.356% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.05449s
Finished Epoch[90 of 160]: [Training] Finished Epoch[90 of 160]: [Training] ce = 0.23924521 * 50000; errs = 8.356% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.0545s
errs = 8.356% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.053s
ce = 0.23924521 * 50000; errs = 8.356% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.05299s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.32-seconds latency this time; accumulated time on sync point = 0.32 seconds , average latency = 0.32 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[91 of 160]: [Training] ce = 0.22194104 * 50000; errs = 7.802% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.37684s
Finished Epoch[91 of 160]: [Training] ce = 0.22194104 * 50000; errs = 7.802% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.37685s
Finished Epoch[91 of 160]: [Training] ce = 0.22194104 * 50000; errs = 7.802% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.37686s
Finished Epoch[91 of 160]: [Training] ce = 0.22194104 * 50000; errs = 7.802% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.37685s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
Finished Epoch[92 of 160]: [Training] ce = 0.21318673 * 50000; errs = 7.528% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.09569s
Finished Epoch[92 of 160]: [Training] ce = Finished Epoch[92 of 160]: [Training] ce = 0.21318673 * 50000; errs = 7.528% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.09568s
Finished Epoch[92 of 160]: [Training] ce = 0.21318673 * 50000; errs = 7.528% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.09594s
0.21318673 * 50000; errs = 7.528% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.09569s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.12-seconds latency this time; accumulated time on sync point = 2.12 seconds , average latency = 2.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.78-seconds latency this time; accumulated time on sync point = 0.78 seconds , average latency = 0.78 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.98-seconds latency this time; accumulated time on sync point = 1.98 seconds , average latency = 1.98 seconds
Finished Epoch[93 of 160]: [Training] ce = 0.20614208 * 50000; errs = 7.202% * 50000; totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=9.83465s
Finished Epoch[93 of 160]: [Training] Finished Epoch[93 of 160]: [Training] ce = ce = 0.20614208 * 50000; errs = 7.202% * 50000Finished Epoch[93 of 160]: [Training] ce = 0.20614208 * 50000; errs = 7.202% * 50000; totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=9.8349s
; 0.20614208 * 50000; errs = 7.202% * 50000; totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=9.83467s
totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=9.83465s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.36-seconds latency this time; accumulated time on sync point = 2.36 seconds , average latency = 2.36 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.92-seconds latency this time; accumulated time on sync point = 0.92 seconds , average latency = 0.92 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.20-seconds latency this time; accumulated time on sync point = 2.20 seconds , average latency = 2.20 seconds
Finished Epoch[94 of 160]: [Training] Finished Epoch[94 of 160]: [Training] ce = 0.19548064 * 50000; errs = 6.914% * 50000; totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.98687s
Finished Epoch[94 of 160]: [Training] ce = 0.19548064 * 50000; errs = 6.914% * 50000; totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.99396s
Finished Epoch[94 of 160]: [Training] ce = 0.19548064 * 50000; errs = 6.914% * 50000; totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.99393s
ce = 0.19548064 * 50000; errs = 6.914% * 50000; totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.994s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.57-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 1.57 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.40-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 1.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.52-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.52 seconds
Finished Epoch[95 of 160]: [Training] ce = 0.18927383 * 50000; errs = 6.560% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.34798s
Finished Epoch[95 of 160]: [Training] Finished Epoch[95 of 160]: [Training] ce = 0.18927383 * 50000; errs = 6.560% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.3551s
Finished Epoch[95 of 160]: [Training] ce = 0.18927383 * 50000; errs = 6.560% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.3551s
ce = 0.18927383 * 50000; errs = 6.560% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.35513s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.65-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 1.65 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
Finished Epoch[96 of 160]: [Training] ce = 0.18073603 * 50000; errs = 6.294% * 50000; totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=9.62302s
Finished Epoch[96 of 160]: [Training] Finished Epoch[96 of 160]: [Training] ce = 0.18073603 * 50000; errs = 6.294% * 50000; totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=9.62303s
Finished Epoch[96 of 160]: [Training] ce = 0.18073603 * 50000; errs = 6.294% * 50000; totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=9.62302s
ce = 0.18073603 * 50000; errs = 6.294% * 50000; totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=9.62301s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.01-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 1.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.11-seconds latency this time; accumulated time on sync point = 1.11 seconds , average latency = 1.11 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.16 seconds
Finished Epoch[97 of 160]: [Training] ce = 0.17298957 * 50000; errs = 5.914% * 50000; totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=8.98579s
Finished Epoch[97 of 160]: [Training] ce = 0.17298957 * 50000; errs = 5.914% * 50000; totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=8.98579s
Finished Epoch[97 of 160]: [Training] Finished Epoch[97 of 160]: [Training] ce = 0.17298957 * 50000; errs = 5.914% * 50000; totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=8.98556s
ce = 0.17298957 * 50000; errs = 5.914% * 50000; totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=8.98579s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.72-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 1.72 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.20-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.20 seconds
Finished Epoch[98 of 160]: [Training] ce = 0.17298092 * 50000; errs = 6.068% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=9.46908s
Finished Epoch[98 of 160]: [Training] ce = 0.17298092 * 50000; errs = 6.068% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=9.46908s
Finished Epoch[98 of 160]: [Training] ce = 0.17298092 * 50000; errs = 6.068% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=9.46909s
Finished Epoch[98 of 160]: [Training] ce = 0.17298092 * 50000; errs = 6.068% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=9.46909s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.72-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 1.72 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.44-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.44 seconds
Finished Epoch[99 of 160]: [Training] ce = 0.16914579 * 50000; errs = 5.938% * 50000; totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=9.13231s
Finished Epoch[99 of 160]: [Training] ce = 0.16914579 * 50000; errs = 5.938% * 50000; totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=9.13231s
Finished Epoch[99 of 160]: [Training] ce = 0.16914579 * 50000; errs = 5.938% * 50000; totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=9.12576s
Finished Epoch[99 of 160]: [Training] ce = 0.16914579 * 50000; errs = 5.938% * 50000; totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=9.13231s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.62-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 1.62 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
Finished Epoch[100 of 160]: [Training] ce = 0.16029574 * 50000; errs = 5.650% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.32004s
Finished Epoch[100 of 160]: [Training] ce = 0.16029574 * 50000; errs = 5.650% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.32004s
Finished Epoch[100 of 160]: [Training] ce = 0.16029574 * 50000; errs = 5.650% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.32007s
Finished Epoch[100 of 160]: [Training] ce = 0.16029574 * 50000; errs = 5.650% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.31648s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.61-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 1.61 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.45-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.45 seconds
Finished Epoch[101 of 160]: [Training] ce = 0.15867831 * 50000; Finished Epoch[101 of 160]: [Training] Finished Epoch[101 of 160]: [Training] ce = 0.15867831 * 50000; errs = 5.650% * 50000; Finished Epoch[101 of 160]: [Training] ce = 0.15867831 * 50000; errs = totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=10.185s
errs = 5.650% * 50000; totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=10.185s
5.650% * 50000; totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=10.185s
ce = 0.15867831 * 50000; errs = 5.650% * 50000; totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=10.185s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.27-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 1.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.31-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 1.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
Finished Epoch[102 of 160]: [Training] ce = 0.15275968 * 50000; errs = 5.444% * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.05228s
Finished Epoch[102 of 160]: [Training] ce = 0.15275968 * 50000; errs = 5.444% * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.05235s
Finished Epoch[102 of 160]: [Training] Finished Epoch[102 of 160]: [Training] ce = 0.15275968 * 50000; errs = 5.444% * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.0523s
ce = 0.15275968 * 50000; errs = 5.444% * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.05235s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.51-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[103 of 160]: [Training] ce = 0.14902550 * 50000; errs = 5.260% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.2613s
Finished Epoch[103 of 160]: [Training] ce = 0.14902550 * 50000; errs = 5.260% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.2613s
Finished Epoch[103 of 160]: [Training] ce = 0.14902550 * 50000; errs = 5.260% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.2613s
Finished Epoch[103 of 160]: [Training] ce = 0.14902550 * 50000; errs = 5.260% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.26131s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[104 of 160]: [Training] ce = 0.14684303 * 50000; errs = 5.106% * 50000; totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=9.73642s
Finished Epoch[104 of 160]: [Training] ce = 0.14684303 * 50000; errs = 5.106% * 50000; totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=9.73642s
Finished Epoch[104 of 160]: [Training] Finished Epoch[104 of 160]: [Training] ce = 0.14684303 * 50000; errs = 5.106% * 50000; totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=9.73642s
ce = 0.14684303 * 50000; errs = 5.106% * 50000; totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=9.73207s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.48-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 1.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.78-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 1.78 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.16 seconds
Finished Epoch[105 of 160]: [Training] ce = 0.14220218 * 50000; errs = 4.956% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.33648s
Finished Epoch[105 of 160]: [Training] ce = 0.14220218 * 50000; errs = 4.956% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.33648s
Finished Epoch[105 of 160]: [Training] Finished Epoch[105 of 160]: [Training] ce = 0.14220218 * 50000; errs = 4.956% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.3365s
ce = 0.14220218 * 50000; errs = 4.956% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.33648s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.36-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 1.36 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.27-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 1.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.27-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.27 seconds
Finished Epoch[106 of 160]: [Training] Finished Epoch[106 of 160]: [Training] ce = 0.14475519 * 50000; errs = 5.098% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.29819s
Finished Epoch[106 of 160]: [Training] ce = 0.14475519 * 50000; errs = 5.098% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.29815s
Finished Epoch[106 of 160]: [Training] ce = 0.14475519 * 50000; errs = 5.098% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.29824s
ce = 0.14475519 * 50000; errs = 5.098% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.29814s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.05-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 1.05 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.33-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 1.33 seconds
Finished Epoch[107 of 160]: [Training] ce = 0.13456471 * 50000; errs = 4.666% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.07017s
Finished Epoch[107 of 160]: [Training] Finished Epoch[107 of 160]: [Training] ce = Finished Epoch[107 of 160]: [Training] ce = 0.13456471 * 50000; errs = 4.666% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.07017s
0.13456471ce = 0.13456471 * 50000; errs = 4.666% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.07017s
 * 50000; errs = 4.666% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.07017s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.60-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 1.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.86-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 1.86 seconds
Finished Epoch[108 of 160]: [Training] ce = 0.13501773 * 50000; errs = 4.758% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.37924s
Finished Epoch[108 of 160]: [Training] ce = 0.13501773 * 50000; errs = 4.758% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.37917s
Finished Epoch[108 of 160]: [Training] ce = 0.13501773 * 50000; errs = 4.758% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.37924s
Finished Epoch[108 of 160]: [Training] ce = 0.13501773 * 50000; errs = 4.758% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.37916s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.40-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.76-seconds latency this time; accumulated time on sync point = 1.76 seconds , average latency = 1.76 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.97-seconds latency this time; accumulated time on sync point = 1.97 seconds , average latency = 1.97 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[109 of 160]: [Training] ce = 0.12660481 * 50000; errs = 4.508% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.64037s
Finished Epoch[109 of 160]: [Training] ce = 0.12660481 * 50000; errs = 4.508% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.64037s
Finished Epoch[109 of 160]: [Training] Finished Epoch[109 of 160]: [Training] ce = 0.12660481 * 50000; errs = 4.508% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.64036s
ce = 0.12660481 * 50000; errs = 4.508% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.64036s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.89-seconds latency this time; accumulated time on sync point = 1.89 seconds , average latency = 1.89 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.07 seconds
Finished Epoch[110 of 160]: [Training] ce = 0.12569479 * 50000; errs = 4.422% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.5424s
Finished Epoch[110 of 160]: [Training] Finished Epoch[110 of 160]: [Training] ce = 0.12569479 * 50000; errs = 4.422% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.54238s
Finished Epoch[110 of 160]: [Training] ce = 0.12569479 * 50000; errs = 4.422% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.54237s
ce = 0.12569479 * 50000; errs = 4.422% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.54238s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.40-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 1.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.04-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 1.04 seconds
Finished Epoch[111 of 160]: [Training] ce = 0.12465023 * 50000; errs = 4.374% * 50000; totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=8.99834s
Finished Epoch[111 of 160]: [Training] ce = 0.12465023Finished Epoch[111 of 160]: [Training] ce = 0.12465023 * 50000; errs = 4.374% * 50000; totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=8.99859s
Finished Epoch[111 of 160]: [Training] ce = 0.12465023 * 50000; errs = 4.374% * 50000; totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=8.9986s
 * 50000; errs = 4.374% * 50000; totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=8.99834s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.68-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.35-seconds latency this time; accumulated time on sync point = 2.35 seconds , average latency = 2.35 seconds
Finished Epoch[112 of 160]: [Training] Finished Epoch[112 of 160]: [Training] ce = 0.12697929 * 50000; errs = 4.468% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.86389s
Finished Epoch[112 of 160]: [Training] ce = 0.12697929 * 50000; errs = 4.468% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.8639s
Finished Epoch[112 of 160]: [Training] ce = 0.12697929 * 50000; errs = 4.468% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.86389s
ce = 0.12697929 * 50000; errs = 4.468% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.86389s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.18-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 1.18 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.93-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.93 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
Finished Epoch[113 of 160]: [Training] ce = 0.12692525 * 50000; errs = 4.530% * 50000; totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=8.82843s
Finished Epoch[113 of 160]: [Training] Finished Epoch[113 of 160]: [Training] ce = 0.12692525 * 50000; errs = 4.530% * 50000; totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=8.82843s
Finished Epoch[113 of 160]: [Training] ce = 0.12692525 * 50000; errs = 4.530% * 50000; totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=8.82843s
ce = 0.12692525 * 50000; errs = 4.530% * 50000; totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=8.82843s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[114 of 160]: [Training] ce = 0.12397140 * 50000; errs = 4.396% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.31732s
Finished Epoch[114 of 160]: [Training] Finished Epoch[114 of 160]: [Training] ce = 0.12397140 * 50000; errs = 4.396% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.31733s
Finished Epoch[114 of 160]: [Training] ce = 0.12397140 * 50000; errs = 4.396% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.31727s
ce = 0.12397140 * 50000; errs = 4.396% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.31732s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.25-seconds latency this time; accumulated time on sync point = 1.25 seconds , average latency = 1.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.75-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.32-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 1.32 seconds
Finished Epoch[115 of 160]: [Training] Finished Epoch[115 of 160]: [Training] ce = 0.11754616 * 50000; errs = 4.074% * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=9.14944s
Finished Epoch[115 of 160]: [Training] ce = 0.11754616 * 50000; errs = 4.074% * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=9.14944s
Finished Epoch[115 of 160]: [Training] ce = 0.11754616 * 50000; errs = 4.074% * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=9.14946s
ce = 0.11754616 * 50000; errs = 4.074% * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=9.14944s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.81-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.81 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.32-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 1.32 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.37-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 1.37 seconds
Finished Epoch[116 of 160]: [Training] ce = 0.11781571 * 50000; errs = 4.152% * 50000; Finished Epoch[116 of 160]: [Training] ce = 0.11781571 * 50000; errs = Finished Epoch[116 of 160]: [Training] Finished Epoch[116 of 160]: [Training] ce = 0.11781571 * 50000; errs = 4.152% * 50000; totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.19105s
4.152% * 50000; totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.19105s
totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.19105s
ce = 0.11781571 * 50000; errs = 4.152% * 50000; totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.19105s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.27-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 1.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.15-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 1.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.40-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.40 seconds
Finished Epoch[117 of 160]: [Training] ce = Finished Epoch[117 of 160]: [Training] ce = 0.11417825 * 50000; errs = 4.120% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.10707s
Finished Epoch[117 of 160]: [Training] ce = 0.11417825 * 50000; errs = 4.120% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.10707s
Finished Epoch[117 of 160]: [Training] ce = 0.11417825 * 50000; errs = 4.120% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.10707s
0.11417825 * 50000; errs = 4.120% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.10707s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.47-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 1.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.84-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 1.84 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
Finished Epoch[118 of 160]: [Training] ce = 0.11841583 * 50000; errs = 4.156% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=9.37079s
Finished Epoch[118 of 160]: [Training] ce = 0.11841583 * 50000; errs = 4.156% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=9.37078s
Finished Epoch[118 of 160]: [Training] Finished Epoch[118 of 160]: [Training] ce = 0.11841583 * 50000; errs = 4.156% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=9.3708s
ce = 0.11841583 * 50000; errs = 4.156% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=9.37078s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.36-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.36 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
Finished Epoch[119 of 160]: [Training] ce = 0.11623830 * 50000; errs = 4.132% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=9.37539s
Finished Epoch[119 of 160]: [Training] ce = 0.11623830 * 50000; errs = 4.132% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=9.37539s
Finished Epoch[119 of 160]: [Training] Finished Epoch[119 of 160]: [Training] ce = 0.11623830 * 50000; errs = 4.132% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=9.37539s
ce = 0.11623830 * 50000; errs = 4.132% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=9.37539s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.32-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 1.32 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.09-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 1.09 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[120 of 160]: [Training] ce = 0.11515858 * 50000; errs = 4.042% * 50000; totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.15472s
Finished Epoch[120 of 160]: [Training] ce = 0.11515858 * 50000; errs = 4.042% * 50000; totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.15473s
Finished Epoch[120 of 160]: [Training] Finished Epoch[120 of 160]: [Training] ce = 0.11515858 * 50000; errs = 4.042% * 50000; totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.15473s
ce = 0.11515858 * 50000; errs = 4.042% * 50000; totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.15472s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.41-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 1.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.23-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 1.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
Finished Epoch[121 of 160]: [Training] ce = 0.09573329 * 50000; errs = Finished Epoch[121 of 160]: [Training] ce = 0.09573329 * 50000; errs = 3.298% * 50000; totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.16967s
Finished Epoch[121 of 160]: [Training] ce = 0.09573329 * 50000; errs = 3.298% * 50000; totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.16969s
Finished Epoch[121 of 160]: [Training] 3.298% * 50000; totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.16967s
ce = 0.09573329 * 50000; errs = 3.298% * 50000; totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.16968s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.71-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 1.71 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.21 seconds
Finished Epoch[122 of 160]: [Training] ce = 0.11238357 * 50000; errs = 3.966% * 50000; Finished Epoch[122 of 160]: [Training] totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.10515s
ce = 0.11238357Finished Epoch[122 of 160]: [Training] ce = 0.11238357 * 50000; errs = 3.966% * 50000; totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.1052s
Finished Epoch[122 of 160]: [Training] ce = 0.11238357 * 50000; errs = 3.966% * 50000; totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.10561s
 * 50000; errs = 3.966% * 50000; totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.10515s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.46-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.97-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.97 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.97-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.97 seconds
Finished Epoch[123 of 160]: [Training] ce = 0.12230352 * 50000; errs = 4.352% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=9.00369s
Finished Epoch[123 of 160]: [Training] Finished Epoch[123 of 160]: [Training] ce = 0.12230352 * 50000; errs = 4.352% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=9.00369s
Finished Epoch[123 of 160]: [Training] ce = 0.12230352 * 50000; errs = 4.352% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=9.00371s
ce = 0.12230352 * 50000; errs = 4.352% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=9.00369s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.35-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.50-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 1.50 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[124 of 160]: [Training] ce = 0.11778238 * 50000; errs = Finished Epoch[124 of 160]: [Training] ce = 0.11778238 * 50000; errs = 4.176% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.23978s
Finished Epoch[124 of 160]: [Training] Finished Epoch[124 of 160]: [Training] ce = 0.11778238 * 50000; errs = 4.176% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.2398s
4.176% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.23993s
ce = 0.11778238 * 50000; errs = 4.176% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.23992s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.21-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 1.21 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.15-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 1.15 seconds
Finished Epoch[125 of 160]: [Training] Finished Epoch[125 of 160]: [Training] ce = 0.11049946 * 50000; errs = 3.974% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=8.82752s
Finished Epoch[125 of 160]: [Training] ce = 0.11049946 * 50000; errs = 3.974% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=8.82752s
Finished Epoch[125 of 160]: [Training] ce = 0.11049946 * 50000; errs = 3.974% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=8.82752s
ce = 0.11049946 * 50000; errs = 3.974% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=8.82751s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.61-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 1.61 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.20-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.20 seconds
Finished Epoch[126 of 160]: [Training] ce = 0.10682840 * 50000; errs = 3.892% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.13167s
Finished Epoch[126 of 160]: [Training] Finished Epoch[126 of 160]: [Training] ce = 0.10682840 * 50000; errs = 3.892% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.13167s
Finished Epoch[126 of 160]: [Training] ce = 0.10682840 * 50000; errs = 3.892% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.13169s
ce = 0.10682840 * 50000; errs = 3.892% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.13167s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.39-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.76-seconds latency this time; accumulated time on sync point = 1.76 seconds , average latency = 1.76 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.84-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 1.84 seconds
Finished Epoch[127 of 160]: [Training] ce = 0.10232998 * 50000; errs = 3.592% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.61797s
Finished Epoch[127 of 160]: [Training] ce = 0.10232998 * 50000; errs = 3.592% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.61797s
Finished Epoch[127 of 160]: [Training] Finished Epoch[127 of 160]: [Training] ce = 0.10232998 * 50000; errs = 3.592% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.61797s
ce = 0.10232998 * 50000; errs = 3.592% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.61797s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.26-seconds latency this time; accumulated time on sync point = 2.26 seconds , average latency = 2.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.12-seconds latency this time; accumulated time on sync point = 2.12 seconds , average latency = 2.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.00-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 1.00 seconds
Finished Epoch[128 of 160]: [Training] ce = 0.09666651 * 50000; errs = 3.384% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=9.80122s
Finished Epoch[128 of 160]: [Training] ce = 0.09666651 * 50000; errs = 3.384% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=9.80122s
Finished Epoch[128 of 160]: [Training] ce = 0.09666651 * 50000; errs = 3.384% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=9.80123s
Finished Epoch[128 of 160]: [Training] ce = 0.09666651 * 50000; errs = 3.384% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=9.80124s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.41-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 1.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
Finished Epoch[129 of 160]: [Training] ce = 0.09388790 * 50000; errs = 3.284% * 50000; totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=9.17854s
Finished Epoch[129 of 160]: [Training] ce = 0.09388790 * 50000; errs = 3.284% * 50000; totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=9.17856s
Finished Epoch[129 of 160]: [Training] ce = 0.09388790 * 50000; errs = 3.284% * 50000; totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=9.17852s
Finished Epoch[129 of 160]: [Training] ce = 0.09388790 * 50000; errs = 3.284% * 50000; totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=9.17857s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.16-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 1.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.79-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 1.79 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[130 of 160]: [Training] ce = 0.08625978 * 50000; errs = 3.016% * 50000; totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.33023s
Finished Epoch[130 of 160]: [Training] Finished Epoch[130 of 160]: [Training] ce = 0.08625978 * 50000; errs = 3.016% * 50000; totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.33023s
Finished Epoch[130 of 160]: [Training] ce = ce = 0.08625978 * 50000; errs = 0.086259783.016% * 50000; totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.32687s
 * 50000; errs = 3.016% * 50000; totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.33022s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.03-seconds latency this time; accumulated time on sync point = 1.03 seconds , average latency = 1.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
Finished Epoch[131 of 160]: [Training] Finished Epoch[131 of 160]: [Training] ce = 0.08668318 * 50000; errs = 3.008% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=9.12269s
Finished Epoch[131 of 160]: [Training] ce = 0.08668318 * 50000; errs = 3.008% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=9.12266s
Finished Epoch[131 of 160]: [Training] ce = 0.08668318 * 50000; errs = 3.008% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=9.12269s
ce = 0.08668318 * 50000; errs = 3.008% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=9.12269s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.94-seconds latency this time; accumulated time on sync point = 1.94 seconds , average latency = 1.94 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.96-seconds latency this time; accumulated time on sync point = 1.96 seconds , average latency = 1.96 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.33-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.33 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[132 of 160]: [Training] ce = 0.08201484 * 50000; errs = 2.870% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=10.0032s
Finished Epoch[132 of 160]: [Training] ce = 0.08201484 * 50000; errs = 2.870% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=10.0032s
Finished Epoch[132 of 160]: [Training] Finished Epoch[132 of 160]: [Training] ce = 0.08201484 * 50000; errs = 2.870% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=10.0032s
ce = 0.08201484 * 50000; errs = 2.870% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=10.0032s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.38-seconds latency this time; accumulated time on sync point = 0.38 seconds , average latency = 0.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.67-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 1.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.95-seconds latency this time; accumulated time on sync point = 1.95 seconds , average latency = 1.95 seconds
Finished Epoch[133 of 160]: [Training] ce = 0.07824275 * 50000; errs = 2.620% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=9.46474s
Finished Epoch[133 of 160]: [Training] ce = 0.07824275 * 50000; errs = 2.620% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=9.46474s
Finished Epoch[133 of 160]: [Training] ce = 0.07824275 * 50000; errs = 2.620% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=9.46473s
Finished Epoch[133 of 160]: [Training] ce = 0.07824275 * 50000; errs = 2.620% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=9.46474s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.63-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 1.63 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.07-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 1.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.65-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.65 seconds
Finished Epoch[134 of 160]: [Training] ce = 0.07714670 * 50000; errs = 2.676% * 50000; totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.048s
Finished Epoch[134 of 160]: [Training] Finished Epoch[134 of 160]: [Training] ce = 0.07714670 * 50000; errs = 2.676% * 50000; totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.04799s
Finished Epoch[134 of 160]: [Training] ce = 0.07714670 * 50000; errs = 2.676% * 50000; totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.04801s
ce = 0.07714670 * 50000; errs = 2.676% * 50000; totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.04799s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.05-seconds latency this time; accumulated time on sync point = 1.05 seconds , average latency = 1.05 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.91-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.91 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.22-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.22 seconds
Finished Epoch[135 of 160]: [Training] Finished Epoch[135 of 160]: [Training] ce = 0.07425899 * 50000; errs = 2.478% * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=8.78523s
ce = 0.07425899 * 50000; errs = 2.478%Finished Epoch[135 of 160]: [Training] ce = 0.07425899 * 50000; errs = 2.478% * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=8.78522s
Finished Epoch[135 of 160]: [Training] ce = 0.07425899 * 50000; errs = 2.478% * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=8.78547s
 * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=8.78523s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.86-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 1.86 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.74-seconds latency this time; accumulated time on sync point = 0.74 seconds , average latency = 0.74 seconds
Finished Epoch[136 of 160]: [Training] ce = 0.07352472 * 50000; errs = 2.426% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.5388s
Finished Epoch[136 of 160]: [Training] ce = 0.07352472 * 50000; errs = 2.426% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.53882s
Finished Epoch[136 of 160]: [Training] Finished Epoch[136 of 160]: [Training] ce = 0.07352472 * 50000; errs = 2.426% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.53881s
ce = 0.07352472 * 50000; errs = 2.426% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.53881s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.43-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.60-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 1.60 seconds
Finished Epoch[137 of 160]: [Training] ce = 0.07417241 * 50000; errs = 2.536% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.36602s
Finished Epoch[137 of 160]: [Training] ce = 0.07417241 * 50000; errs = 2.536% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.36041s
Finished Epoch[137 of 160]: [Training] ce = 0.07417241 * 50000; errs = 2.536% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.36602s
Finished Epoch[137 of 160]: [Training] ce = 0.07417241 * 50000; errs = 2.536% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.3604s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.99-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.99 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.00-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 1.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
Finished Epoch[138 of 160]: [Training] ce = 0.07083325 * 50000; errs = 2.398% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=8.85523s
Finished Epoch[138 of 160]: [Training] ce = 0.07083325 * 50000; errs = 2.398% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=8.85523s
Finished Epoch[138 of 160]: [Training] Finished Epoch[138 of 160]: [Training] ce = 0.07083325 * 50000; errs = 2.398% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=8.85523s
ce = 0.07083325 * 50000; errs = 2.398% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=8.85523s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.33-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.33 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.03-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 2.03 seconds
Finished Epoch[139 of 160]: [Training] Finished Epoch[139 of 160]: [Training] ce = 0.06731477 * 50000; errs = 2.224% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.43411s
Finished Epoch[139 of 160]: [Training] ce = 0.06731477 * 50000; errs = 2.224% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.43411s
Finished Epoch[139 of 160]: [Training] ce = 0.06731477 * 50000; errs = 2.224% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.43412s
ce = 0.06731477 * 50000; errs = 2.224% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.4341s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[140 of 160]: [Training] ce = 0.06976257 * 50000; errs = 2.386% * 50000; totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.33325s
Finished Epoch[140 of 160]: [Training] ce = 0.06976257 * 50000; errs = 2.386% * 50000; totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.33124s
Finished Epoch[140 of 160]: [Training] ce = 0.06976257 * 50000; errs = 2.386% * 50000; totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.33325s
Finished Epoch[140 of 160]: [Training] ce = 0.06976257 * 50000; errs = 2.386% * 50000; totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.33325s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.97-seconds latency this time; accumulated time on sync point = 0.97 seconds , average latency = 0.97 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.88-seconds latency this time; accumulated time on sync point = 1.88 seconds , average latency = 1.88 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.40-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 1.40 seconds
Finished Epoch[141 of 160]: [Training] Finished Epoch[141 of 160]: [Training] ce = 0.06920736 * 50000; errs = 2.370%Finished Epoch[141 of 160]: [Training] ce = 0.06920736 * 50000; errs = 2.370% * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.39478s
Finished Epoch[141 of 160]: [Training] ce = 0.06920736 * 50000; errs = 2.370% * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.39476s
 * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.39477s
ce = 0.06920736 * 50000; errs = 2.370% * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.39477s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.70-seconds latency this time; accumulated time on sync point = 0.70 seconds , average latency = 0.70 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.14-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 1.14 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
Finished Epoch[142 of 160]: [Training] Finished Epoch[142 of 160]: [Training] ce = 0.06780076 * 50000; errs = 2.286% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=8.74404s
Finished Epoch[142 of 160]: [Training] ce = 0.06780076 * 50000; errs = 2.286% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=8.74403s
Finished Epoch[142 of 160]: [Training] ce = 0.06780076 * 50000; errs = 2.286% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=8.74403s
ce = 0.06780076 * 50000; errs = 2.286% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=8.73675s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.98-seconds latency this time; accumulated time on sync point = 1.98 seconds , average latency = 1.98 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.59-seconds latency this time; accumulated time on sync point = 0.59 seconds , average latency = 0.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.02-seconds latency this time; accumulated time on sync point = 2.02 seconds , average latency = 2.02 seconds
Finished Epoch[143 of 160]: [Training] ce = 0.06649503 * 50000; errs = 2.194% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.64445s
Finished Epoch[143 of 160]: [Training] ce = 0.06649503 * 50000; errs = 2.194% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.64445s
Finished Epoch[143 of 160]: [Training] Finished Epoch[143 of 160]: [Training] ce = 0.06649503 * 50000; errs = 2.194% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.64445s
ce = 0.06649503 * 50000; errs = 2.194% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.64445s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.20-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 1.20 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.40-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 1.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.16 seconds
Finished Epoch[144 of 160]: [Training] ce = 0.06516201 * 50000; errs = 2.184% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.12665s
Finished Epoch[144 of 160]: [Training] ce = 0.06516201 * 50000; errs = 2.184% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.12665s
Finished Epoch[144 of 160]: [Training] ce = 0.06516201 * 50000; errs = 2.184% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.12666s
Finished Epoch[144 of 160]: [Training] ce = 0.06516201 * 50000; errs = 2.184% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.11401s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.48-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 1.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.12-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 1.12 seconds
Finished Epoch[145 of 160]: [Training] ce = 0.06373031 * 50000; errs = 2.080% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.05923s
Finished Epoch[145 of 160]: [Training] ce = 0.06373031 * 50000; errs = 2.080% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.0591s
Finished Epoch[145 of 160]: [Training] ce = 0.06373031 * 50000; errs = 2.080% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.05305s
Finished Epoch[145 of 160]: [Training] ce = 0.06373031 * 50000; errs = 2.080% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.05912s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.39-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.25-seconds latency this time; accumulated time on sync point = 1.25 seconds , average latency = 1.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.06-seconds latency this time; accumulated time on sync point = 1.06 seconds , average latency = 1.06 seconds
Finished Epoch[146 of 160]: [Training] ce = 0.06409368 * 50000; errs = 2.174% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.26514s
Finished Epoch[146 of 160]: [Training] ce = 0.06409368 * 50000; errs = 2.174% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.26514s
Finished Epoch[146 of 160]: [Training] ce = 0.06409368 * 50000; errs = 2.174% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.26513s
Finished Epoch[146 of 160]: [Training] ce = 0.06409368 * 50000; errs = 2.174% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.26514s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
Finished Epoch[147 of 160]: [Training] ce = 0.06302707 * 50000; errs = 2.056% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.44254s
Finished Epoch[147 of 160]: [Training] ce = 0.06302707 * 50000; errs = 2.056% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.43804s
Finished Epoch[147 of 160]: [Training] Finished Epoch[147 of 160]: [Training] ce = 0.06302707 * 50000; errs = 2.056% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.43804s
ce = 0.06302707 * 50000; errs = 2.056% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.4352s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.40-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 1.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.28-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.28 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.61-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 1.61 seconds
Finished Epoch[148 of 160]: [Training] ce = 0.06171716 * 50000; errs = 2.042% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.27906s
Finished Epoch[148 of 160]: [Training] ce = 0.06171716 * 50000; errs = 2.042% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.27906s
Finished Epoch[148 of 160]: [Training] ce = 0.06171716 * 50000; errs = 2.042% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.27909s
Finished Epoch[148 of 160]: [Training] ce = 0.06171716 * 50000; errs = 2.042% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.27909s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.80-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 1.80 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.93-seconds latency this time; accumulated time on sync point = 1.93 seconds , average latency = 1.93 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
Finished Epoch[149 of 160]: [Training] Finished Epoch[149 of 160]: [Training] ce = 0.06232924 * 50000; errs = 2.114% * 50000; totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=9.57766s
Finished Epoch[149 of 160]: [Training] ce = 0.06232924 * 50000; errs = 2.114% * 50000; totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=9.57765s
Finished Epoch[149 of 160]: [Training] ce = 0.06232924 * 50000; errs = 2.114% * 50000; totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=9.57766s
ce = 0.06232924 * 50000; errs = 2.114% * 50000; totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=9.57766s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.26-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 1.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.57-seconds latency this time; accumulated time on sync point = 0.57 seconds , average latency = 0.57 seconds
Finished Epoch[150 of 160]: [Training] ce = 0.05995607 * 50000; errs = 2.014% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.12596s
Finished Epoch[150 of 160]: [Training] ce = 0.05995607 * 50000; errs = 2.014% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.12596s
Finished Epoch[150 of 160]: [Training] Finished Epoch[150 of 160]: [Training] ce = 0.05995607 * 50000; errs = 2.014% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.12621s
ce = 0.05995607 * 50000; errs = 2.014% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.12596s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.14 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.86-seconds latency this time; accumulated time on sync point = 1.86 seconds , average latency = 1.86 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
Finished Epoch[151 of 160]: [Training] ce = 0.06048568 * 50000; errs = 2.062% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.40743s
Finished Epoch[151 of 160]: [Training] ce = 0.06048568 * 50000; errs = 2.062% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.40743s
Finished Epoch[151 of 160]: [Training] ce = 0.06048568 * 50000; errs = 2.062% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.40743s
Finished Epoch[151 of 160]: [Training] ce = 0.06048568 * 50000; errs = 2.062% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.40742s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.84-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.84 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.70-seconds latency this time; accumulated time on sync point = 1.70 seconds , average latency = 1.70 seconds
Finished Epoch[152 of 160]: [Training] Finished Epoch[152 of 160]: [Training] ce = 0.05965636 * 50000; errs = 1.986% * 50000; totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=9.26459s
Finished Epoch[152 of 160]: [Training] ce = 0.05965636 * 50000; errs = 1.986% * 50000; totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=9.26459s
Finished Epoch[152 of 160]: [Training] ce = 0.05965636 * 50000; errs = 1.986% * 50000; totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=9.26459s
ce = 0.05965636 * 50000; errs = 1.986% * 50000; totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=9.26434s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.56-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.31-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 1.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
Finished Epoch[153 of 160]: [Training] ce = 0.06063470 * 50000; errs = 1.992% * 50000; totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.17866s
Finished Epoch[153 of 160]: [Training] Finished Epoch[153 of 160]: [Training] ce = 0.06063470 * 50000; errs = 1.992% * 50000; totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.17865s
Finished Epoch[153 of 160]: [Training] ce = 0.06063470 * 50000; errs = 1.992% * 50000; totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.17865s
ce = 0.06063470 * 50000; errs = 1.992% * 50000; totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.17865s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.30-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 1.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.98-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.98 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
Finished Epoch[154 of 160]: [Training] ce = 0.05870667 * 50000; errs = 1.954% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.20865s
Finished Epoch[154 of 160]: [Training] ce = 0.05870667 * 50000; errs = 1.954% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.20865s
Finished Epoch[154 of 160]: [Training] ce = 0.05870667 * 50000; errs = 1.954% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.20866s
Finished Epoch[154 of 160]: [Training] ce = 0.05870667 * 50000; errs = 1.954% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.20865s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.17-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 1.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.94-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.94 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[155 of 160]: [Training] ce = 0.05673701 * 50000; errs = 1.884% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=8.91636s
Finished Epoch[155 of 160]: [Training] ce = 0.05673701 * 50000; errs = 1.884% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=8.91636s
Finished Epoch[155 of 160]: [Training] ce = 0.05673701 * 50000; errs = 1.884% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=8.91636s
Finished Epoch[155 of 160]: [Training] ce = 0.05673701 * 50000; errs = 1.884% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=8.91635s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.67-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 1.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[156 of 160]: [Training] ce = 0.05736574 * 50000; errs = 1.816% * 50000; Finished Epoch[156 of 160]: [Training] ce = 0.05736574 * 50000; errs = 1.816% * 50000; totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.25232s
Finished Epoch[156 of 160]: [Training] ce = 0.05736574 * 50000; errs = 1.816% * 50000; totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.25222s
Finished Epoch[156 of 160]: [Training] totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.25222s
ce = 0.05736574 * 50000; errs = 1.816% * 50000; totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.25231s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.62-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 1.62 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.07 seconds
Finished Epoch[157 of 160]: [Training] ce = 0.05868235 * 50000; errs = 1.930% * 50000; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.37419s
Finished Epoch[157 of 160]: [Training] ce = 0.05868235 * 50000; errs = 1.930% * 50000; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.37417s
Finished Epoch[157 of 160]: [Training] Finished Epoch[157 of 160]: [Training] ce = 0.05868235 * 50000; errs = 1.930% * 50000; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.37419s
ce = 0.05868235 * 50000; errs = 1.930% * 50000; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.37418s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.00-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 1.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.50-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 1.50 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.06 seconds
Finished Epoch[158 of 160]: [Training] Finished Epoch[158 of 160]: [Training] ce = 0.05675188 * 50000; errs = 1.852% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=9.05719s
Finished Epoch[158 of 160]: [Training] ce = 0.05675188 * 50000; errs = 1.852% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=9.05718s
Finished Epoch[158 of 160]: [Training] ce = 0.05675188 * 50000; errs = 1.852% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=9.05719s
ce = 0.05675188 * 50000; errs = 1.852% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=9.04457s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.46-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 1.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.65-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 1.65 seconds
Finished Epoch[159 of 160]: [Training] ce = 0.05525911 * 50000; errs = 1.840% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=9.54637s
Finished Epoch[159 of 160]: [Training] Finished Epoch[159 of 160]: [Training] ce = 0.05525911 * 50000; errs = 1.840% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=9.54637s
Finished Epoch[159 of 160]: [Training] ce = 0.05525911 * 50000; errs = 1.840% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=9.54637s
ce = 0.05525911 * 50000; errs = 1.840% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=9.54636s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.61-seconds latency this time; accumulated time on sync point = 1.61 seconds , average latency = 1.61 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
Finished Epoch[160 of 160]: [Training] ce = 0.05677588 * 50000; errs = 1.934% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.1556s
Finished Epoch[160 of 160]: [Training] ce = 0.05677588 * 50000; errs = 1.934% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.15561s
Finished Epoch[160 of 160]: [Training] Finished Epoch[160 of 160]: [Training] ce = 0.05677588 * 50000; errs = 1.934% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.1556s
ce = 0.05677588 * 50000; errs = 1.934% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.1556s





##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################


##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################


##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################


##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################

Final Results: Minibatch[1-21]: errs = 8.600% * 10000; top5Errs = 0.200% * 10000
Final Results: Minibatch[1-21]: errs = 8.600% * 10000; top5Errs = 0.200% * 10000
Final Results: Minibatch[1-21]: Final Results: Minibatch[1-21]: errs = 8.600% * 10000; top5Errs = 0.200% * 10000
errs = 8.600% * 10000; top5Errs = 0.200% * 10000



COMPLETED.

COMPLETED.
COMPLETED.
COMPLETED.
