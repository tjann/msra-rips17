CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/08/02 09:34:06

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/08/02 09:34:06

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/08/02 09:34:06

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
--------------------------------------------------------------------------
[[48447,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: Multiversohk

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/08/02 09:34:06

cntk  configFile=cnn-new-bm.cntk  makeMode=false  syncPeriod=12800
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (0) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (1) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (3) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (2) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11436 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# TrainConvNet command (train action)                                        #
#                                                                            #
##############################################################################

WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
[Multiversohk:21580] 3 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[Multiversohk:21580] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 3 x 16].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'model.arrayOfFunctions[1].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 16].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [16 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 16 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 16 x 32].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 32].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node 'z.x.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [32 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 32 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node '_z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [1 x 1 x 32 x 64].
Node 'z.x.x.x.s.arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.s.arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[0].arrayOfFunctions[0].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[0].W' (LearnableParameter operation) operation: Tensor shape was inferred as [3 x 3 x 64 x 64].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].scale' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].bias' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runMean' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'z.x.x.b.arrayOfFunctions[1].arrayOfFunctions[1].runVariance' (LearnableParameter operation) operation: Tensor shape was inferred as [64 x 1].
Node 'model.arrayOfFunctions[8].W' (LearnableParameter operation) operation: Tensor shape was inferred as [10 x 1 x 1 x 64].

Model has 209 nodes. Using GPU 1.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.


Model has 209 nodes. Using GPU 2.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.


Model has 209 nodes. Using GPU 0.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.


Model has 209 nodes. Using GPU 3.

Training criterion:   ce = CrossEntropyWithSoftmax
Evaluation criterion: errs = ClassificationError

Training 272474 parameters in 65 parameter tensors.

Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 9.10-seconds latency this time; accumulated time on sync point = 9.10 seconds , average latency = 9.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 9.82-seconds latency this time; accumulated time on sync point = 9.82 seconds , average latency = 9.82 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.15-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 1.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 1 of 160]: [Training] ce = 2.01960484 * 50000; errs = 75.124% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=25.1502s
Finished Epoch[ 1 of 160]: [Training] ce = 2.01960484 * 50000; errs = 75.124% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=25.1503s
Finished Epoch[ 1 of 160]: [Training] ce = 2.01960484 * 50000; errs = 75.124% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=25.1502s
Finished Epoch[ 1 of 160]: [Training] ce = 2.01960484 * 50000; errs = 75.124% * 50000; totalSamplesSeen = 50000; learningRatePerSample = 0.03125; epochTime=25.1516s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.01-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 1.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.20-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.20 seconds
Finished Epoch[ 2 of 160]: [Training] ce = 1.78366863 * 50000; errs = 67.746% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=8.97493s
Finished Epoch[ 2 of 160]: [Training] ce = 1.78366863 * 50000; errs = 67.746% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=8.97491s
Finished Epoch[ 2 of 160]: [Training] ce = 1.78366863 * 50000; errs = 67.746% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=8.97492s
Finished Epoch[ 2 of 160]: [Training] ce = 1.78366863 * 50000; errs = 67.746% * 50000; totalSamplesSeen = 100000; learningRatePerSample = 0.03125; epochTime=8.97493s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.12-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 1.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.98-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.98 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 3 of 160]: [Training] ce = 1.73867887 * 50000; errs = 65.080% * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=9.29955s
Finished Epoch[ 3 of 160]: [Training] ce = 1.73867887 * 50000; errs = 65.080% * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=9.29955s
Finished Epoch[ 3 of 160]: [Training] ce = 1.73867887 * 50000; errs = 65.080% * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=9.29956s
Finished Epoch[ 3 of 160]: [Training] ce = 1.73867887 * 50000; errs = 65.080% * 50000; totalSamplesSeen = 150000; learningRatePerSample = 0.03125; epochTime=9.29956s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.01-seconds latency this time; accumulated time on sync point = 2.01 seconds , average latency = 2.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.67-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 4 of 160]: [Training] Finished Epoch[ 4 of 160]: [Training] ce = 1.58998348 * 50000; errs = 58.898% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.38241s
ce = 1.58998348 * 50000Finished Epoch[ 4 of 160]: [Training] ce = 1.58998348 * 50000; errs = 58.898% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.38241s
Finished Epoch[ 4 of 160]: [Training] ce = 1.58998348 * 50000; errs = 58.898% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.38265s
; errs = 58.898% * 50000; totalSamplesSeen = 200000; learningRatePerSample = 0.03125; epochTime=9.3824s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.44-seconds latency this time; accumulated time on sync point = 1.44 seconds , average latency = 1.44 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.93-seconds latency this time; accumulated time on sync point = 1.93 seconds , average latency = 1.93 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.44-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.44 seconds
Finished Epoch[ 5 of 160]: [Training] ce = 1.48585121 * 50000; errs = 55.138% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=10.0014s
Finished Epoch[ 5 of 160]: [Training] ce = 1.48585121 * 50000; errs = 55.138% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=10.0014s
Finished Epoch[ 5 of 160]: [Training] Finished Epoch[ 5 of 160]: [Training] ce = 1.48585121 * 50000; errs = 55.138% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=10.0014s
ce = 1.48585121 * 50000; errs = 55.138% * 50000; totalSamplesSeen = 250000; learningRatePerSample = 0.03125; epochTime=10.0014s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.91-seconds latency this time; accumulated time on sync point = 1.91 seconds , average latency = 1.91 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.39-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.74-seconds latency this time; accumulated time on sync point = 1.74 seconds , average latency = 1.74 seconds
Finished Epoch[ 6 of 160]: [Training] Finished Epoch[ 6 of 160]: [Training] ce = 1.42963676 * 50000; errs = 52.352% * 50000; totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=9.79646s
ce = 1.42963676Finished Epoch[ 6 of 160]: [Training] ce = 1.42963676 * 50000; errs = 52.352% * 50000; totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=9.79673s
 * 50000; errs = 52.352% * 50000; totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=9.79646s
Finished Epoch[ 6 of 160]: [Training] ce = 1.42963676 * 50000; errs = 52.352% * 50000; totalSamplesSeen = 300000; learningRatePerSample = 0.03125; epochTime=9.79672s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.10-seconds latency this time; accumulated time on sync point = 2.10 seconds , average latency = 2.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.31-seconds latency this time; accumulated time on sync point = 2.31 seconds , average latency = 2.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.02-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 1.02 seconds
Finished Epoch[ 7 of 160]: [Training] Finished Epoch[ 7 of 160]: [Training] ce = 1.32040102 * 50000; errs = 48.216% * 50000; totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=9.9655s
Finished Epoch[ 7 of 160]: [Training] ce = 1.32040102 * 50000; errs = 48.216% * 50000; totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=9.96392s
Finished Epoch[ 7 of 160]: [Training] ce = 1.32040102 * 50000; errs = 48.216% * 50000; totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=9.96551s
ce = 1.32040102 * 50000; errs = 48.216% * 50000; totalSamplesSeen = 350000; learningRatePerSample = 0.03125; epochTime=9.9655s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.02-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 1.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.63-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 1.63 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.17 seconds
Finished Epoch[ 8 of 160]: [Training] ce = 1.23496631 * 50000; errs = 44.904% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.08725s
Finished Epoch[ 8 of 160]: [Training] ce = 1.23496631 * 50000; errs = 44.904% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.08862s
Finished Epoch[ 8 of 160]: [Training] ce = 1.23496631 * 50000; errs = 44.904% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.0922s
Finished Epoch[ 8 of 160]: [Training] ce = 1.23496631 * 50000; errs = 44.904% * 50000; totalSamplesSeen = 400000; learningRatePerSample = 0.03125; epochTime=9.08861s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.30-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.04-seconds latency this time; accumulated time on sync point = 1.04 seconds , average latency = 1.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.86-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.86 seconds
Finished Epoch[ 9 of 160]: [Training] ce = 1.18827209 * 50000; errs = 42.684% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.16909s
Finished Epoch[ 9 of 160]: [Training] ce = 1.18827209 * 50000; errs = 42.684% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.16909s
Finished Epoch[ 9 of 160]: [Training] ce = 1.18827209 * 50000; errs = 42.684% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.16916s
Finished Epoch[ 9 of 160]: [Training] ce = 1.18827209 * 50000; errs = 42.684% * 50000; totalSamplesSeen = 450000; learningRatePerSample = 0.03125; epochTime=9.16916s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.90-seconds latency this time; accumulated time on sync point = 1.90 seconds , average latency = 1.90 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.49-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.49 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.83-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 1.83 seconds
Finished Epoch[10 of 160]: [Training] ce = 1.11204744 * 50000; errs = Finished Epoch[10 of 160]: [Training] ce = 1.11204744 * 50000; errs = 39.680% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=9.57754s
Finished Epoch[10 of 160]: [Training] ce = 1.11204744 * 50000; errs = 39.680% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=9.57748s
Finished Epoch[10 of 160]: [Training] 39.680% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=9.57755s
ce = 1.11204744 * 50000; errs = 39.680% * 50000; totalSamplesSeen = 500000; learningRatePerSample = 0.03125; epochTime=9.57754s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.74-seconds latency this time; accumulated time on sync point = 1.74 seconds , average latency = 1.74 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[11 of 160]: [Training] ce = 1.06092430 * 50000; errs = 37.966%Finished Epoch[11 of 160]: [Training] ce = 1.06092430 * 50000; errs = 37.966% * 50000; totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.37282s
Finished Epoch[11 of 160]: [Training] Finished Epoch[11 of 160]: [Training] ce = 1.06092430 * 50000; errs = 37.966% * 50000; totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.37282s
 * 50000; totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.37287s
ce = 1.06092430 * 50000; errs = 37.966% * 50000; totalSamplesSeen = 550000; learningRatePerSample = 0.03125; epochTime=9.37287s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.59-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 1.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.44-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.44 seconds
Finished Epoch[12 of 160]: [Training] ce = 1.00755467 * 50000; errs = 35.836% * 50000; totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.36463s
Finished Epoch[12 of 160]: [Training] ce = 1.00755467 * 50000; errs = 35.836% * 50000; totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.36485s
Finished Epoch[12 of 160]: [Training] ce = 1.00755467 * 50000; errs = 35.836% * 50000; totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.36463s
Finished Epoch[12 of 160]: [Training] ce = 1.00755467 * 50000; errs = 35.836% * 50000; totalSamplesSeen = 600000; learningRatePerSample = 0.03125; epochTime=9.36486s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.18-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.18 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.37-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 1.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
Finished Epoch[13 of 160]: [Training] ce = 0.93528887 * 50000; errs = 33.374% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.43992s
Finished Epoch[13 of 160]: [Training] ce = 0.93528887 * 50000; errs = 33.374% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.43991s
Finished Epoch[13 of 160]: [Training] ce = 0.93528887 * 50000; errs = 33.374% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.4399s
Finished Epoch[13 of 160]: [Training] ce = 0.93528887 * 50000; errs = 33.374% * 50000; totalSamplesSeen = 650000; learningRatePerSample = 0.03125; epochTime=9.4399s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.07-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 1.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.93-seconds latency this time; accumulated time on sync point = 0.93 seconds , average latency = 0.93 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.14-seconds latency this time; accumulated time on sync point = 0.14 seconds , average latency = 0.14 seconds
Finished Epoch[14 of 160]: [Training] ce = 0.90965789 * 50000; errs = 32.390% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.05476s
Finished Epoch[14 of 160]: [Training] ce = 0.90965789 * 50000; errs = 32.390% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.05476s
Finished Epoch[14 of 160]: [Training] Finished Epoch[14 of 160]: [Training] ce = 0.90965789 * 50000; errs = 32.390% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.05476s
ce = 0.90965789 * 50000; errs = 32.390% * 50000; totalSamplesSeen = 700000; learningRatePerSample = 0.03125; epochTime=9.05475s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.23-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 1.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.12-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 1.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
Finished Epoch[15 of 160]: [Training] Finished Epoch[15 of 160]: [Training] ce = 0.87032697 * 50000; errs = 30.812% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.08306s
Finished Epoch[15 of 160]: [Training] ce = 0.87032697 * 50000; errs = 30.812% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.08305s
Finished Epoch[15 of 160]: [Training] ce = 0.87032697 * 50000; errs = 30.812% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.08305s
ce = 0.87032697 * 50000; errs = 30.812% * 50000; totalSamplesSeen = 750000; learningRatePerSample = 0.03125; epochTime=9.08305s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.83-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.83 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.18-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 1.18 seconds
Finished Epoch[16 of 160]: [Training] ce = 0.82457148 * 50000; errs = Finished Epoch[16 of 160]: [Training] ce = 0.82457148 * 50000; errs = 28.878% * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=8.85087s
Finished Epoch[16 of 160]: [Training] 28.878% * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=8.85088s
Finished Epoch[16 of 160]: [Training] ce = 0.82457148 * 50000; errs = 28.878% * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=8.85088s
ce = 0.82457148 * 50000; errs = 28.878% * 50000; totalSamplesSeen = 800000; learningRatePerSample = 0.03125; epochTime=8.85087s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.54-seconds latency this time; accumulated time on sync point = 3.54 seconds , average latency = 3.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.36-seconds latency this time; accumulated time on sync point = 2.36 seconds , average latency = 2.36 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.40-seconds latency this time; accumulated time on sync point = 3.40 seconds , average latency = 3.40 seconds
Finished Epoch[17 of 160]: [Training] ce = 0.79749920 * 50000; errs = Finished Epoch[17 of 160]: [Training] Finished Epoch[17 of 160]: [Training] ce = 0.79749920 * 50000; errs = 27.944% * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=11.0999s
27.944% * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=11.0999s
Finished Epoch[17 of 160]: [Training] ce = 0.79749920 * 50000; errs = 27.944% * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=11.0987s
ce = 0.79749920 * 50000; errs = 27.944% * 50000; totalSamplesSeen = 850000; learningRatePerSample = 0.03125; epochTime=11.0999s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.87-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.87 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.07-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 1.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.12 seconds
Finished Epoch[18 of 160]: [Training] ce = Finished Epoch[18 of 160]: [Training] Finished Epoch[18 of 160]: [Training] ce = 0.76701490 * 50000; errs = 26.596% * 50000; totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=9.43485s
Finished Epoch[18 of 160]: [Training] ce = 0.76701490 * 50000; errs = 26.596% * 50000; totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=9.43485s
ce = 0.76701490 * 50000; 0.76701490 * 50000; errs = 26.596% * 50000; totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=9.43483s
errs = 26.596% * 50000; totalSamplesSeen = 900000; learningRatePerSample = 0.03125; epochTime=9.43484s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.27-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 1.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.24-seconds latency this time; accumulated time on sync point = 0.24 seconds , average latency = 0.24 seconds
Finished Epoch[19 of 160]: [Training] ce = 0.72570908 * 50000; errs = 25.214% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=9.16341s
Finished Epoch[19 of 160]: [Training] ce = 0.72570908 * 50000; errs = 25.214% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=9.16341s
Finished Epoch[19 of 160]: [Training] ce = Finished Epoch[19 of 160]: [Training] ce = 0.72570908 * 50000; errs = 25.214% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=9.1634s
0.72570908 * 50000; errs = 25.214% * 50000; totalSamplesSeen = 950000; learningRatePerSample = 0.03125; epochTime=9.1634s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.22-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 1.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.50-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 1.50 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
Finished Epoch[20 of 160]: [Training] ce = 0.71484404 * 50000; errs = 24.974% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.11092s
Finished Epoch[20 of 160]: [Training] ce = 0.71484404 * 50000; errs = 24.974% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.11092s
Finished Epoch[20 of 160]: [Training] ce = 0.71484404 * 50000; errs = 24.974% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.11092s
Finished Epoch[20 of 160]: [Training] ce = 0.71484404 * 50000; errs = 24.974% * 50000; totalSamplesSeen = 1000000; learningRatePerSample = 0.03125; epochTime=9.11091s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.30-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.75-seconds latency this time; accumulated time on sync point = 1.75 seconds , average latency = 1.75 seconds
Finished Epoch[21 of 160]: [Training] Finished Epoch[21 of 160]: [Training] ce = 0.68684525 * 50000; errs = 24.054% * 50000; totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.19326s
Finished Epoch[21 of 160]: [Training] ce = 0.68684525 * 50000; errs = 24.054% * 50000; totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.19326s
Finished Epoch[21 of 160]: [Training] ce = 0.68684525 * 50000; errs = 24.054% * 50000; totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.1933s
ce = 0.68684525 * 50000; errs = 24.054% * 50000; totalSamplesSeen = 1050000; learningRatePerSample = 0.03125; epochTime=9.19328s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.91-seconds latency this time; accumulated time on sync point = 1.91 seconds , average latency = 1.91 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.51-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.51 seconds
Finished Epoch[22 of 160]: [Training] ce = 0.67175148 * 50000; errs = 23.242% * 50000; Finished Epoch[22 of 160]: [Training] totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.55168s
ce = 0.67175148 * 50000; errs = 23.242% * 50000; totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.55168s
Finished Epoch[22 of 160]: [Training] ce = 0.67175148 * 50000; errs = 23.242% * 50000Finished Epoch[22 of 160]: [Training] ce = 0.67175148 * 50000; errs = 23.242% * 50000; totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.55278s
; totalSamplesSeen = 1100000; learningRatePerSample = 0.03125; epochTime=9.55234s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.45-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.45 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
Finished Epoch[23 of 160]: [Training] ce = 0.65155115 * 50000; errs = 22.594% * 50000; Finished Epoch[23 of 160]: [Training] Finished Epoch[23 of 160]: [Training] ce = 0.65155115 * 50000; errs = 22.594% * 50000; totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=9.44279s
Finished Epoch[23 of 160]: [Training] ce = 0.65155115 * 50000; errs = 22.594% * 50000; totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=9.4428s
totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=9.44278s
ce = 0.65155115 * 50000; errs = 22.594% * 50000; totalSamplesSeen = 1150000; learningRatePerSample = 0.03125; epochTime=9.44278s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.53-seconds latency this time; accumulated time on sync point = 0.53 seconds , average latency = 0.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.70-seconds latency this time; accumulated time on sync point = 0.70 seconds , average latency = 0.70 seconds
Finished Epoch[24 of 160]: [Training] ce = 0.63914752 * 50000; errs = 21.938% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.54427s
Finished Epoch[24 of 160]: [Training] ce = 0.63914752 * 50000; errs = 21.938% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.54426s
Finished Epoch[24 of 160]: [Training] Finished Epoch[24 of 160]: [Training] ce = 0.63914752 * 50000; errs = 21.938% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.54428s
ce = 0.63914752 * 50000; errs = 21.938% * 50000; totalSamplesSeen = 1200000; learningRatePerSample = 0.03125; epochTime=9.54426s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.17-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 1.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.15-seconds latency this time; accumulated time on sync point = 1.15 seconds , average latency = 1.15 seconds
Finished Epoch[25 of 160]: [Training] ce = 0.61234920 * 50000; errs = 21.110% * 50000; totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.71531s
Finished Epoch[25 of 160]: [Training] Finished Epoch[25 of 160]: [Training] ce = 0.61234920 * 50000; errs = 21.110% * 50000; totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.71558s
Finished Epoch[25 of 160]: [Training] ce = 0.61234920 * 50000; errs = 21.110% * 50000; totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.71558s
ce = 0.61234920 * 50000; errs = 21.110% * 50000; totalSamplesSeen = 1250000; learningRatePerSample = 0.03125; epochTime=9.71531s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.45-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 1.45 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[26 of 160]: [Training] ce = 0.59916385 * 50000; errs = 20.722% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.61185s
Finished Epoch[26 of 160]: [Training] ce = 0.59916385 * 50000; errs = 20.722% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.61185s
Finished Epoch[26 of 160]: [Training] ce = 0.59916385 * 50000; errs = 20.722% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.61185s
Finished Epoch[26 of 160]: [Training] ce = 0.59916385 * 50000; errs = 20.722% * 50000; totalSamplesSeen = 1300000; learningRatePerSample = 0.03125; epochTime=9.61184s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.90-seconds latency this time; accumulated time on sync point = 1.90 seconds , average latency = 1.90 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.58-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
Finished Epoch[27 of 160]: [Training] ce = 0.59754666 * 50000; errs = 20.624% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.83321s
Finished Epoch[27 of 160]: [Training] ce = 0.59754666 * 50000; errs = 20.624% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.83323s
Finished Epoch[27 of 160]: [Training] ce = 0.59754666 * 50000; errs = 20.624% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.83322s
Finished Epoch[27 of 160]: [Training] ce = 0.59754666 * 50000; errs = 20.624% * 50000; totalSamplesSeen = 1350000; learningRatePerSample = 0.03125; epochTime=9.83324s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.27-seconds latency this time; accumulated time on sync point = 2.27 seconds , average latency = 2.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.27-seconds latency this time; accumulated time on sync point = 2.27 seconds , average latency = 2.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.58-seconds latency this time; accumulated time on sync point = 0.58 seconds , average latency = 0.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[28 of 160]: [Training] ce = 0.57385972 * 50000; errs = 19.790% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=10.5093s
Finished Epoch[28 of 160]: [Training] ce = 0.57385972 * 50000; errs = 19.790% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=10.5093s
Finished Epoch[28 of 160]: [Training] ce = 0.57385972 * 50000; errs = 19.790% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=10.5073s
Finished Epoch[28 of 160]: [Training] ce = 0.57385972 * 50000; errs = 19.790% * 50000; totalSamplesSeen = 1400000; learningRatePerSample = 0.03125; epochTime=10.5093s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.77-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 1.77 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.79-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 1.79 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.35-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.35 seconds
Finished Epoch[29 of 160]: [Training] ce = 0.57129992 * 50000; errs = 19.642% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=10.288s
Finished Epoch[29 of 160]: [Training] ce = 0.57129992 * 50000; errs = 19.642% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=10.288s
Finished Epoch[29 of 160]: [Training] ce = 0.57129992 * 50000; errs = 19.642% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=10.2881s
Finished Epoch[29 of 160]: [Training] ce = 0.57129992 * 50000; errs = 19.642% * 50000; totalSamplesSeen = 1450000; learningRatePerSample = 0.03125; epochTime=10.2882s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
Finished Epoch[30 of 160]: [Training] ce = 0.55646654 * 50000; errs = 19.148% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=9.73388s
Finished Epoch[30 of 160]: [Training] ce = 0.55646654 * 50000; errs = 19.148% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=9.73388s
Finished Epoch[30 of 160]: [Training] ce = 0.55646654 * 50000; errs = 19.148% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=9.73389s
Finished Epoch[30 of 160]: [Training] ce = 0.55646654 * 50000; errs = 19.148% * 50000; totalSamplesSeen = 1500000; learningRatePerSample = 0.03125; epochTime=9.73388s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.34-seconds latency this time; accumulated time on sync point = 2.34 seconds , average latency = 2.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.71-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 1.71 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[31 of 160]: [Training] Finished Epoch[31 of 160]: [Training] ce = 0.54443708 * 50000; errs = 18.706% * 50000; totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=10.4268s
Finished Epoch[31 of 160]: [Training] ce = 0.54443708 * 50000; errs = 18.706% * 50000; totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=10.4271s
Finished Epoch[31 of 160]: [Training] ce = 0.54443708 * 50000; errs = 18.706% * 50000; totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=10.4271s
ce = 0.54443708 * 50000; errs = 18.706% * 50000; totalSamplesSeen = 1550000; learningRatePerSample = 0.03125; epochTime=10.4268s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.02-seconds latency this time; accumulated time on sync point = 1.02 seconds , average latency = 1.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.87-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.87 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.31-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[32 of 160]: [Training] ce = 0.52936948 * 50000; errs = Finished Epoch[32 of 160]: [Training] ce = 0.52936948 * 50000; errs = 18.270% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=9.10449s
Finished Epoch[32 of 160]: [Training] ce = 0.52936948 * 50000; errs = 18.270% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=9.09973s
Finished Epoch[32 of 160]: [Training] 18.270% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=9.1045s
ce = 0.52936948 * 50000; errs = 18.270% * 50000; totalSamplesSeen = 1600000; learningRatePerSample = 0.03125; epochTime=9.10449s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.16-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 1.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.32-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 1.32 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
Finished Epoch[33 of 160]: [Training] ce = 0.53798855 * 50000; errs = 18.556% * 50000; Finished Epoch[33 of 160]: [Training] ce = 0.53798855 * 50000; errs = 18.556% * 50000; totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=9.24536s
Finished Epoch[33 of 160]: [Training] Finished Epoch[33 of 160]: [Training] ce = 0.53798855 * 50000; errs = 18.556% * 50000; totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=9.24535s
totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=9.24535s
ce = 0.53798855 * 50000; errs = 18.556% * 50000; totalSamplesSeen = 1650000; learningRatePerSample = 0.03125; epochTime=9.24535s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.72-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 1.72 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.62-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.62 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.99-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 1.99 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[34 of 160]: [Training] ce = 0.51625314 * 50000; errs = Finished Epoch[34 of 160]: [Training] ce = 0.51625314 * 50000; errs = 17.854% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.70308s
Finished Epoch[34 of 160]: [Training] Finished Epoch[34 of 160]: [Training] ce = 0.51625314 * 50000; errs = 17.854% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.70303s
17.854% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.70303s
ce = 0.51625314 * 50000; errs = 17.854% * 50000; totalSamplesSeen = 1700000; learningRatePerSample = 0.03125; epochTime=9.70307s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.19 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.76-seconds latency this time; accumulated time on sync point = 1.76 seconds , average latency = 1.76 seconds
Finished Epoch[35 of 160]: [Training] ce = 0.52434458 * 50000; errs = 17.810% * 50000; totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.81531s
Finished Epoch[35 of 160]: [Training] ce = Finished Epoch[35 of 160]: [Training] ce = 0.52434458 * 50000; errs = 17.810% * 50000; totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.81531s
Finished Epoch[35 of 160]: [Training] ce = 0.52434458 * 50000; errs = 17.810% * 50000; totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.81531s
0.52434458 * 50000; errs = 17.810% * 50000; totalSamplesSeen = 1750000; learningRatePerSample = 0.03125; epochTime=9.81534s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.79-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 1.79 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.82-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 1.82 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.79-seconds latency this time; accumulated time on sync point = 0.79 seconds , average latency = 0.79 seconds
Finished Epoch[36 of 160]: [Training] ce = 0.51125377 * 50000; errs = 17.626% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=9.64168s
Finished Epoch[36 of 160]: [Training] ce = 0.51125377 * 50000; errs = 17.626% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=9.64167s
Finished Epoch[36 of 160]: [Training] ce = 0.51125377 * 50000; errs = 17.626% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=9.64167s
Finished Epoch[36 of 160]: [Training] ce = 0.51125377 * 50000; errs = 17.626% * 50000; totalSamplesSeen = 1800000; learningRatePerSample = 0.03125; epochTime=9.64168s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.33-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.33 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.67-seconds latency this time; accumulated time on sync point = 1.67 seconds , average latency = 1.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.47-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 1.47 seconds
Finished Epoch[37 of 160]: [Training] Finished Epoch[37 of 160]: [Training] ce = 0.50071906 * 50000; errs = 17.186% * 50000; totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=9.23359s
Finished Epoch[37 of 160]: [Training] ce = 0.50071906 * 50000; errs = 17.186% * 50000; totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=9.23359s
Finished Epoch[37 of 160]: [Training] ce = 0.50071906 * 50000; errs = 17.186% * 50000; totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=9.23359s
ce = 0.50071906 * 50000; errs = 17.186% * 50000; totalSamplesSeen = 1850000; learningRatePerSample = 0.03125; epochTime=9.2336s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.50-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 1.50 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.11-seconds latency this time; accumulated time on sync point = 1.11 seconds , average latency = 1.11 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
Finished Epoch[38 of 160]: [Training] Finished Epoch[38 of 160]: [Training] ce = 0.49714594 * 50000; errs = 17.158% * 50000; totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.23962s
ce = 0.49714594Finished Epoch[38 of 160]: [Training] ce = 0.49714594 * 50000; errs = 17.158% * 50000; totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.2399s
Finished Epoch[38 of 160]: [Training] ce = 0.49714594 * 50000; errs = 17.158% * 50000; totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.23987s
 * 50000; errs = 17.158% * 50000; totalSamplesSeen = 1900000; learningRatePerSample = 0.03125; epochTime=9.23961s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.69-seconds latency this time; accumulated time on sync point = 0.69 seconds , average latency = 0.69 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.71-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 1.71 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
Finished Epoch[39 of 160]: [Training] ce = 0.49220449 * 50000; errs = 16.960% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.70192s
Finished Epoch[39 of 160]: [Training] Finished Epoch[39 of 160]: [Training] ce = 0.49220449 * 50000; errs = 16.960% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.70192s
Finished Epoch[39 of 160]: [Training] ce = 0.49220449 * 50000; errs = 16.960% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.70192s
ce = 0.49220449 * 50000; errs = 16.960% * 50000; totalSamplesSeen = 1950000; learningRatePerSample = 0.03125; epochTime=9.70192s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.69-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 1.69 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.94-seconds latency this time; accumulated time on sync point = 1.94 seconds , average latency = 1.94 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.30-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.30 seconds
Finished Epoch[40 of 160]: [Training] ce = 0.49019562 * 50000; errs = Finished Epoch[40 of 160]: [Training] Finished Epoch[40 of 160]: [Training] ce = 0.49019562 * 50000; errs = 16.924% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.39427s
16.924% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.39426s
Finished Epoch[40 of 160]: [Training] ce = 0.49019562 * 50000; errs = 16.924% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.39429s
ce = 0.49019562 * 50000; errs = 16.924% * 50000; totalSamplesSeen = 2000000; learningRatePerSample = 0.03125; epochTime=9.39426s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.33-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 1.33 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.19-seconds latency this time; accumulated time on sync point = 1.19 seconds , average latency = 1.19 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.19-seconds latency this time; accumulated time on sync point = 0.19 seconds , average latency = 0.19 seconds
Finished Epoch[41 of 160]: [Training] Finished Epoch[41 of 160]: [Training] ce = 0.47523804 * 50000; errs = 16.240% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.33562s
Finished Epoch[41 of 160]: [Training] ce = 0.47523804 * 50000; errs = 16.240% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.33564s
Finished Epoch[41 of 160]: [Training] ce = 0.47523804 * 50000; errs = 16.240% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.33567s
ce = 0.47523804 * 50000; errs = 16.240% * 50000; totalSamplesSeen = 2050000; learningRatePerSample = 0.03125; epochTime=9.33564s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.46-seconds latency this time; accumulated time on sync point = 0.46 seconds , average latency = 0.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
Finished Epoch[42 of 160]: [Training] ce = 0.47448183 * 50000; errs = Finished Epoch[42 of 160]: [Training] Finished Epoch[42 of 160]: [Training] ce = 0.47448183 * 50000; errs = 16.260% * 50000; Finished Epoch[42 of 160]: [Training] ce = 0.47448183 * 50000; errs = 16.260% * 50000; totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.92313s
totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.92302s
16.260% * 50000; totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.92302s
ce = 0.47448183 * 50000; errs = 16.260% * 50000; totalSamplesSeen = 2100000; learningRatePerSample = 0.03125; epochTime=9.92313s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.06-seconds latency this time; accumulated time on sync point = 2.06 seconds , average latency = 2.06 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.61-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.61 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.19-seconds latency this time; accumulated time on sync point = 2.19 seconds , average latency = 2.19 seconds
Finished Epoch[43 of 160]: [Training] ce = 0.46900485 * 50000; errs = 16.324% * 50000; totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=10.3027s
Finished Epoch[43 of 160]: [Training] ce = 0.46900485 * 50000; errs = 16.324% * 50000; totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=10.3027s
Finished Epoch[43 of 160]: [Training] ce = 0.46900485 * 50000; errs = 16.324% * 50000; totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=10.3027s
Finished Epoch[43 of 160]: [Training] ce = 0.46900485 * 50000; errs = 16.324% * 50000; totalSamplesSeen = 2150000; learningRatePerSample = 0.03125; epochTime=10.3027s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.83-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 1.83 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.00-seconds latency this time; accumulated time on sync point = 2.00 seconds , average latency = 2.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.68-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.68 seconds
Finished Epoch[44 of 160]: [Training] Finished Epoch[44 of 160]: [Training] ce = 0.47291589 * 50000; errs = 16.334% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.81723s
ce = 0.47291589Finished Epoch[44 of 160]: [Training] ce = 0.47291589 * 50000; errs = 16.334% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.81625s
Finished Epoch[44 of 160]: [Training] ce = 0.47291589 * 50000; errs = 16.334% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.81748s
 * 50000; errs = 16.334% * 50000; totalSamplesSeen = 2200000; learningRatePerSample = 0.03125; epochTime=9.81723s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.69-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 1.69 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.83-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 1.83 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.21 seconds
Finished Epoch[45 of 160]: [Training] Finished Epoch[45 of 160]: [Training] ce = 0.46960848 * 50000; errs = 16.036% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.88657s
Finished Epoch[45 of 160]: [Training] ce = 0.46960848 * 50000; errs = 16.036% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.88656s
Finished Epoch[45 of 160]: [Training] ce = 0.46960848 * 50000; errs = 16.036% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.88034s
ce = 0.46960848 * 50000; errs = 16.036% * 50000; totalSamplesSeen = 2250000; learningRatePerSample = 0.03125; epochTime=9.88032s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.19-seconds latency this time; accumulated time on sync point = 1.19 seconds , average latency = 1.19 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
Finished Epoch[46 of 160]: [Training] Finished Epoch[46 of 160]: [Training] ce = 0.46570101 * 50000; errs = 15.900% * 50000; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=9.09088s
Finished Epoch[46 of 160]: [Training] ce = 0.46570101 * 50000; errs = 15.900% * 50000; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=9.09095s
Finished Epoch[46 of 160]: [Training] ce = 0.46570101 * 50000; errs = 15.900% * 50000; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=9.09095s
ce = 0.46570101 * 50000; errs = 15.900% * 50000; totalSamplesSeen = 2300000; learningRatePerSample = 0.03125; epochTime=9.09088s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.34-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
Finished Epoch[47 of 160]: [Training] ce = 0.44676844 * 50000; errs = 15.282% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=9.32898s
Finished Epoch[47 of 160]: [Training] ce = 0.44676844 * 50000; errs = 15.282% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=9.32899s
Finished Epoch[47 of 160]: [Training] ce = 0.44676844 * 50000; errs = 15.282% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=9.32898s
Finished Epoch[47 of 160]: [Training] ce = 0.44676844 * 50000; errs = 15.282% * 50000; totalSamplesSeen = 2350000; learningRatePerSample = 0.03125; epochTime=9.32898s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.09-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 1.09 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.08-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 1.08 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.22-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.22 seconds
Finished Epoch[48 of 160]: [Training] ce = 0.45096207 * 50000; errs = 15.524% * 50000; totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.58291s
Finished Epoch[48 of 160]: [Training] ce = 0.45096207 * 50000; errs = 15.524% * 50000; totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.58317s
Finished Epoch[48 of 160]: [Training] ce = 0.45096207 * 50000; errs = 15.524% * 50000; totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.58319s
Finished Epoch[48 of 160]: [Training] ce = 0.45096207 * 50000; errs = 15.524% * 50000; totalSamplesSeen = 2400000; learningRatePerSample = 0.03125; epochTime=9.58286s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
Finished Epoch[49 of 160]: [Training] ce = 0.45048361 * 50000; errs = Finished Epoch[49 of 160]: [Training] Finished Epoch[49 of 160]: [Training] ce = 0.45048361 * 50000; errs = 15.604%Finished Epoch[49 of 160]: [Training] ce = 0.45048361 * 50000; errs = 15.604% * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=9.46157s
15.604% * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=9.46645s
 * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=9.46156s
ce = 0.45048361 * 50000; errs = 15.604% * 50000; totalSamplesSeen = 2450000; learningRatePerSample = 0.03125; epochTime=9.46645s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.40-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 1.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.22-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 1.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[50 of 160]: [Training] Finished Epoch[50 of 160]: [Training] ce = 0.44004376 * 50000; errs = 15.056% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=9.19798s
Finished Epoch[50 of 160]: [Training] ce = 0.44004376 * 50000; errs = 15.056% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=9.20337s
Finished Epoch[50 of 160]: [Training] ce = 0.44004376 * 50000; errs = 15.056% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=9.20335s
ce = 0.44004376 * 50000; errs = 15.056% * 50000; totalSamplesSeen = 2500000; learningRatePerSample = 0.03125; epochTime=9.20335s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.18-seconds latency this time; accumulated time on sync point = 2.18 seconds , average latency = 2.18 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.18-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 1.18 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.28-seconds latency this time; accumulated time on sync point = 3.28 seconds , average latency = 3.28 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[51 of 160]: [Training] ce = 0.44540980 * 50000; errs = Finished Epoch[51 of 160]: [Training] ce = 0.44540980 * 50000; errs = Finished Epoch[51 of 160]: [Training] ce = 0.44540980 * 50000; errs = 15.116% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=10.6342s
Finished Epoch[51 of 160]: [Training] 15.116% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=10.6342s
15.116% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=10.6342s
ce = 0.44540980 * 50000; errs = 15.116% * 50000; totalSamplesSeen = 2550000; learningRatePerSample = 0.03125; epochTime=10.6342s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.25-seconds latency this time; accumulated time on sync point = 1.25 seconds , average latency = 1.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.91-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.91 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[52 of 160]: [Training] ce = 0.43570473 * 50000; errs = 15.088% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.59943s
Finished Epoch[52 of 160]: [Training] Finished Epoch[52 of 160]: [Training] ce = 0.43570473 * 50000; errs = 15.088% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.59941s
Finished Epoch[52 of 160]: [Training] ce = 0.43570473 * 50000; errs = 15.088% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.59943s
ce = 0.43570473 * 50000; errs = 15.088% * 50000; totalSamplesSeen = 2600000; learningRatePerSample = 0.03125; epochTime=9.59941s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.36-seconds latency this time; accumulated time on sync point = 1.36 seconds , average latency = 1.36 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.33-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 1.33 seconds
Finished Epoch[53 of 160]: [Training] ce = 0.44116267 * 50000; errs = 15.128% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.09236s
Finished Epoch[53 of 160]: [Training] Finished Epoch[53 of 160]: [Training] ce = 0.44116267 * 50000; errs = 15.128% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.09238s
Finished Epoch[53 of 160]: [Training] ce = 0.44116267 * 50000; errs = 15.128% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.09237s
ce = 0.44116267 * 50000; errs = 15.128% * 50000; totalSamplesSeen = 2650000; learningRatePerSample = 0.03125; epochTime=9.09237s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.92-seconds latency this time; accumulated time on sync point = 1.92 seconds , average latency = 1.92 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.23-seconds latency this time; accumulated time on sync point = 2.23 seconds , average latency = 2.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.21-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 1.21 seconds
Finished Epoch[54 of 160]: [Training] Finished Epoch[54 of 160]: [Training] ce = 0.43184554 * 50000; errs = 14.800% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.88366s
Finished Epoch[54 of 160]: [Training] ce = 0.43184554 * 50000; errs = 14.800% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.88368s
Finished Epoch[54 of 160]: [Training] ce = 0.43184554 * 50000; errs = 14.800% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.88367s
ce = 0.43184554 * 50000; errs = 14.800% * 50000; totalSamplesSeen = 2700000; learningRatePerSample = 0.03125; epochTime=9.88367s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.12-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 1.12 seconds
Finished Epoch[55 of 160]: [Training] Finished Epoch[55 of 160]: [Training] Finished Epoch[55 of 160]: [Training] ce = 0.42749020 * 50000; errs = 14.818% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.64833s
Finished Epoch[55 of 160]: [Training] ce = 0.42749020 * 50000; errs = 14.818% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.64835s
ce = 0.42749020 * 50000; ce = 0.42749020 * 50000; errs = 14.818% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.64835s
errs = 14.818% * 50000; totalSamplesSeen = 2750000; learningRatePerSample = 0.03125; epochTime=9.64835s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.12-seconds latency this time; accumulated time on sync point = 0.12 seconds , average latency = 0.12 seconds
Finished Epoch[56 of 160]: [Training] ce = 0.43267729 * 50000; errs = 14.810% * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.51963s
Finished Epoch[56 of 160]: [Training] Finished Epoch[56 of 160]: [Training] ce = 0.43267729 * 50000; errs = 14.810% * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.51964s
Finished Epoch[56 of 160]: [Training] ce = 0.43267729 * 50000; errs = 14.810% * 50000ce = 0.43267729 * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.51463s
; errs = 14.810% * 50000; totalSamplesSeen = 2800000; learningRatePerSample = 0.03125; epochTime=9.51963s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.86-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.86 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.27-seconds latency this time; accumulated time on sync point = 2.27 seconds , average latency = 2.27 seconds
Finished Epoch[57 of 160]: [Training] ce = 0.43528564 * 50000; errs = 14.884% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=10.0197s
Finished Epoch[57 of 160]: [Training] ce = 0.43528564 * 50000; errs = 14.884% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=10.0198s
Finished Epoch[57 of 160]: [Training] ce = 0.43528564 * 50000; errs = 14.884% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=10.0198s
Finished Epoch[57 of 160]: [Training] ce = 0.43528564 * 50000; errs = 14.884% * 50000; totalSamplesSeen = 2850000; learningRatePerSample = 0.03125; epochTime=10.0197s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.90-seconds latency this time; accumulated time on sync point = 1.90 seconds , average latency = 1.90 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.17-seconds latency this time; accumulated time on sync point = 2.17 seconds , average latency = 2.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.49-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.49 seconds
Finished Epoch[58 of 160]: [Training] ce = 0.43668987 * 50000; errs = 14.974% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=10.2759s
Finished Epoch[58 of 160]: [Training] ce = 0.43668987 * 50000; errs = 14.974% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=10.2702s
Finished Epoch[58 of 160]: [Training] ce = 0.43668987 * 50000; errs = 14.974% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=10.2759s
Finished Epoch[58 of 160]: [Training] ce = 0.43668987 * 50000; errs = 14.974% * 50000; totalSamplesSeen = 2900000; learningRatePerSample = 0.03125; epochTime=10.2759s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.01-seconds latency this time; accumulated time on sync point = 2.01 seconds , average latency = 2.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.87-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.87 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.82-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 1.82 seconds
Finished Epoch[59 of 160]: [Training] Finished Epoch[59 of 160]: [Training] ce = 0.42067910 * 50000; errs = 14.610% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=9.66637s
Finished Epoch[59 of 160]: [Training] ce = 0.42067910 * 50000; errs = 14.610% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=9.66639s
Finished Epoch[59 of 160]: [Training] ce = 0.42067910 * 50000; errs = 14.610% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=9.66644s
ce = 0.42067910 * 50000; errs = 14.610% * 50000; totalSamplesSeen = 2950000; learningRatePerSample = 0.03125; epochTime=9.66637s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.14-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 1.14 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.90-seconds latency this time; accumulated time on sync point = 0.90 seconds , average latency = 0.90 seconds
Finished Epoch[60 of 160]: [Training] ce = 0.42458268 * 50000; errs = 14.680% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=9.07408s
Finished Epoch[60 of 160]: [Training] ce = 0.42458268 * 50000; errs = 14.680% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=9.07408s
Finished Epoch[60 of 160]: [Training] ce = 0.42458268 * 50000; errs = 14.680% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=9.07408s
Finished Epoch[60 of 160]: [Training] ce = 0.42458268 * 50000; errs = 14.680% * 50000; totalSamplesSeen = 3000000; learningRatePerSample = 0.03125; epochTime=9.06613s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.59-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 1.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.77-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 1.77 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
Finished Epoch[61 of 160]: [Training] ce = 0.41826304 * 50000; errs = 14.434% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=9.59733s
Finished Epoch[61 of 160]: [Training] ce = 0.41826304 * 50000; errs = 14.434% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=9.59734s
Finished Epoch[61 of 160]: [Training] ce = 0.41826304 * 50000; errs = 14.434% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=9.59733s
Finished Epoch[61 of 160]: [Training] ce = 0.41826304 * 50000; errs = 14.434% * 50000; totalSamplesSeen = 3050000; learningRatePerSample = 0.03125; epochTime=9.59733s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.21-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 1.21 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
Finished Epoch[62 of 160]: [Training] ce = 0.41647262 * 50000; errs = 14.308% * 50000; Finished Epoch[62 of 160]: [Training] ce = 0.41647262 * 50000; errs = 14.308% * 50000; totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.31047s
Finished Epoch[62 of 160]: [Training] Finished Epoch[62 of 160]: [Training] ce = 0.41647262 * 50000; errs = 14.308% * 50000; totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.31048s
totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.31047s
ce = 0.41647262 * 50000; errs = 14.308% * 50000; totalSamplesSeen = 3100000; learningRatePerSample = 0.03125; epochTime=9.31046s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.12-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 1.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.75-seconds latency this time; accumulated time on sync point = 0.75 seconds , average latency = 0.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.28-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.28 seconds
Finished Epoch[63 of 160]: [Training] Finished Epoch[63 of 160]: [Training] ce = 0.41793958 * 50000; errs = 14.416% * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=8.91255s
Finished Epoch[63 of 160]: [Training] ce = 0.41793958 * 50000; errs = 14.416% * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=8.91255s
Finished Epoch[63 of 160]: [Training] ce = 0.41793958 * 50000; errs = 14.416% * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=8.91253s
ce = 0.41793958 * 50000; errs = 14.416% * 50000; totalSamplesSeen = 3150000; learningRatePerSample = 0.03125; epochTime=8.91255s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.29-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.74-seconds latency this time; accumulated time on sync point = 1.74 seconds , average latency = 1.74 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.82-seconds latency this time; accumulated time on sync point = 1.82 seconds , average latency = 1.82 seconds
Finished Epoch[64 of 160]: [Training] ce = 0.42201975 * 50000; errs = 14.426% * 50000Finished Epoch[64 of 160]: [Training] Finished Epoch[64 of 160]: [Training] ce = 0.42201975 * 50000; errs = 14.426% * 50000; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=9.73999s
Finished Epoch[64 of 160]: [Training] ce = 0.42201975 * 50000; errs = 14.426% * 50000; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=9.74s
ce = 0.42201975; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=9.73999s
 * 50000; errs = 14.426% * 50000; totalSamplesSeen = 3200000; learningRatePerSample = 0.03125; epochTime=9.73999s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.57-seconds latency this time; accumulated time on sync point = 0.57 seconds , average latency = 0.57 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[65 of 160]: [Training] ce = 0.41120628 * 50000; errs = 14.122% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.46437s
Finished Epoch[65 of 160]: [Training] Finished Epoch[65 of 160]: [Training] ce = 0.41120628 * 50000; errs = 14.122% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.46437s
ce = 0.41120628 * 50000; errs = 14.122% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.46437s
Finished Epoch[65 of 160]: [Training] ce = 0.41120628 * 50000; errs = 14.122% * 50000; totalSamplesSeen = 3250000; learningRatePerSample = 0.03125; epochTime=9.4644s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.71-seconds latency this time; accumulated time on sync point = 0.71 seconds , average latency = 0.71 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.47-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 1.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[66 of 160]: [Training] ce = 0.41111113 * 50000; errs = 14.142% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.75996s
Finished Epoch[66 of 160]: [Training] ce = 0.41111113 * 50000; errs = 14.142% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.75997s
Finished Epoch[66 of 160]: [Training] ce = 0.41111113 * 50000; errs = 14.142% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.75862s
Finished Epoch[66 of 160]: [Training] ce = 0.41111113 * 50000; errs = 14.142% * 50000; totalSamplesSeen = 3300000; learningRatePerSample = 0.03125; epochTime=9.75995s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.87-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.87 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.07-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 1.07 seconds
Finished Epoch[67 of 160]: [Training] ce = 0.40670024 * 50000; errs = 14.006% * 50000; totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.34135s
Finished Epoch[67 of 160]: [Training] ce = 0.40670024 * 50000; errs = 14.006% * 50000; totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.34136s
Finished Epoch[67 of 160]: [Training] ce = 0.40670024 * 50000; errs = 14.006% * 50000; totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.34136s
Finished Epoch[67 of 160]: [Training] ce = 0.40670024 * 50000; errs = 14.006% * 50000; totalSamplesSeen = 3350000; learningRatePerSample = 0.03125; epochTime=9.33421s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.72-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 1.72 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.08-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 1.08 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.08-seconds latency this time; accumulated time on sync point = 0.08 seconds , average latency = 0.08 seconds
Finished Epoch[68 of 160]: [Training] ce = 0.41298422 * 50000; errs = 14.184% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.45302s
Finished Epoch[68 of 160]: [Training] Finished Epoch[68 of 160]: [Training] ce = 0.41298422 * 50000; errs = 14.184% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.45303s
Finished Epoch[68 of 160]: [Training] ce = 0.41298422 * 50000; errs = 14.184% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.45305s
ce = 0.41298422 * 50000; errs = 14.184% * 50000; totalSamplesSeen = 3400000; learningRatePerSample = 0.03125; epochTime=9.45302s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.85-seconds latency this time; accumulated time on sync point = 1.85 seconds , average latency = 1.85 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.27-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.27 seconds
Finished Epoch[69 of 160]: [Training] ce = 0.40895943 * 50000; errs = 14.022% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.76106s
Finished Epoch[69 of 160]: [Training] Finished Epoch[69 of 160]: [Training] ce = ce = 0.40895943 * 50000; errs = 14.022% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.76135s
Finished Epoch[69 of 160]: [Training] ce = 0.40895943 * 50000; errs = 14.022% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.76167s
0.40895943 * 50000; errs = 14.022% * 50000; totalSamplesSeen = 3450000; learningRatePerSample = 0.03125; epochTime=9.76103s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.29-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.08-seconds latency this time; accumulated time on sync point = 1.08 seconds , average latency = 1.08 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.45-seconds latency this time; accumulated time on sync point = 1.45 seconds , average latency = 1.45 seconds
Finished Epoch[70 of 160]: [Training] ce = 0.41189153 * 50000; errs = 14.136% * 50000Finished Epoch[70 of 160]: [Training] ; totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.28012s
ce = 0.41189153Finished Epoch[70 of 160]: [Training] Finished Epoch[70 of 160]: [Training] ce = 0.41189153 * 50000; errs = 14.136% * 50000; totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.28039s
ce = 0.41189153 * 50000; errs = 14.136% * 50000; totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.28012s
 * 50000; errs = 14.136% * 50000; totalSamplesSeen = 3500000; learningRatePerSample = 0.03125; epochTime=9.28011s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.32-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 1.32 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.22-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.33-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 1.33 seconds
Finished Epoch[71 of 160]: [Training] ce = 0.41230131 * 50000; errs = 14.126% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.47125s
Finished Epoch[71 of 160]: [Training] ce = 0.41230131 * 50000; errs = 14.126% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.47125s
Finished Epoch[71 of 160]: [Training] Finished Epoch[71 of 160]: [Training] ce = 0.41230131 * 50000; errs = 14.126% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.471s
ce = 0.41230131 * 50000; errs = 14.126% * 50000; totalSamplesSeen = 3550000; learningRatePerSample = 0.03125; epochTime=9.47125s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.18-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.18 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.63-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 1.63 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.62-seconds latency this time; accumulated time on sync point = 1.62 seconds , average latency = 1.62 seconds
Finished Epoch[72 of 160]: [Training] ce = 0.40836728 * 50000; errs = 14.116% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=9.25049s
Finished Epoch[72 of 160]: [Training] Finished Epoch[72 of 160]: [Training] ce = 0.40836728 * 50000; errs = 14.116% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=9.27188s
Finished Epoch[72 of 160]: [Training] ce = 0.40836728 * 50000; errs = 14.116% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=9.27213s
ce = 0.40836728 * 50000; errs = 14.116% * 50000; totalSamplesSeen = 3600000; learningRatePerSample = 0.03125; epochTime=9.27187s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.10-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 1.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.22-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 1.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.45-seconds latency this time; accumulated time on sync point = 0.45 seconds , average latency = 0.45 seconds
Finished Epoch[73 of 160]: [Training] ce = 0.39168729 * 50000; errs = 13.464% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.35752s
Finished Epoch[73 of 160]: [Training] Finished Epoch[73 of 160]: [Training] ce = 0.39168729 * 50000; errs = 13.464% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.35752s
Finished Epoch[73 of 160]: [Training] ce = 0.39168729 * 50000; errs = 13.464% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.35752s
ce = 0.39168729 * 50000; errs = 13.464% * 50000; totalSamplesSeen = 3650000; learningRatePerSample = 0.03125; epochTime=9.35752s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.39-seconds latency this time; accumulated time on sync point = 0.39 seconds , average latency = 0.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.23-seconds latency this time; accumulated time on sync point = 1.23 seconds , average latency = 1.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.65-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 1.65 seconds
Finished Epoch[74 of 160]: [Training] ce = 0.40583575 * 50000; errs = Finished Epoch[74 of 160]: [Training] ce = 0.40583575 * 50000; errs = Finished Epoch[74 of 160]: [Training] ce = 0.40583575 * 50000; errs = 13.978% * 50000; totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=9.56276s
Finished Epoch[74 of 160]: [Training] ce = 0.40583575 * 50000; errs = 13.978% * 50000; totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=9.55564s
13.978% * 50000; totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=9.56274s
13.978% * 50000; totalSamplesSeen = 3700000; learningRatePerSample = 0.03125; epochTime=9.56274s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.74-seconds latency this time; accumulated time on sync point = 1.74 seconds , average latency = 1.74 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.22-seconds latency this time; accumulated time on sync point = 0.22 seconds , average latency = 0.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.48-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 1.48 seconds
Finished Epoch[75 of 160]: [Training] ce = 0.39850311 * 50000; errs = 13.630% * 50000; totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=9.38392s
Finished Epoch[75 of 160]: [Training] ce = 0.39850311 * 50000; errs = 13.630% * 50000; totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=9.38391s
Finished Epoch[75 of 160]: [Training] ce = 0.39850311 * 50000; errs = 13.630% * 50000; totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=9.38392s
Finished Epoch[75 of 160]: [Training] ce = 0.39850311 * 50000; errs = 13.630% * 50000; totalSamplesSeen = 3750000; learningRatePerSample = 0.03125; epochTime=9.38392s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.13-seconds latency this time; accumulated time on sync point = 1.13 seconds , average latency = 1.13 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.53-seconds latency this time; accumulated time on sync point = 0.53 seconds , average latency = 0.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.34-seconds latency this time; accumulated time on sync point = 1.34 seconds , average latency = 1.34 seconds
Finished Epoch[76 of 160]: [Training] ce = 0.38198284 * 50000; errs = 13.120%Finished Epoch[76 of 160]: [Training] ce = 0.38198284 * 50000; errs = 13.120% * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=9.6144s
Finished Epoch[76 of 160]: [Training] ce = 0.38198284 * 50000; errs = 13.120% * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=9.61439s
Finished Epoch[76 of 160]: [Training]  * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=9.61439s
ce = 0.38198284 * 50000; errs = 13.120% * 50000; totalSamplesSeen = 3800000; learningRatePerSample = 0.03125; epochTime=9.6073s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.94-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.94 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.31-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.68-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.68 seconds
Finished Epoch[77 of 160]: [Training] ce = 0.39736824 * 50000; errs = 13.544% * 50000; totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=8.67411s
Finished Epoch[77 of 160]: [Training] ce = 0.39736824 * 50000; Finished Epoch[77 of 160]: [Training] ce = 0.39736824 * 50000; errs = 13.544% * 50000; Finished Epoch[77 of 160]: [Training] totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=8.67435s
ce = 0.39736824 * 50000; errs = 13.544% * 50000; totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=8.67409s
errs = 13.544% * 50000; totalSamplesSeen = 3850000; learningRatePerSample = 0.03125; epochTime=8.67409s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.05-seconds latency this time; accumulated time on sync point = 2.05 seconds , average latency = 2.05 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.81-seconds latency this time; accumulated time on sync point = 1.81 seconds , average latency = 1.81 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.06 seconds
Finished Epoch[78 of 160]: [Training] ce = 0.39455153 * 50000; errs = 13.616% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=9.63765s
Finished Epoch[78 of 160]: [Training] ce = 0.39455153 * 50000; errs = 13.616% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=9.63766s
Finished Epoch[78 of 160]: [Training] ce = 0.39455153 * 50000; errs = 13.616% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=9.63766s
Finished Epoch[78 of 160]: [Training] ce = 0.39455153 * 50000; errs = 13.616% * 50000; totalSamplesSeen = 3900000; learningRatePerSample = 0.03125; epochTime=9.63767s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.40-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.09-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 1.09 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.20-seconds latency this time; accumulated time on sync point = 1.20 seconds , average latency = 1.20 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[79 of 160]: [Training] ce = 0.39979056 * 50000; errs = 13.634% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.06189s
Finished Epoch[79 of 160]: [Training] ce = 0.39979056 * 50000; errs = 13.634% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.06189s
Finished Epoch[79 of 160]: [Training] ce = 0.39979056 * 50000; errs = 13.634% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.06189s
Finished Epoch[79 of 160]: [Training] ce = 0.39979056 * 50000; errs = 13.634% * 50000; totalSamplesSeen = 3950000; learningRatePerSample = 0.03125; epochTime=9.06198s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.80-seconds latency this time; accumulated time on sync point = 0.80 seconds , average latency = 0.80 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.81-seconds latency this time; accumulated time on sync point = 0.81 seconds , average latency = 0.81 seconds
Finished Epoch[80 of 160]: [Training] Finished Epoch[80 of 160]: [Training] ce = 0.38476125 * 50000; errs = 13.294% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=9.06646s
Finished Epoch[80 of 160]: [Training] ce = 0.38476125 * 50000; errs = 13.294% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=9.06646s
Finished Epoch[80 of 160]: [Training] ce = 0.38476125 * 50000; errs = 13.294% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=9.06646s
ce = 0.38476125 * 50000; errs = 13.294% * 50000; totalSamplesSeen = 4000000; learningRatePerSample = 0.03125; epochTime=9.06645s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.31-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 1.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.01-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 1.01 seconds
Finished Epoch[81 of 160]: [Training] Finished Epoch[81 of 160]: [Training] ce = 0.29748764 * 50000; errs = 10.110% * 50000; totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.15527s
Finished Epoch[81 of 160]: [Training] ce = 0.29748764 * 50000; errs = 10.110% * 50000; totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.15557s
Finished Epoch[81 of 160]: [Training] ce = 0.29748764 * 50000; errs = 10.110% * 50000; totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.15529s
ce = 0.29748764 * 50000; errs = 10.110% * 50000; totalSamplesSeen = 4050000; learningRatePerSample = 0.003125; epochTime=9.15531s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.62-seconds latency this time; accumulated time on sync point = 0.62 seconds , average latency = 0.62 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.68-seconds latency this time; accumulated time on sync point = 0.68 seconds , average latency = 0.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.43-seconds latency this time; accumulated time on sync point = 1.43 seconds , average latency = 1.43 seconds
Finished Epoch[82 of 160]: [Training] ce = 0.33974930 * 50000; errs = 11.786%Finished Epoch[82 of 160]: [Training] Finished Epoch[82 of 160]: [Training] ce = 0.33974930 * 50000; errs = 11.786% * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=9.36167s
 * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=9.36167s
Finished Epoch[82 of 160]: [Training] ce = 0.33974930 * 50000; errs = 11.786% * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=9.36169s
ce = 0.33974930 * 50000; errs = 11.786% * 50000; totalSamplesSeen = 4100000; learningRatePerSample = 0.003125; epochTime=9.36166s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.90-seconds latency this time; accumulated time on sync point = 1.90 seconds , average latency = 1.90 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.37-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 1.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.49-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.49 seconds
Finished Epoch[83 of 160]: [Training] ce = 0.35307799 * 50000; errs = 12.292% * 50000; Finished Epoch[83 of 160]: [Training] ce = 0.35307799 * 50000; errs = Finished Epoch[83 of 160]: [Training] ce = 0.35307799 * 50000; errs = 12.292% * 50000; totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=9.86475s
Finished Epoch[83 of 160]: [Training] totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=9.86476s
12.292% * 50000; totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=9.86477s
ce = 0.35307799 * 50000; errs = 12.292% * 50000; totalSamplesSeen = 4150000; learningRatePerSample = 0.003125; epochTime=9.86476s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.80-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 1.80 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.37-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 1.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.47-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[84 of 160]: [Training] ce = 0.34487327 * 50000; errs = 11.966% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=10.0053s
Finished Epoch[84 of 160]: [Training] ce = 0.34487327 * 50000; errs = 11.966% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=10.0053s
Finished Epoch[84 of 160]: [Training] ce = 0.34487327 * 50000; errs = 11.966% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=9.99741s
Finished Epoch[84 of 160]: [Training] ce = 0.34487327 * 50000; errs = 11.966% * 50000; totalSamplesSeen = 4200000; learningRatePerSample = 0.003125; epochTime=10.0053s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.71-seconds latency this time; accumulated time on sync point = 1.71 seconds , average latency = 1.71 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.84-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 1.84 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.37-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[85 of 160]: [Training] ce = 0.32499321 * 50000; errs = Finished Epoch[85 of 160]: [Training] Finished Epoch[85 of 160]: [Training] ce = 0.32499321 * 50000; errs = 11.226% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.92121s
Finished Epoch[85 of 160]: [Training] ce = 0.32499321 * 50000; errs = 11.226% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.92477s
11.226% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.92477s
ce = 0.32499321 * 50000; errs = 11.226% * 50000; totalSamplesSeen = 4250000; learningRatePerSample = 0.003125; epochTime=9.92452s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.16-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 1.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.85-seconds latency this time; accumulated time on sync point = 0.85 seconds , average latency = 0.85 seconds
Finished Epoch[86 of 160]: [Training] ce = 0.30929725 * 50000; errs = 10.840% * 50000; totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.05873s
Finished Epoch[86 of 160]: [Training] ce = 0.30929725 * 50000; errs = 10.840% * 50000; Finished Epoch[86 of 160]: [Training] ce = 0.30929725 * 50000; errs = 10.840% * 50000; totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.05272s
Finished Epoch[86 of 160]: [Training] totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.05873s
ce = 0.30929725 * 50000; errs = 10.840% * 50000; totalSamplesSeen = 4300000; learningRatePerSample = 0.003125; epochTime=9.05885s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
Finished Epoch[87 of 160]: [Training] Finished Epoch[87 of 160]: [Training] ce = 0.28986145 * 50000; errs = 10.158% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.52765s
Finished Epoch[87 of 160]: [Training] ce = 0.28986145 * 50000; errs = 10.158% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.52765s
Finished Epoch[87 of 160]: [Training] ce = 0.28986145 * 50000; errs = 10.158% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.52764s
ce = 0.28986145 * 50000; errs = 10.158% * 50000; totalSamplesSeen = 4350000; learningRatePerSample = 0.003125; epochTime=9.52763s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.37-seconds latency this time; accumulated time on sync point = 3.37 seconds , average latency = 3.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.10-seconds latency this time; accumulated time on sync point = 3.10 seconds , average latency = 3.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.29-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.29 seconds
Finished Epoch[88 of 160]: [Training] ce = 0.27339125 * 50000; errs = 9.512% * 50000; totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=10.7922s
Finished Epoch[88 of 160]: [Training] ce = 0.27339125 * 50000; errs = 9.512% * 50000; totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=10.7922s
Finished Epoch[88 of 160]: [Training] ce = 0.27339125 * 50000; errs = 9.512% * 50000; totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=10.7922s
Finished Epoch[88 of 160]: [Training] ce = 0.27339125 * 50000; errs = 9.512% * 50000; totalSamplesSeen = 4400000; learningRatePerSample = 0.003125; epochTime=10.7922s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.40-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
Finished Epoch[89 of 160]: [Training] Finished Epoch[89 of 160]: [Training] ce = 0.25777213 * 50000; errs = 8.876% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=10.1489s
Finished Epoch[89 of 160]: [Training] ce = 0.25777213 * 50000; errs = 8.876% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=10.1489s
Finished Epoch[89 of 160]: [Training] ce = 0.25777213 * 50000; errs = 8.876% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=10.1489s
ce = 0.25777213 * 50000; errs = 8.876% * 50000; totalSamplesSeen = 4450000; learningRatePerSample = 0.003125; epochTime=10.1489s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.91-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.91 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.30-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 1.30 seconds
Finished Epoch[90 of 160]: [Training] ce = 0.24333128 * 50000; errs = 8.326% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.25173s
Finished Epoch[90 of 160]: [Training] ce = 0.24333128 * 50000; errs = 8.326% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.25173s
Finished Epoch[90 of 160]: [Training] ce = 0.24333128 * 50000; Finished Epoch[90 of 160]: [Training] ce = 0.24333128 * 50000; errs = 8.326% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.25209s
errs = 8.326% * 50000; totalSamplesSeen = 4500000; learningRatePerSample = 0.003125; epochTime=9.25173s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.37-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 1.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.18-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 1.18 seconds
Finished Epoch[91 of 160]: [Training] ce = 0.23064032 * 50000; errs = 8.048% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.13543s
Finished Epoch[91 of 160]: [Training] Finished Epoch[91 of 160]: [Training] ce = 0.23064032 * 50000; errs = 8.048% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.1354s
Finished Epoch[91 of 160]: [Training] ce = 0.23064032 * 50000; errs = 8.048% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.13546s
ce = 0.23064032 * 50000; errs = 8.048% * 50000; totalSamplesSeen = 4550000; learningRatePerSample = 0.003125; epochTime=9.13545s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.17-seconds latency this time; accumulated time on sync point = 0.17 seconds , average latency = 0.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.77-seconds latency this time; accumulated time on sync point = 1.77 seconds , average latency = 1.77 seconds
Finished Epoch[92 of 160]: [Training] Finished Epoch[92 of 160]: [Training] ce = 0.21967752 * 50000; errs = 7.654% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.75094s
Finished Epoch[92 of 160]: [Training] ce = 0.21967752 * 50000; errs = 7.654% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.75095s
Finished Epoch[92 of 160]: [Training] ce = 0.21967752 * 50000; errs = 7.654% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.75095s
ce = 0.21967752 * 50000; errs = 7.654% * 50000; totalSamplesSeen = 4600000; learningRatePerSample = 0.003125; epochTime=9.75094s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.02-seconds latency this time; accumulated time on sync point = 2.02 seconds , average latency = 2.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.31-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.31 seconds
Finished Epoch[93 of 160]: [Training] Finished Epoch[93 of 160]: [Training] ce = 0.21269048 * 50000; errs = 7.330% * 50000; totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=9.69725s
Finished Epoch[93 of 160]: [Training] ce = 0.21269048 * 50000; errs = 7.330% * 50000; totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=9.69426s
Finished Epoch[93 of 160]: [Training] ce = 0.21269048 * 50000; errs = 7.330% * 50000; totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=9.69725s
ce = 0.21269048 * 50000; errs = 7.330% * 50000; totalSamplesSeen = 4650000; learningRatePerSample = 0.003125; epochTime=9.69426s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.38-seconds latency this time; accumulated time on sync point = 1.38 seconds , average latency = 1.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.79-seconds latency this time; accumulated time on sync point = 0.79 seconds , average latency = 0.79 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
Finished Epoch[94 of 160]: [Training] ce = 0.20441422 * 50000; errs = 7.114% * 50000; totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.33226s
Finished Epoch[94 of 160]: [Training] ce = 0.20441422 * 50000; errs = 7.114% * 50000; totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.33225s
Finished Epoch[94 of 160]: [Training] ce = 0.20441422 * 50000; errs = 7.114% * 50000; totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.33226s
Finished Epoch[94 of 160]: [Training] ce = 0.20441422 * 50000; errs = 7.114% * 50000; totalSamplesSeen = 4700000; learningRatePerSample = 0.003125; epochTime=9.32296s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.99-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 1.99 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.47-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 1.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[95 of 160]: [Training] ce = 0.19510983 * 50000; errs = 6.684% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.78569s
Finished Epoch[95 of 160]: [Training] ce = 0.19510983 * 50000; errs = 6.684% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.7857s
Finished Epoch[95 of 160]: [Training] Finished Epoch[95 of 160]: [Training] ce = 0.19510983 * 50000; errs = 6.684% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.7857s
ce = 0.19510983 * 50000; errs = 6.684% * 50000; totalSamplesSeen = 4750000; learningRatePerSample = 0.003125; epochTime=9.78569s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.84-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.84 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.07-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 1.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
Finished Epoch[96 of 160]: [Training] Finished Epoch[96 of 160]: [Training] ce = 0.19231761 * 50000; errs = 6.840% * 50000; totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=8.81593s
Finished Epoch[96 of 160]: [Training] ce = 0.19231761 * 50000; errs = 6.840% * 50000; totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=8.81594s
Finished Epoch[96 of 160]: [Training] ce = 0.19231761 * 50000; errs = 6.840% * 50000; totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=8.81594s
ce = 0.19231761 * 50000; errs = 6.840% * 50000; totalSamplesSeen = 4800000; learningRatePerSample = 0.003125; epochTime=8.81593s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.24-seconds latency this time; accumulated time on sync point = 1.24 seconds , average latency = 1.24 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.80-seconds latency this time; accumulated time on sync point = 1.80 seconds , average latency = 1.80 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.36-seconds latency this time; accumulated time on sync point = 0.36 seconds , average latency = 0.36 seconds
Finished Epoch[97 of 160]: [Training] ce = 0.18215892 * 50000; errs = 6.250% * 50000; totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=9.72142s
Finished Epoch[97 of 160]: [Training] ce = 0.18215892 * 50000; errs = 6.250% * 50000; totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=9.72135s
Finished Epoch[97 of 160]: [Training] ce = 0.18215892 * 50000; errs = 6.250% * 50000; totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=9.72142s
Finished Epoch[97 of 160]: [Training] ce = 0.18215892 * 50000; errs = 6.250% * 50000; totalSamplesSeen = 4850000; learningRatePerSample = 0.003125; epochTime=9.72136s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.02-seconds latency this time; accumulated time on sync point = 3.02 seconds , average latency = 3.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 3.21-seconds latency this time; accumulated time on sync point = 3.21 seconds , average latency = 3.21 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.05-seconds latency this time; accumulated time on sync point = 2.05 seconds , average latency = 2.05 seconds
Finished Epoch[98 of 160]: [Training] ce = 0.18225118 * 50000; errs = 6.380% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=10.8086s
Finished Epoch[98 of 160]: [Training] ce = 0.18225118 * 50000; errs = 6.380% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=10.8086s
Finished Epoch[98 of 160]: [Training] ce = 0.18225118 * 50000; errs = 6.380% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=10.8086s
Finished Epoch[98 of 160]: [Training] ce = 0.18225118 * 50000; errs = 6.380% * 50000; totalSamplesSeen = 4900000; learningRatePerSample = 0.003125; epochTime=10.8086s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.26-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 1.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.55-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.55 seconds
Finished Epoch[99 of 160]: [Training] ce = 0.17350555 * 50000; errs = 6.012% * 50000; Finished Epoch[99 of 160]: [Training] Finished Epoch[99 of 160]: [Training] ce = 0.17350555 * 50000; errs = 6.012% * 50000; totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=9.37754s
Finished Epoch[99 of 160]: [Training] ce = 0.17350555 * 50000; errs = 6.012% * 50000; totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=9.37755s
totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=9.37754s
ce = 0.17350555 * 50000; errs = 6.012% * 50000; totalSamplesSeen = 4950000; learningRatePerSample = 0.003125; epochTime=9.37753s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.09 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.01-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 1.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.07-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 1.07 seconds
Finished Epoch[100 of 160]: [Training] Finished Epoch[100 of 160]: [Training] ce = 0.16957222 * 50000; errs = 5.896% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.12463s
ce = 0.16957222 * 50000Finished Epoch[100 of 160]: [Training] Finished Epoch[100 of 160]: [Training] ce = 0.16957222 * 50000; errs = 5.896% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.12493s
; errs = 5.896% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.12463s
ce = 0.16957222 * 50000; errs = 5.896% * 50000; totalSamplesSeen = 5000000; learningRatePerSample = 0.003125; epochTime=9.12491s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.91-seconds latency this time; accumulated time on sync point = 0.91 seconds , average latency = 0.91 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.00-seconds latency this time; accumulated time on sync point = 2.00 seconds , average latency = 2.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.13-seconds latency this time; accumulated time on sync point = 2.13 seconds , average latency = 2.13 seconds
Finished Epoch[101 of 160]: [Training] ce = 0.16539437 * 50000; errs = 5.836% * 50000; totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=10.0501s
Finished Epoch[101 of 160]: [Training] Finished Epoch[101 of 160]: [Training] ce = 0.16539437 * 50000; errs = 5.836% * 50000; totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=10.0501s
Finished Epoch[101 of 160]: [Training] ce = 0.16539437 * 50000; errs = 5.836% * 50000; totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=10.0501s
ce = 0.16539437 * 50000; errs = 5.836% * 50000; totalSamplesSeen = 5050000; learningRatePerSample = 0.003125; epochTime=10.0501s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.94-seconds latency this time; accumulated time on sync point = 0.94 seconds , average latency = 0.94 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.20-seconds latency this time; accumulated time on sync point = 0.20 seconds , average latency = 0.20 seconds
Finished Epoch[102 of 160]: [Training] ce = 0.16322527 * 50000; errs = 5.728% * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.36179s
Finished Epoch[102 of 160]: [Training] ce = 0.16322527 * 50000; errs = 5.728% * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.3595s
Finished Epoch[102 of 160]: [Training] ce = 0.16322527 * 50000; errs = 5.728% * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.36178s
Finished Epoch[102 of 160]: [Training] ce = 0.16322527 * 50000; errs = 5.728% * 50000; totalSamplesSeen = 5100000; learningRatePerSample = 0.003125; epochTime=9.36178s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.33-seconds latency this time; accumulated time on sync point = 1.33 seconds , average latency = 1.33 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.47-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 1.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.07-seconds latency this time; accumulated time on sync point = 0.07 seconds , average latency = 0.07 seconds
Finished Epoch[103 of 160]: [Training] Finished Epoch[103 of 160]: [Training] ce = 0.15492347 * 50000; errs = 5.464% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.22013s
Finished Epoch[103 of 160]: [Training] ce = 0.15492347 * 50000; errs = 5.464% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.22013s
Finished Epoch[103 of 160]: [Training] ce = 0.15492347 * 50000; errs = 5.464% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.22014s
ce = 0.15492347 * 50000; errs = 5.464% * 50000; totalSamplesSeen = 5150000; learningRatePerSample = 0.003125; epochTime=9.22012s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.29-seconds latency this time; accumulated time on sync point = 1.29 seconds , average latency = 1.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.41-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 1.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.10-seconds latency this time; accumulated time on sync point = 0.10 seconds , average latency = 0.10 seconds
Finished Epoch[104 of 160]: [Training] ce = 0.15772380 * 50000; errs = 5.566% * 50000; totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=9.60522s
Finished Epoch[104 of 160]: [Training] ce = 0.15772380 * 50000; errs = 5.566% * 50000; totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=9.60522s
Finished Epoch[104 of 160]: [Training] ce = 0.15772380 * 50000; errs = 5.566% * 50000; totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=9.60522s
Finished Epoch[104 of 160]: [Training] ce = 0.15772380 * 50000; errs = 5.566% * 50000; totalSamplesSeen = 5200000; learningRatePerSample = 0.003125; epochTime=9.60523s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.83-seconds latency this time; accumulated time on sync point = 1.83 seconds , average latency = 1.83 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.40-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 1.40 seconds
Finished Epoch[105 of 160]: [Training] ce = Finished Epoch[105 of 160]: [Training] ce = 0.15252437 * 50000; errs = 5.302% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.1557s
Finished Epoch[105 of 160]: [Training] ce = 0.15252437 * 50000; errs = 5.302% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.1557s
Finished Epoch[105 of 160]: [Training] 0.15252437 * 50000; errs = 5.302% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.15572s
ce = 0.15252437 * 50000; errs = 5.302% * 50000; totalSamplesSeen = 5250000; learningRatePerSample = 0.003125; epochTime=9.15575s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.10-seconds latency this time; accumulated time on sync point = 1.10 seconds , average latency = 1.10 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.28-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.28 seconds
Finished Epoch[106 of 160]: [Training] ce = 0.15203823 * 50000; errs = 5.326% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.15025s
Finished Epoch[106 of 160]: [Training] ce = 0.15203823 * 50000; errs = 5.326% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.15026s
Finished Epoch[106 of 160]: [Training] ce = 0.15203823 * 50000; errs = 5.326% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.15026s
Finished Epoch[106 of 160]: [Training] ce = 0.15203823 * 50000; errs = 5.326% * 50000; totalSamplesSeen = 5300000; learningRatePerSample = 0.003125; epochTime=9.15025s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.59-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 1.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.57-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 1.57 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
Finished Epoch[107 of 160]: [Training] ce = 0.14708790 * 50000; errs = 5.160% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.41533s
Finished Epoch[107 of 160]: [Training] Finished Epoch[107 of 160]: [Training] ce = 0.14708790 * 50000; errs = 5.160% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.41533s
Finished Epoch[107 of 160]: [Training] ce = 0.14708790 * 50000; errs = 5.160% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.41532s
ce = 0.14708790 * 50000; errs = 5.160% * 50000; totalSamplesSeen = 5350000; learningRatePerSample = 0.003125; epochTime=9.41532s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.11-seconds latency this time; accumulated time on sync point = 2.11 seconds , average latency = 2.11 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.97-seconds latency this time; accumulated time on sync point = 1.97 seconds , average latency = 1.97 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.34-seconds latency this time; accumulated time on sync point = 0.34 seconds , average latency = 0.34 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[108 of 160]: [Training] ce = 0.13977176 * 50000; errs = 4.864% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.86343s
Finished Epoch[108 of 160]: [Training] Finished Epoch[108 of 160]: [Training] ce = 0.13977176 * 50000; errs = 4.864% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.86343s
ce = 0.13977176Finished Epoch[108 of 160]: [Training] ce = 0.13977176 * 50000; errs = 4.864% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.86557s
 * 50000; errs = 4.864% * 50000; totalSamplesSeen = 5400000; learningRatePerSample = 0.003125; epochTime=9.86515s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.69-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 1.69 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.13-seconds latency this time; accumulated time on sync point = 2.13 seconds , average latency = 2.13 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.21-seconds latency this time; accumulated time on sync point = 0.21 seconds , average latency = 0.21 seconds
Finished Epoch[109 of 160]: [Training] ce = 0.13700588 * 50000; errs = 4.774% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.56714s
Finished Epoch[109 of 160]: [Training] ce = 0.13700588 * 50000; errs = 4.774% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.56715s
Finished Epoch[109 of 160]: [Training] ce = 0.13700588 * 50000; errs = 4.774% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.56714s
Finished Epoch[109 of 160]: [Training] ce = 0.13700588 * 50000; errs = 4.774% * 50000; totalSamplesSeen = 5450000; learningRatePerSample = 0.003125; epochTime=9.56715s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
Finished Epoch[110 of 160]: [Training] ce = 0.13745172 * 50000; errs = 4.886% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.49052s
Finished Epoch[110 of 160]: [Training] ce = 0.13745172 * 50000; errs = 4.886% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.49051s
Finished Epoch[110 of 160]: [Training] ce = 0.13745172 * 50000; errs = 4.886% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.4905s
Finished Epoch[110 of 160]: [Training] ce = 0.13745172 * 50000; errs = 4.886% * 50000; totalSamplesSeen = 5500000; learningRatePerSample = 0.003125; epochTime=9.49051s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.61-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.61 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.46-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 1.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[111 of 160]: [Training] ce = 0.13376644 * 50000; errs = 4.762% * 50000; totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=9.69524s
Finished Epoch[111 of 160]: [Training] ce = 0.13376644 * 50000; errs = 4.762% * 50000; totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=9.69525s
Finished Epoch[111 of 160]: [Training] ce = 0.13376644 * 50000; errs = 4.762% * 50000; totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=9.695s
Finished Epoch[111 of 160]: [Training] ce = 0.13376644 * 50000; errs = 4.762% * 50000; totalSamplesSeen = 5550000; learningRatePerSample = 0.003125; epochTime=9.69524s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.47-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.09-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 1.09 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.18-seconds latency this time; accumulated time on sync point = 1.18 seconds , average latency = 1.18 seconds
Finished Epoch[112 of 160]: [Training] ce = 0.13311103 * 50000; errs = 4.684% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.05962s
Finished Epoch[112 of 160]: [Training] Finished Epoch[112 of 160]: [Training] ce = 0.13311103 * 50000; errs = 4.684% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.05962s
Finished Epoch[112 of 160]: [Training] ce = 0.13311103 * 50000; errs = 4.684% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.05963s
ce = 0.13311103 * 50000; errs = 4.684% * 50000; totalSamplesSeen = 5600000; learningRatePerSample = 0.003125; epochTime=9.05936s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.06-seconds latency this time; accumulated time on sync point = 2.06 seconds , average latency = 2.06 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.03-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 2.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.71-seconds latency this time; accumulated time on sync point = 0.71 seconds , average latency = 0.71 seconds
Finished Epoch[113 of 160]: [Training] Finished Epoch[113 of 160]: [Training] ce = 0.13026196 * 50000; errs = 4.596% * 50000; totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=9.82752s
Finished Epoch[113 of 160]: [Training] ce = 0.13026196 * 50000; errs = 4.596% * 50000; totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=9.82753s
Finished Epoch[113 of 160]: [Training] ce = 0.13026196 * 50000; errs = 4.596% * 50000; totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=9.82752s
ce = 0.13026196 * 50000; errs = 4.596% * 50000; totalSamplesSeen = 5650000; learningRatePerSample = 0.003125; epochTime=9.82171s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.29-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.29 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.69-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 1.69 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
Finished Epoch[114 of 160]: [Training] ce = 0.13031018 * 50000; errs = 4.592% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.80949s
Finished Epoch[114 of 160]: [Training] ce = 0.13031018 * 50000; errs = 4.592% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.8095s
Finished Epoch[114 of 160]: [Training] ce = 0.13031018 * 50000; errs = 4.592% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.81272s
Finished Epoch[114 of 160]: [Training] ce = 0.13031018 * 50000; errs = 4.592% * 50000; totalSamplesSeen = 5700000; learningRatePerSample = 0.003125; epochTime=9.81271s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.51-seconds latency this time; accumulated time on sync point = 1.51 seconds , average latency = 1.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.05-seconds latency this time; accumulated time on sync point = 0.05 seconds , average latency = 0.05 seconds
Finished Epoch[115 of 160]: [Training] ce = Finished Epoch[115 of 160]: [Training] 0.12398017 * 50000; errs = 4.342% * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=9.24702s
ce = 0.12398017Finished Epoch[115 of 160]: [Training] ce = 0.12398017 * 50000; errs = 4.342% * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=9.2473s
 * 50000; errs = 4.342%Finished Epoch[115 of 160]: [Training] ce = 0.12398017 * 50000; errs = 4.342% * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=9.24742s
 * 50000; totalSamplesSeen = 5750000; learningRatePerSample = 0.003125; epochTime=9.24701s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.30-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.59-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 1.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.96-seconds latency this time; accumulated time on sync point = 1.96 seconds , average latency = 1.96 seconds
Finished Epoch[116 of 160]: [Training] ce = 0.12486081 * 50000; errs = 4.486% * 50000; totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.29261s
Finished Epoch[116 of 160]: [Training] ce = 0.12486081 * 50000; errs = 4.486% * 50000; totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.29261s
Finished Epoch[116 of 160]: [Training] Finished Epoch[116 of 160]: [Training] ce = 0.12486081 * 50000; errs = 4.486% * 50000; totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.29263s
ce = 0.12486081 * 50000; errs = 4.486% * 50000; totalSamplesSeen = 5800000; learningRatePerSample = 0.003125; epochTime=9.29261s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.73-seconds latency this time; accumulated time on sync point = 1.73 seconds , average latency = 1.73 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.63-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 1.63 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.31-seconds latency this time; accumulated time on sync point = 0.31 seconds , average latency = 0.31 seconds
Finished Epoch[117 of 160]: [Training] Finished Epoch[117 of 160]: [Training] ce = 0.12638131 * 50000; errs = 4.538% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.84114s
Finished Epoch[117 of 160]: [Training] ce = 0.12638131 * 50000; errs = 4.538% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.84114s
Finished Epoch[117 of 160]: [Training] ce = 0.12638131 * 50000; errs = 4.538% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.84114s
ce = 0.12638131 * 50000; errs = 4.538% * 50000; totalSamplesSeen = 5850000; learningRatePerSample = 0.003125; epochTime=9.84113s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.94-seconds latency this time; accumulated time on sync point = 1.94 seconds , average latency = 1.94 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.55-seconds latency this time; accumulated time on sync point = 1.55 seconds , average latency = 1.55 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.25-seconds latency this time; accumulated time on sync point = 0.25 seconds , average latency = 0.25 seconds
Finished Epoch[118 of 160]: [Training] Finished Epoch[118 of 160]: [Training] ce = 0.12548613 * 50000; errs = 4.368% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=9.39202s
Finished Epoch[118 of 160]: [Training] ce = 0.12548613 * 50000; errs = 4.368% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=9.39203s
Finished Epoch[118 of 160]: [Training] ce = 0.12548613 * 50000; errs = 4.368% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=9.39203s
ce = 0.12548613 * 50000; errs = 4.368% * 50000; totalSamplesSeen = 5900000; learningRatePerSample = 0.003125; epochTime=9.39202s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.97-seconds latency this time; accumulated time on sync point = 1.97 seconds , average latency = 1.97 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.00-seconds latency this time; accumulated time on sync point = 2.00 seconds , average latency = 2.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.87-seconds latency this time; accumulated time on sync point = 0.87 seconds , average latency = 0.87 seconds
Finished Epoch[119 of 160]: [Training] ce = 0.12263035 * 50000; errs = 4.408% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=10.1626s
Finished Epoch[119 of 160]: [Training] ce = 0.12263035 * 50000; errs = 4.408% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=10.1626s
Finished Epoch[119 of 160]: [Training] ce = 0.12263035 * 50000; errs = 4.408% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=10.1626s
Finished Epoch[119 of 160]: [Training] ce = 0.12263035 * 50000; errs = 4.408% * 50000; totalSamplesSeen = 5950000; learningRatePerSample = 0.003125; epochTime=10.1626s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.80-seconds latency this time; accumulated time on sync point = 0.80 seconds , average latency = 0.80 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.94-seconds latency this time; accumulated time on sync point = 1.94 seconds , average latency = 1.94 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.13-seconds latency this time; accumulated time on sync point = 0.13 seconds , average latency = 0.13 seconds
Finished Epoch[120 of 160]: [Training] ce = 0.12469170 * 50000; errs = 4.482% * 50000; Finished Epoch[120 of 160]: [Training] ce = 0.12469170 * 50000; errs = 4.482% * 50000; totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.93633s
Finished Epoch[120 of 160]: [Training] totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.93633s
ce = 0.12469170Finished Epoch[120 of 160]: [Training] ce = 0.12469170 * 50000; errs = 4.482% * 50000; totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.93642s
 * 50000; errs = 4.482% * 50000; totalSamplesSeen = 6000000; learningRatePerSample = 0.003125; epochTime=9.93633s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.40-seconds latency this time; accumulated time on sync point = 0.40 seconds , average latency = 0.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.48-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 1.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.47-seconds latency this time; accumulated time on sync point = 1.47 seconds , average latency = 1.47 seconds
Finished Epoch[121 of 160]: [Training] Finished Epoch[121 of 160]: [Training] ce = 0.10404948 * 50000; errs = 3.578% * 50000; Finished Epoch[121 of 160]: [Training] ce = 0.10404948 * 50000; errs = 3.578% * 50000; totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.24054s
totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.24208s
ce = 0.10404948 * 50000; errs = 3.578% * 50000; totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.24207s
Finished Epoch[121 of 160]: [Training] ce = 0.10404948 * 50000; errs = 3.578% * 50000; totalSamplesSeen = 6050000; learningRatePerSample = 0.00031249999; epochTime=9.24284s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.56-seconds latency this time; accumulated time on sync point = 0.56 seconds , average latency = 0.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.99-seconds latency this time; accumulated time on sync point = 1.99 seconds , average latency = 1.99 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.72-seconds latency this time; accumulated time on sync point = 1.72 seconds , average latency = 1.72 seconds
Finished Epoch[122 of 160]: [Training] ce = Finished Epoch[122 of 160]: [Training] ce = 0.12181477 * 50000; errs = 4.308% * 50000; totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.43645s
Finished Epoch[122 of 160]: [Training] Finished Epoch[122 of 160]: [Training] ce = 0.12181477 * 50000; errs = 4.308% * 50000; totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.43647s
ce = 0.12181477 * 50000; errs = 0.12181477 * 50000; errs = 4.308% * 50000; totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.43646s
4.308% * 50000; totalSamplesSeen = 6100000; learningRatePerSample = 0.00031249999; epochTime=9.43646s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.78-seconds latency this time; accumulated time on sync point = 1.78 seconds , average latency = 1.78 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.91-seconds latency this time; accumulated time on sync point = 1.91 seconds , average latency = 1.91 seconds
Finished Epoch[123 of 160]: [Training] ce = 0.13044712 * 50000; errs = 4.616% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=10.0921s
Finished Epoch[123 of 160]: [Training] ce = 0.13044712 * 50000; errs = 4.616% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=10.0921s
Finished Epoch[123 of 160]: [Training] ce = 0.13044712 * 50000; errs = 4.616% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=10.0921s
Finished Epoch[123 of 160]: [Training] ce = 0.13044712 * 50000; errs = 4.616% * 50000; totalSamplesSeen = 6150000; learningRatePerSample = 0.00031249999; epochTime=10.0921s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.87-seconds latency this time; accumulated time on sync point = 1.87 seconds , average latency = 1.87 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.49-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.49 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.79-seconds latency this time; accumulated time on sync point = 1.79 seconds , average latency = 1.79 seconds
Finished Epoch[124 of 160]: [Training] ce = 0.12940708 * 50000; errs = 4.602% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.40258s
Finished Epoch[124 of 160]: [Training] ce = 0.12940708 * 50000; errs = 4.602% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.40259s
Finished Epoch[124 of 160]: [Training] ce = 0.12940708 * 50000; errs = 4.602% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.4026s
Finished Epoch[124 of 160]: [Training] ce = 0.12940708 * 50000; errs = 4.602% * 50000; totalSamplesSeen = 6200000; learningRatePerSample = 0.00031249999; epochTime=9.40258s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.33-seconds latency this time; accumulated time on sync point = 0.33 seconds , average latency = 0.33 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.37-seconds latency this time; accumulated time on sync point = 1.37 seconds , average latency = 1.37 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[125 of 160]: [Training] Finished Epoch[125 of 160]: [Training] ce = 0.12034103 * 50000; errs = 4.322% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=9.54199s
Finished Epoch[125 of 160]: [Training] ce = 0.12034103 * 50000; errs = 4.322% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=9.542s
Finished Epoch[125 of 160]: [Training] ce = 0.12034103 * 50000; errs = 4.322% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=9.54199s
ce = 0.12034103 * 50000; errs = 4.322% * 50000; totalSamplesSeen = 6250000; learningRatePerSample = 0.00031249999; epochTime=9.542s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.27-seconds latency this time; accumulated time on sync point = 1.27 seconds , average latency = 1.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
Finished Epoch[126 of 160]: [Training] Finished Epoch[126 of 160]: [Training] ce = 0.11119643 * 50000; errs = 3.912% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.17982s
ce = 0.11119643 * 50000; errs = 3.912% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.17982s
Finished Epoch[126 of 160]: [Training] ce = 0.11119643 * 50000; errs = 3.912% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.18006s
Finished Epoch[126 of 160]: [Training] ce = 0.11119643 * 50000; errs = 3.912% * 50000; totalSamplesSeen = 6300000; learningRatePerSample = 0.00031249999; epochTime=9.17117s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.46-seconds latency this time; accumulated time on sync point = 1.46 seconds , average latency = 1.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.48-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 1.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.16-seconds latency this time; accumulated time on sync point = 0.16 seconds , average latency = 0.16 seconds
Finished Epoch[127 of 160]: [Training] ce = 0.11002785 * 50000; errs = 3.858% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.46578s
Finished Epoch[127 of 160]: [Training] ce = 0.11002785 * 50000; errs = Finished Epoch[127 of 160]: [Training] Finished Epoch[127 of 160]: [Training] ce = 0.11002785 * 50000; errs = 3.858% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.46579s
3.858% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.46578s
ce = 0.11002785 * 50000; errs = 3.858% * 50000; totalSamplesSeen = 6350000; learningRatePerSample = 0.00031249999; epochTime=9.46425s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.63-seconds latency this time; accumulated time on sync point = 1.63 seconds , average latency = 1.63 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.47-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[128 of 160]: [Training] Finished Epoch[128 of 160]: [Training] ce = 0.10296782 * 50000; errs = 3.626% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=9.45452s
Finished Epoch[128 of 160]: [Training] ce = 0.10296782 * 50000; errs = 3.626% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=9.45453s
Finished Epoch[128 of 160]: [Training] ce = 0.10296782 * 50000; errs = 3.626% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=9.45452s
ce = 0.10296782 * 50000; errs = 3.626% * 50000; totalSamplesSeen = 6400000; learningRatePerSample = 0.00031249999; epochTime=9.45451s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.52-seconds latency this time; accumulated time on sync point = 1.52 seconds , average latency = 1.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.18-seconds latency this time; accumulated time on sync point = 0.18 seconds , average latency = 0.18 seconds
Finished Epoch[129 of 160]: [Training] Finished Epoch[129 of 160]: [Training] ce = 0.10124179 * 50000; errs = 3.546% * 50000; totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=9.22573s
Finished Epoch[129 of 160]: [Training] ce = 0.10124179 * 50000; errs = 3.546% * 50000; totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=9.22573s
Finished Epoch[129 of 160]: [Training] ce = 0.10124179 * 50000; errs = 3.546% * 50000; totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=9.22573s
ce = 0.10124179 * 50000; errs = 3.546% * 50000; totalSamplesSeen = 6450000; learningRatePerSample = 0.00031249999; epochTime=9.22573s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.84-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 1.84 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.84-seconds latency this time; accumulated time on sync point = 1.84 seconds , average latency = 1.84 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.29-seconds latency this time; accumulated time on sync point = 0.29 seconds , average latency = 0.29 seconds
Finished Epoch[130 of 160]: [Training] ce = 0.09799906 * 50000; errs = 3.410% * 50000; Finished Epoch[130 of 160]: [Training] ce = 0.09799906 * 50000; errs = 3.410% * 50000; totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.86472s
Finished Epoch[130 of 160]: [Training] Finished Epoch[130 of 160]: [Training] ce = 0.09799906 * 50000; errs = 3.410% * 50000; totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.86668s
totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.86668s
ce = 0.09799906 * 50000; errs = 3.410% * 50000; totalSamplesSeen = 6500000; learningRatePerSample = 0.00031249999; epochTime=9.86667s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.57-seconds latency this time; accumulated time on sync point = 1.57 seconds , average latency = 1.57 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.60-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 1.60 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[131 of 160]: [Training] Finished Epoch[131 of 160]: [Training] ce = 0.09365959 * 50000; errs = 3.226% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=9.27087s
Finished Epoch[131 of 160]: [Training] ce = 0.09365959 * 50000; errs = 3.226% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=9.27305s
Finished Epoch[131 of 160]: [Training] ce = 0.09365959 * 50000; errs = 3.226% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=9.27278s
ce = 0.09365959 * 50000; errs = 3.226% * 50000; totalSamplesSeen = 6550000; learningRatePerSample = 0.00031249999; epochTime=9.27087s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.43-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.61-seconds latency this time; accumulated time on sync point = 0.61 seconds , average latency = 0.61 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.27-seconds latency this time; accumulated time on sync point = 2.27 seconds , average latency = 2.27 seconds
Finished Epoch[132 of 160]: [Training] Finished Epoch[132 of 160]: [Training] ce = 0.08941016 * 50000; errs = 3.064% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=9.69321s
Finished Epoch[132 of 160]: [Training] ce = 0.08941016 * 50000; errs = 3.064% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=9.69321s
Finished Epoch[132 of 160]: [Training] ce = 0.08941016 * 50000; errs = 3.064% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=9.69321s
ce = 0.08941016 * 50000; errs = 3.064% * 50000; totalSamplesSeen = 6600000; learningRatePerSample = 0.00031249999; epochTime=9.69321s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.06-seconds latency this time; accumulated time on sync point = 0.06 seconds , average latency = 0.06 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.11-seconds latency this time; accumulated time on sync point = 1.11 seconds , average latency = 1.11 seconds
Finished Epoch[133 of 160]: [Training] Finished Epoch[133 of 160]: [Training] ce = 0.08867216 * 50000; errs = 3.022% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=9.24293s
Finished Epoch[133 of 160]: [Training] ce = 0.08867216 * 50000; errs = 3.022% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=9.24293s
Finished Epoch[133 of 160]: [Training] ce = 0.08867216 * 50000; errs = 3.022% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=9.24294s
ce = 0.08867216 * 50000; errs = 3.022% * 50000; totalSamplesSeen = 6650000; learningRatePerSample = 0.00031249999; epochTime=9.24293s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.27-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.85-seconds latency this time; accumulated time on sync point = 1.85 seconds , average latency = 1.85 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.04-seconds latency this time; accumulated time on sync point = 0.04 seconds , average latency = 0.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.93-seconds latency this time; accumulated time on sync point = 1.93 seconds , average latency = 1.93 seconds
Finished Epoch[134 of 160]: [Training] Finished Epoch[134 of 160]: [Training] ce = 0.08406672 * 50000; errs = 2.830% * 50000; Finished Epoch[134 of 160]: [Training] ce = 0.08406672 * 50000; errs = 2.830% * 50000; totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.80199s
Finished Epoch[134 of 160]: [Training] ce = 0.08406672 * 50000; errs = 2.830% * 50000; totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.80199s
totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.802s
ce = 0.08406672 * 50000; errs = 2.830% * 50000; totalSamplesSeen = 6700000; learningRatePerSample = 0.00031249999; epochTime=9.80199s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.47-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.32-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 1.32 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
Finished Epoch[135 of 160]: [Training] Finished Epoch[135 of 160]: [Training] ce = 0.08218695 * 50000; errs = 2.770% * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=9.32892s
Finished Epoch[135 of 160]: [Training] ce = 0.08218695 * 50000; errs = 2.770% * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=9.32893s
Finished Epoch[135 of 160]: [Training] ce = 0.08218695 * 50000; errs = 2.770% * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=9.32893s
ce = 0.08218695 * 50000; errs = 2.770% * 50000; totalSamplesSeen = 6750000; learningRatePerSample = 0.00031249999; epochTime=9.32892s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.98-seconds latency this time; accumulated time on sync point = 0.98 seconds , average latency = 0.98 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.19-seconds latency this time; accumulated time on sync point = 1.19 seconds , average latency = 1.19 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.03-seconds latency this time; accumulated time on sync point = 0.03 seconds , average latency = 0.03 seconds
Finished Epoch[136 of 160]: [Training] ce = 0.08088837 * 50000; errs = 2.798% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.40115s
Finished Epoch[136 of 160]: [Training] ce = 0.08088837 * 50000; errs = 2.798% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.40122s
Finished Epoch[136 of 160]: [Training] ce = 0.08088837 * 50000; errs = 2.798% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.40115s
Finished Epoch[136 of 160]: [Training] ce = 0.08088837 * 50000; errs = 2.798% * 50000; totalSamplesSeen = 6800000; learningRatePerSample = 0.00031249999; epochTime=9.40124s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.41-seconds latency this time; accumulated time on sync point = 0.41 seconds , average latency = 0.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.22-seconds latency this time; accumulated time on sync point = 1.22 seconds , average latency = 1.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.12-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 1.12 seconds
Finished Epoch[137 of 160]: [Training] ce = 0.07815202 * 50000; errs = 2.690% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.15578s
Finished Epoch[137 of 160]: [Training] ce = 0.07815202 * 50000; errs = 2.690% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.15575s
Finished Epoch[137 of 160]: [Training] ce = 0.07815202 * 50000; errs = 2.690% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.15575s
Finished Epoch[137 of 160]: [Training] ce = 0.07815202 * 50000; errs = 2.690% * 50000; totalSamplesSeen = 6850000; learningRatePerSample = 0.00031249999; epochTime=9.15588s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.41-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 1.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.23-seconds latency this time; accumulated time on sync point = 0.23 seconds , average latency = 0.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.49-seconds latency this time; accumulated time on sync point = 1.49 seconds , average latency = 1.49 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[138 of 160]: [Training] ce = 0.07647342 * 50000; errs = 2.526% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=9.18782s
Finished Epoch[138 of 160]: [Training] ce = 0.07647342 * 50000; errs = 2.526% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=9.18782s
Finished Epoch[138 of 160]: [Training] Finished Epoch[138 of 160]: [Training] ce = 0.07647342 * 50000; errs = 2.526% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=9.18783s
ce = 0.07647342 * 50000; errs = 2.526% * 50000; totalSamplesSeen = 6900000; learningRatePerSample = 0.00031249999; epochTime=9.18782s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.15-seconds latency this time; accumulated time on sync point = 0.15 seconds , average latency = 0.15 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.30-seconds latency this time; accumulated time on sync point = 1.30 seconds , average latency = 1.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.40-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 1.40 seconds
Finished Epoch[139 of 160]: [Training] ce = 0.07666546 * 50000; errs = 2.626% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.09637s
Finished Epoch[139 of 160]: [Training] ce = 0.07666546 * 50000; errs = 2.626% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.09637s
Finished Epoch[139 of 160]: [Training] ce = 0.07666546 * 50000; errs = 2.626% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.09637s
Finished Epoch[139 of 160]: [Training] ce = 0.07666546 * 50000; errs = 2.626% * 50000; totalSamplesSeen = 6950000; learningRatePerSample = 0.00031249999; epochTime=9.09637s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.52-seconds latency this time; accumulated time on sync point = 0.52 seconds , average latency = 0.52 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.22-seconds latency this time; accumulated time on sync point = 2.22 seconds , average latency = 2.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.95-seconds latency this time; accumulated time on sync point = 1.95 seconds , average latency = 1.95 seconds
Finished Epoch[140 of 160]: [Training] ce = 0.07590449 * 50000; errs = 2.544% * 50000; Finished Epoch[140 of 160]: [Training] ce = 0.07590449 * 50000; errs = 2.544% * 50000; Finished Epoch[140 of 160]: [Training] totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.83519s
totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.83496s
Finished Epoch[140 of 160]: [Training] ce = 0.07590449 * 50000; errs = 2.544% * 50000; totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.83521s
ce = 0.07590449 * 50000; errs = 2.544% * 50000; totalSamplesSeen = 7000000; learningRatePerSample = 0.00031249999; epochTime=9.83495s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.28-seconds latency this time; accumulated time on sync point = 0.28 seconds , average latency = 0.28 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.07-seconds latency this time; accumulated time on sync point = 2.07 seconds , average latency = 2.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[141 of 160]: [Training] Finished Epoch[141 of 160]: [Training] ce = 0.07351858 * 50000; errs = 2.498% * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.76533s
Finished Epoch[141 of 160]: [Training] ce = 0.07351858 * 50000; errs = 2.498% * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.76533s
Finished Epoch[141 of 160]: [Training] ce = 0.07351858 * 50000; errs = 2.498% * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.76533s
ce = 0.07351858 * 50000; errs = 2.498% * 50000; totalSamplesSeen = 7050000; learningRatePerSample = 0.00031249999; epochTime=9.76533s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.30-seconds latency this time; accumulated time on sync point = 0.30 seconds , average latency = 0.30 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.53-seconds latency this time; accumulated time on sync point = 1.53 seconds , average latency = 1.53 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.68-seconds latency this time; accumulated time on sync point = 1.68 seconds , average latency = 1.68 seconds
Finished Epoch[142 of 160]: [Training] Finished Epoch[142 of 160]: [Training] ce = 0.07543525 * 50000; errs = 2.556% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=9.64436s
Finished Epoch[142 of 160]: [Training] ce = 0.07543525 * 50000; errs = 2.556% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=9.64439s
Finished Epoch[142 of 160]: [Training] ce = 0.07543525 * 50000; errs = 2.556% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=9.64447s
ce = 0.07543525 * 50000; errs = 2.556% * 50000; totalSamplesSeen = 7100000; learningRatePerSample = 0.00031249999; epochTime=9.64444s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.35-seconds latency this time; accumulated time on sync point = 0.35 seconds , average latency = 0.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.41-seconds latency this time; accumulated time on sync point = 1.41 seconds , average latency = 1.41 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.07-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 1.07 seconds
Finished Epoch[143 of 160]: [Training] Finished Epoch[143 of 160]: [Training] ce = 0.07227048 * 50000; errs = 2.388% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.05021s
Finished Epoch[143 of 160]: [Training] ce = 0.07227048 * 50000; errs = 2.388% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.05011s
Finished Epoch[143 of 160]: [Training] ce = 0.07227048 * 50000; errs = 2.388% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.05056s
ce = 0.07227048 * 50000; errs = 2.388% * 50000; totalSamplesSeen = 7150000; learningRatePerSample = 0.00031249999; epochTime=9.0503s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.77-seconds latency this time; accumulated time on sync point = 0.77 seconds , average latency = 0.77 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.54-seconds latency this time; accumulated time on sync point = 1.54 seconds , average latency = 1.54 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[144 of 160]: [Training] ce = 0.07209027 * 50000; errs = 2.412% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.38995s
Finished Epoch[144 of 160]: [Training] Finished Epoch[144 of 160]: [Training] ce = 0.07209027 * 50000; errs = 2.412% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.38997s
Finished Epoch[144 of 160]: [Training] ce = 0.07209027 * 50000; errs = 2.412% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.38999s
ce = 0.07209027 * 50000; errs = 2.412% * 50000; totalSamplesSeen = 7200000; learningRatePerSample = 0.00031249999; epochTime=9.38998s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.40-seconds latency this time; accumulated time on sync point = 1.40 seconds , average latency = 1.40 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.42-seconds latency this time; accumulated time on sync point = 1.42 seconds , average latency = 1.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.02-seconds latency this time; accumulated time on sync point = 0.02 seconds , average latency = 0.02 seconds
Finished Epoch[145 of 160]: [Training] ce = 0.07008494 * 50000; errs = 2.360% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.62915s
Finished Epoch[145 of 160]: [Training] ce = 0.07008494 * 50000; errs = 2.360% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.62915s
Finished Epoch[145 of 160]: [Training] Finished Epoch[145 of 160]: [Training] ce = 0.07008494 * 50000; errs = 2.360% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.62915s
ce = 0.07008494 * 50000; errs = 2.360% * 50000; totalSamplesSeen = 7250000; learningRatePerSample = 0.00031249999; epochTime=9.62914s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.00-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 1.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.97-seconds latency this time; accumulated time on sync point = 1.97 seconds , average latency = 1.97 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.38-seconds latency this time; accumulated time on sync point = 2.38 seconds , average latency = 2.38 seconds
Finished Epoch[146 of 160]: [Training] ce = 0.06950934 * 50000; errs = 2.346% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.72151s
Finished Epoch[146 of 160]: [Training] Finished Epoch[146 of 160]: [Training] ce = 0.06950934 * 50000; errs = 2.346% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.7215s
Finished Epoch[146 of 160]: [Training] ce = 0.06950934 * 50000; errs = 2.346% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.72152s
ce = 0.06950934 * 50000; errs = 2.346% * 50000; totalSamplesSeen = 7300000; learningRatePerSample = 0.00031249999; epochTime=9.72151s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.67-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.31-seconds latency this time; accumulated time on sync point = 1.31 seconds , average latency = 1.31 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.09-seconds latency this time; accumulated time on sync point = 0.09 seconds , average latency = 0.09 seconds
Finished Epoch[147 of 160]: [Training] ce = 0.06712771 * 50000; errs = 2.184% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.29672s
Finished Epoch[147 of 160]: [Training] ce = 0.06712771 * 50000; errs = 2.184% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.29673s
Finished Epoch[147 of 160]: [Training] ce = 0.06712771 * 50000; errs = 2.184% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.29673s
Finished Epoch[147 of 160]: [Training] ce = 0.06712771 * 50000; errs = 2.184% * 50000; totalSamplesSeen = 7350000; learningRatePerSample = 0.00031249999; epochTime=9.29672s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.56-seconds latency this time; accumulated time on sync point = 1.56 seconds , average latency = 1.56 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.88-seconds latency this time; accumulated time on sync point = 1.88 seconds , average latency = 1.88 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.55-seconds latency this time; accumulated time on sync point = 0.55 seconds , average latency = 0.55 seconds
Finished Epoch[148 of 160]: [Training] ce = 0.06906357 * 50000; errs = 2.324% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.5713s
Finished Epoch[148 of 160]: [Training] ce = 0.06906357 * 50000; errs = 2.324% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.57127s
Finished Epoch[148 of 160]: [Training] ce = 0.06906357 * 50000; errs = 2.324% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.57131s
Finished Epoch[148 of 160]: [Training] ce = 0.06906357 * 50000; errs = 2.324% * 50000; totalSamplesSeen = 7400000; learningRatePerSample = 0.00031249999; epochTime=9.57126s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.73-seconds latency this time; accumulated time on sync point = 2.73 seconds , average latency = 2.73 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.32-seconds latency this time; accumulated time on sync point = 1.32 seconds , average latency = 1.32 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.01-seconds latency this time; accumulated time on sync point = 0.01 seconds , average latency = 0.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.50-seconds latency this time; accumulated time on sync point = 1.50 seconds , average latency = 1.50 seconds
Finished Epoch[149 of 160]: [Training] ce = 0.06827200 * 50000; errs = 2.272% * 50000; Finished Epoch[149 of 160]: [Training] ce = 0.06827200 * 50000; errs = 2.272% * 50000; totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=10.3437s
Finished Epoch[149 of 160]: [Training] ce = 0.06827200 * 50000; errs = 2.272% * 50000; totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=10.3476s
Finished Epoch[149 of 160]: [Training] totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=10.3437s
ce = 0.06827200 * 50000; errs = 2.272% * 50000; totalSamplesSeen = 7450000; learningRatePerSample = 0.00031249999; epochTime=10.3354s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.65-seconds latency this time; accumulated time on sync point = 0.65 seconds , average latency = 0.65 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.11-seconds latency this time; accumulated time on sync point = 1.11 seconds , average latency = 1.11 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[150 of 160]: [Training] ce = 0.06614516 * 50000; errs = 2.192% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.88926s
Finished Epoch[150 of 160]: [Training] ce = 0.06614516 * 50000; errs = 2.192% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.88923s
Finished Epoch[150 of 160]: [Training] ce = 0.06614516 * 50000; errs = 2.192% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.88922s
Finished Epoch[150 of 160]: [Training] ce = 0.06614516 * 50000; errs = 2.192% * 50000; totalSamplesSeen = 7500000; learningRatePerSample = 0.00031249999; epochTime=9.88926s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.80-seconds latency this time; accumulated time on sync point = 0.80 seconds , average latency = 0.80 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.76-seconds latency this time; accumulated time on sync point = 1.76 seconds , average latency = 1.76 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.60-seconds latency this time; accumulated time on sync point = 1.60 seconds , average latency = 1.60 seconds
Finished Epoch[151 of 160]: [Training] ce = 0.06374302 * 50000; errs = 2.100% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.71079s
Finished Epoch[151 of 160]: [Training] ce = 0.06374302 * 50000; errs = 2.100% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.7108s
Finished Epoch[151 of 160]: [Training] ce = 0.06374302 * 50000; errs = 2.100% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.7108s
Finished Epoch[151 of 160]: [Training] ce = 0.06374302 * 50000; errs = 2.100% * 50000; totalSamplesSeen = 7550000; learningRatePerSample = 0.00031249999; epochTime=9.71088s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.74-seconds latency this time; accumulated time on sync point = 1.74 seconds , average latency = 1.74 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.80-seconds latency this time; accumulated time on sync point = 0.80 seconds , average latency = 0.80 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.65-seconds latency this time; accumulated time on sync point = 1.65 seconds , average latency = 1.65 seconds
Finished Epoch[152 of 160]: [Training] ce = 0.06455616 * 50000; errs = 2.140% * 50000; totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=10.0173s
Finished Epoch[152 of 160]: [Training] ce = 0.06455616 * 50000; errs = 2.140% * 50000; totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=10.0173s
Finished Epoch[152 of 160]: [Training] ce = 0.06455616 * 50000; errs = 2.140% * 50000; totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=10.0173s
Finished Epoch[152 of 160]: [Training] ce = 0.06455616 * 50000; errs = 2.140% * 50000; totalSamplesSeen = 7600000; learningRatePerSample = 0.00031249999; epochTime=10.0173s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.44-seconds latency this time; accumulated time on sync point = 0.44 seconds , average latency = 0.44 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.58-seconds latency this time; accumulated time on sync point = 1.58 seconds , average latency = 1.58 seconds
Finished Epoch[153 of 160]: [Training] ce = 0.06612171 * 50000; errs = 2.224% * 50000; Finished Epoch[153 of 160]: [Training] Finished Epoch[153 of 160]: [Training] ce = 0.06612171 * 50000; errs = 2.224% * 50000; totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.95652s
Finished Epoch[153 of 160]: [Training] ce = 0.06612171 * 50000; errs = 2.224% * 50000; totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.95653s
totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.95653s
ce = 0.06612171 * 50000; errs = 2.224% * 50000; totalSamplesSeen = 7650000; learningRatePerSample = 0.00031249999; epochTime=9.95271s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.64-seconds latency this time; accumulated time on sync point = 1.64 seconds , average latency = 1.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.03-seconds latency this time; accumulated time on sync point = 2.03 seconds , average latency = 2.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.37-seconds latency this time; accumulated time on sync point = 0.37 seconds , average latency = 0.37 seconds
Finished Epoch[154 of 160]: [Training] ce = Finished Epoch[154 of 160]: [Training] 0.06492049 * 50000; errs = 2.252% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.54673s
Finished Epoch[154 of 160]: [Training] ce = 0.06492049 * 50000; errs = Finished Epoch[154 of 160]: [Training] ce = 0.06492049 * 50000; errs = 2.252% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.54674s
2.252% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.54674s
ce = 0.06492049 * 50000; errs = 2.252% * 50000; totalSamplesSeen = 7700000; learningRatePerSample = 0.00031249999; epochTime=9.54672s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.83-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.83 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.66-seconds latency this time; accumulated time on sync point = 1.66 seconds , average latency = 1.66 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.59-seconds latency this time; accumulated time on sync point = 1.59 seconds , average latency = 1.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[155 of 160]: [Training] ce = 0.06457575 * 50000; errs = 2.168% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=9.7286s
Finished Epoch[155 of 160]: [Training] ce = 0.06457575 * 50000; errs = 2.168% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=9.7286s
Finished Epoch[155 of 160]: [Training] Finished Epoch[155 of 160]: [Training] ce = 0.06457575 * 50000; errs = 2.168% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=9.72863s
ce = 0.06457575 * 50000; errs = 2.168% * 50000; totalSamplesSeen = 7750000; learningRatePerSample = 0.00031249999; epochTime=9.7286s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.21-seconds latency this time; accumulated time on sync point = 1.21 seconds , average latency = 1.21 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.39-seconds latency this time; accumulated time on sync point = 1.39 seconds , average latency = 1.39 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.67-seconds latency this time; accumulated time on sync point = 0.67 seconds , average latency = 0.67 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[156 of 160]: [Training] ce = 0.06165368 * 50000; errs = 2.084% * 50000; totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.37968s
Finished Epoch[156 of 160]: [Training] ce = 0.06165368 * 50000; errs = 2.084% * 50000; totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.37968s
Finished Epoch[156 of 160]: [Training] Finished Epoch[156 of 160]: [Training] ce = 0.06165368 * 50000; errs = 2.084% * 50000; totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.3786s
ce = 0.06165368 * 50000; errs = 2.084% * 50000; totalSamplesSeen = 7800000; learningRatePerSample = 0.00031249999; epochTime=9.37967s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.01-seconds latency this time; accumulated time on sync point = 1.01 seconds , average latency = 1.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.11-seconds latency this time; accumulated time on sync point = 0.11 seconds , average latency = 0.11 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.24-seconds latency this time; accumulated time on sync point = 1.24 seconds , average latency = 1.24 seconds
Finished Epoch[157 of 160]: [Training] Finished Epoch[157 of 160]: [Training] ce = 0.06179245 * 50000; errs = 2.060% * 50000; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.06265s
Finished Epoch[157 of 160]: [Training] ce = 0.06179245 * 50000; errs = 2.060% * 50000; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.06438s
Finished Epoch[157 of 160]: [Training] ce = 0.06179245 * 50000; errs = 2.060% * 50000; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.06438s
ce = 0.06179245 * 50000; errs = 2.060% * 50000; totalSamplesSeen = 7850000; learningRatePerSample = 0.00031249999; epochTime=9.06412s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.00-seconds latency this time; accumulated time on sync point = 1.00 seconds , average latency = 1.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.27-seconds latency this time; accumulated time on sync point = 0.27 seconds , average latency = 0.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.25-seconds latency this time; accumulated time on sync point = 1.25 seconds , average latency = 1.25 seconds
Finished Epoch[158 of 160]: [Training] Finished Epoch[158 of 160]: [Training] ce = 0.06359337 * 50000; errs = 2.104% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=8.8888s
Finished Epoch[158 of 160]: [Training] ce = 0.06359337 * 50000; errs = 2.104% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=8.88879s
Finished Epoch[158 of 160]: [Training] ce = 0.06359337 * 50000; errs = 2.104% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=8.88888s
ce = 0.06359337 * 50000; errs = 2.104% * 50000; totalSamplesSeen = 7900000; learningRatePerSample = 0.00031249999; epochTime=8.88889s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.69-seconds latency this time; accumulated time on sync point = 1.69 seconds , average latency = 1.69 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 2.78-seconds latency this time; accumulated time on sync point = 2.78 seconds , average latency = 2.78 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.26-seconds latency this time; accumulated time on sync point = 0.26 seconds , average latency = 0.26 seconds
Finished Epoch[159 of 160]: [Training] Finished Epoch[159 of 160]: [Training] ce = 0.06226715 * 50000; errs = 2.076% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=10.3733s
Finished Epoch[159 of 160]: [Training] ce = 0.06226715 * 50000; errs = 2.076% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=10.3676s
Finished Epoch[159 of 160]: [Training] ce = 0.06226715 * 50000; errs = 2.076% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=10.3733s
ce = 0.06226715 * 50000; errs = 2.076% * 50000; totalSamplesSeen = 7950000; learningRatePerSample = 0.00031249999; epochTime=10.3733s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 44493.5616, block learning rate = 1.0000, block size per worker = 12800 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.42-seconds latency this time; accumulated time on sync point = 0.42 seconds , average latency = 0.42 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.48-seconds latency this time; accumulated time on sync point = 1.48 seconds , average latency = 1.48 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 1.97-seconds latency this time; accumulated time on sync point = 1.97 seconds , average latency = 1.97 seconds
Finished Epoch[160 of 160]: [Training] ce = 0.06382215 * 50000; errs = 2.192% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.48663s
Finished Epoch[160 of 160]: [Training] ce = 0.06382215 * 50000; errs = 2.192% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.48663s
Finished Epoch[160 of 160]: [Training] ce = 0.06382215 * 50000; errs = 2.192% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.48636s
Finished Epoch[160 of 160]: [Training] ce = 0.06382215 * 50000; errs = 2.192% * 50000; totalSamplesSeen = 8000000; learningRatePerSample = 0.00031249999; epochTime=9.48665s






##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################


##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################


##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################

##############################################################################
#                                                                            #
# Eval command (eval action)                                                 #
#                                                                            #
##############################################################################

Final Results: Minibatch[1-21]: errs = Final Results: Minibatch[1-21]: errs = 8.690% * 10000; top5Errs = 0.230% * 10000
Final Results: Minibatch[1-21]: errs = 8.690% * 10000; top5Errs = 0.230% * 10000
Final Results: Minibatch[1-21]: 8.690% * 10000; top5Errs = 0.230% * 10000
errs = 8.690% * 10000; top5Errs = 0.230% * 10000

COMPLETED.

COMPLETED.

COMPLETED.

COMPLETED.
