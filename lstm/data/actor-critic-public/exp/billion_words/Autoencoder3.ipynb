{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas\n",
    "smooth = pandas.rolling_mean\n",
    "sys.path += [os.getenv('HOME') + path for path in \n",
    "             ['/Dist/fully-neural-lvsr',\n",
    "              '/Dist/fully-neural-lvsr/libs/Theano',\n",
    "              '/Dist/fully-neural-lvsr/libs/blocks']]\n",
    "from blocks.serialization import load, load_parameters\n",
    "from matplotlib import pyplot\n",
    "from pprint import pprint\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lvsr.datasets.text import char2code, code2char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cd /data/lisatmp4/bahdanau/autoencoder3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logs = {}\n",
    "dfs = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_model(path):\n",
    "    log = load(open(path), name='log')\n",
    "    df = pandas.DataFrame.from_dict(log, orient='index')\n",
    "    logs[path[:-4]] = log\n",
    "    dfs[path[:-4]] = df\n",
    "    print log.status['best_valid_mean_total_reward'], log.status['best_valid_per']\n",
    "    \n",
    "def compare_rewards_and_errors(models, s=slice(None)):\n",
    "    pyplot.figure(figsize=(15, 10))\n",
    "    legend = []\n",
    "    for m in models:\n",
    "        dfs[m].mean_total_reward.astype('float32').dropna().loc[s].plot()\n",
    "        legend += ['train_' + m]        \n",
    "        if 'valid_mean_total_reward' in dfs[m]:\n",
    "            dfs[m].valid_mean_total_reward.astype('float32').dropna().loc[s].plot(ls='--')\n",
    "            legend += ['valid_' + m]\n",
    "    pyplot.ylim(ymin=0)\n",
    "    pyplot.legend(legend, loc='best')\n",
    "    pyplot.xlabel(\"Iterations\")\n",
    "    pyplot.ylabel(\"Reward\")\n",
    "    pyplot.title('Reward')\n",
    "    pyplot.show()\n",
    "    \n",
    "    pyplot.figure(figsize=(10, 5))\n",
    "    for m in models:\n",
    "        dfs[m].readout_costs_mean_critic_cost.astype('float32').dropna().loc[s].plot()\n",
    "    pyplot.legend(models, loc='best')\n",
    "    pyplot.xlabel(\"Iterations\")\n",
    "    pyplot.ylabel(\"Reward\")\n",
    "    pyplot.title('Critic error')\n",
    "    pyplot.show()\n",
    "    \n",
    "    pyplot.figure(figsize=(10, 5))\n",
    "    for m in models:\n",
    "        dfs[m].readout_costs_mean_actor_cost.astype('float32').dropna().loc[s].plot()\n",
    "    pyplot.legend(models, loc='best')\n",
    "    pyplot.xlabel(\"Iterations\")\n",
    "    pyplot.ylabel(\"Reward\")\n",
    "    pyplot.title('Actor error')\n",
    "    pyplot.show()    \n",
    "    \n",
    "def compare_per(models, s=slice(None)):\n",
    "    pyplot.figure(figsize=(10, 5))\n",
    "    legend = []\n",
    "    for m in models:\n",
    "        dfs[m].valid_per.astype('float32').dropna().loc[s].plot()\n",
    "        legend += [m]\n",
    "    pyplot.legend(legend)      \n",
    "    \n",
    "def compare_gradient_norms(models):\n",
    "    legend = []\n",
    "    for m in models:\n",
    "        dfs[m].total_gradient_norm.astype('float32').dropna().plot(use_index=False)\n",
    "        legend += [m]\n",
    "    pyplot.legend(legend)\n",
    "    pyplot.title('Gradient norm')\n",
    "\n",
    "def compare_max_adjustments(models):\n",
    "    legend = []\n",
    "    for m in models:\n",
    "        dfs[m].readout_costs_max_adjustment.astype('float32').dropna().plot()\n",
    "        legend += [m]\n",
    "    pyplot.legend(legend)\n",
    "    pyplot.title('Max adjustment')\n",
    "    \n",
    "def compare_weight_entropy(models):\n",
    "    legend = []\n",
    "    for m in models:\n",
    "        dfs[m].average_weights_entropy_per_label.astype('float32').dropna().plot()\n",
    "        legend += [m]\n",
    "    pyplot.legend(legend)\n",
    "    pyplot.title('Weight entropy')\n",
    "    pyplot.show()\n",
    "    \n",
    "def compare_entropies(models, s=slice(None)):\n",
    "    legend = []\n",
    "    for m in models:\n",
    "        dfs[m].readout_costs_mean_actor_entropy.astype('float32').dropna().loc[s].plot()\n",
    "        legend += [m]\n",
    "    pyplot.legend(legend)\n",
    "    pyplot.title('Max adjustment')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_model('actor_critic4/critic_pretraining.tar')\n",
    "load_model('actor_critic4a/critic_pretraining.tar')\n",
    "load_model('actor_critic4b/critic_pretraining.tar')\n",
    "# load_model('actor_critic4c/critic_pretraining.tar')\n",
    "# load_model('actor_critic4d/critic_pretraining.tar')\n",
    "load_model('actor_critic4e/critic_pretraining.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_rewards_and_errors(\n",
    "    ['actor_critic4/critic_pretraining',\n",
    "     'actor_critic4a/critic_pretraining',\n",
    "     'actor_critic4b/critic_pretraining',\n",
    "     'actor_critic4e/critic_pretraining',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_model('actor_critic5/critic_pretraining.tar')\n",
    "load_model('actor_critic5a/critic_pretraining.tar')\n",
    "# load_model('actor_critic5b/critic_pretraining.tar')\n",
    "load_model('actor_critic5c/critic_pretraining.tar')\n",
    "load_model('actor_critic6/critic_pretraining.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_rewards_and_errors(\n",
    "    ['actor_critic5/critic_pretraining',\n",
    "     'actor_critic5a/critic_pretraining',\n",
    "     'actor_critic5c/critic_pretraining',\n",
    "     'actor_critic6/critic_pretraining',\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_model('actor_critic4/main.tar')\n",
    "load_model('actor_critic4a/main.tar')\n",
    "load_model('actor_critic4b/main.tar')\n",
    "load_model('actor_critic4c/main.tar')\n",
    "load_model('actor_critic4d/main.tar')\n",
    "load_model('actor_critic4e/main.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_rewards_and_errors(\n",
    "     [\n",
    "        'actor_critic4/main',\n",
    "        'actor_critic4a/main',\n",
    "        'actor_critic4b/main',\n",
    "        'actor_critic4c/main',\n",
    "        'actor_critic4d/main',\n",
    "        'actor_critic4e/main',\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_per(\n",
    "    ['actor_critic4/main',\n",
    "     'actor_critic4a/main',\n",
    "     'actor_critic4b/main',\n",
    "     'actor_critic4c/main',\n",
    "     'actor_critic4d/main',\n",
    "     'actor_critic4e/main',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_model('actor_critic5/main.tar')\n",
    "load_model('actor_critic5a/main.tar')\n",
    "load_model('actor_critic5b/main.tar')\n",
    "load_model('actor_critic5c/main.tar')\n",
    "load_model('actor_critic6/main.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load_model('actor_critic7/main.tar')\n",
    "# load_model('actor_critic7a/main.tar')\n",
    "load_model('actor_critic7b/main.tar')\n",
    "load_model('actor_critic7c/main.tar')\n",
    "load_model('actor_critic7d/main.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "compare_rewards_and_errors(\n",
    "     [\n",
    "        'actor_critic5/main',\n",
    "        'actor_critic5a/main',\n",
    "        'actor_critic5b/main',\n",
    "        'actor_critic5c/main',\n",
    "        'actor_critic6/main',\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_rewards_and_errors(\n",
    "    ['actor_critic7/main', \n",
    "     'actor_critic7a/main', \n",
    "     'actor_critic7b/main',\n",
    "     'actor_critic7c/main',\n",
    "     'actor_critic7d/main'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_per(\n",
    "     [\n",
    "        #'actor_critic5/main',\n",
    "        #'actor_critic5a/main',\n",
    "        #'actor_critic5c/main',\n",
    "        'actor_critic7/main',\n",
    "        'actor_critic7a/main',\n",
    "        'actor_critic7b/main',\n",
    "        'actor_critic7c/main',\n",
    "        'actor_critic7d/main',\n",
    "     ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "compare_entropies(\n",
    "    [\n",
    "        'actor_critic4/main',\n",
    "        'actor_critic4e/main',\n",
    "        'actor_critic5/main',\n",
    "        'actor_critic5a/main',\n",
    "        'actor_critic7/main',\n",
    "        'actor_critic7a/main',\n",
    "        'actor_critic7b/main',\n",
    "        'actor_critic7c/main',        \n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_critic_suggestions(it, i, just_from_groundtruth=False, n_best=5):\n",
    "    print \"Groundtruth: \", groundtruth[it][i]\n",
    "    print \"Prediction: \", predictions[it][i]\n",
    "    print \"Rewards: \", rewards[it][:, i]\n",
    "    groundtruth_nums = set([char2code[word] for word in groundtruth[it][i]])\n",
    "    predictions_words = predictions[it][i]\n",
    "    for step in range(len(predictions_words)):\n",
    "        actions = enumerate(values[it][step, i])\n",
    "        if just_from_groundtruth:\n",
    "            actions = [(n, q) for n, q in actions if n in groundtruth_nums]\n",
    "        best = list(sorted(actions, key=lambda (i, o): -o))\n",
    "        print [(code2char[c], o, probs[it][step, i , c]) for c, o in best[:n_best]]\n",
    "        if predictions_words[step] == '$':\n",
    "            break\n",
    "            \n",
    "def show_probs(it, i):\n",
    "    pyplot.matshow(probs[it][:, i])\n",
    "    pyplot.colorbar()\n",
    "    pyplot.show()            \n",
    "            \n",
    "def show_values(it, i):\n",
    "    pyplot.matshow(values[it][:, i])\n",
    "    pyplot.colorbar()\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "load_model('actor_critic7/critic_pretraining.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log = logs['actor_critic7/critic_pretraining']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print log.status['iterations_done']\n",
    "print log.status['_config']\n",
    "train_cost = [log[t].get('train_cost') for t in range(0, log.status['iterations_done'])]\n",
    "rewards = [log[t].get('readout_costs_rewards') for t in range(0, log.status['iterations_done'])]\n",
    "mean_reward = [log[t].get('mean_total_reward') for t in range(0, log.status['iterations_done'])]\n",
    "critic_cost = [log[t].get('readout_costs_mean_critic_cost') for t in range(0, log.status['iterations_done'])]\n",
    "actor_cost = [log[t].get('readout_costs_mean_actor_cost') for t in range(0, log.status['iterations_done'])]\n",
    "\n",
    "inputs = [log[t].get('average_inputs') for t in range(0, log.status['iterations_done'])]\n",
    "predictions = [log[t].get('average_predictions') for t in range(0, log.status['iterations_done'])]\n",
    "prediction_masks = [log[t].get('readout_costs_prediction_mask') for t in range(0, log.status['iterations_done'])]\n",
    "groundtruth = [log[t].get('average_groundtruth') for t in range(0, log.status['iterations_done'])]\n",
    "\n",
    "value_biases = [log[t].get('readout_costs_value_biases') for t in range(0, log.status['iterations_done'])]\n",
    "values = [log[t].get('readout_costs_values') for t in range(0, log.status['iterations_done'])]\n",
    "probs = [log[t].get('readout_costs_probs') for t in range(0, log.status['iterations_done'])]\n",
    "outputs = [log[t].get('readout_costs_outputs') for t in range(0, log.status['iterations_done'])]\n",
    "\n",
    "prediction_values = [log[t].get('readout_costs_prediction_values') for t in range(0, log.status['iterations_done'])]\n",
    "prediction_outputs = [log[t].get('readout_costs_prediction_outputs') for t in range(0, log.status['iterations_done'])]\n",
    "\n",
    "value_targets = [log[t].get('readout_costs_value_targets') for t in range(0, log.status['iterations_done'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "it = 29800 # training iteration\n",
    "print mean_reward[it]\n",
    "#print rewards[t].sum(axis=0)\n",
    "print train_cost[it], critic_cost[it], actor_cost[it]\n",
    "print \"Groundtruth\"\n",
    "pprint(groundtruth[it])\n",
    "print \"Predictions\"\n",
    "pprint(predictions[it])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "i = 3\n",
    "print_critic_suggestions(it, i)\n",
    "show_probs(it, i)\n",
    "show_values(it, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print prediction_values[it][:, i][:-3]\n",
    "print [probs[it][t, i, char2code[c]] for t, c in enumerate(predictions[it][i])][:-3]\n",
    "print value_targets[it][:, i][:-3]\n",
    "print ((prediction_values[it][:, i] - value_targets[it][:, i]) ** 2)[:-3].sum()\n",
    "print (probs[it][:, i] * values[it][:, i]).sum(axis=-1)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
