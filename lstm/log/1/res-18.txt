CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/19 03:06:18

cntk  configFile=translate.cntk  makeMode=false  syncPeriod=18
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/19 03:06:18

cntk  configFile=translate.cntk  makeMode=false  syncPeriod=18
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/19 03:06:18

cntk  configFile=translate.cntk  makeMode=false  syncPeriod=18
--------------------------------------------------------------------------
[[61909,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: Multiversohk

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/19 03:06:18

cntk  configFile=translate.cntk  makeMode=false  syncPeriod=18
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (3) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (1) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (2) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (0) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# train command (train action)                                               #
#                                                                            #
##############################################################################

-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# train command (train action)                                               #
#                                                                            #
##############################################################################

WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# train command (train action)                                               #
#                                                                            #
##############################################################################

-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11436 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# train command (train action)                                               #
#                                                                            #
##############################################################################

WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
[Multiversohk:06310] 3 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[Multiversohk:06310] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Node 'decoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].

Model has 625 nodes. Using GPU 1.

Training criterion:   ce = Pass
Evaluation criterion: errs = Pass
Node 'encoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].

Training 138016746 parameters in 120 parameter tensors.

Node 'decoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].

Model has 625 nodes. Using GPU 2.

Training criterion:   ce = Pass
Evaluation criterion: errs = Pass

Training 138016746 parameters in 120 parameter tensors.


Model has 625 nodes. Using GPU 3.

Training criterion:   ce = Pass
Evaluation criterion: errs = Pass

Training 138016746 parameters in 120 parameter tensors.


Model has 625 nodes. Using GPU 0.

Training criterion:   ce = Pass
Evaluation criterion: errs = Pass

Training 138016746 parameters in 120 parameter tensors.

Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 2.13-seconds latency this time; accumulated time on sync point = 2.13 seconds , average latency = 2.13 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 334.18-seconds latency this time; accumulated time on sync point = 334.18 seconds , average latency = 334.18 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 978.40-seconds latency this time; accumulated time on sync point = 978.40 seconds , average latency = 978.40 seconds
Finished Epoch[ 1 of 10]: [Training] Finished Epoch[ 1 of 10]: [Training] ce = 7.37712057 * 274896; errs = 94.697% * 274896; totalSamplesSeen = 274896; learningRatePerSample = 0.0070000002; epochTime=1653s
Finished Epoch[ 1 of 10]: [Training] ce = 7.37712057 * 274896; errs = 94.697% * 274896; totalSamplesSeen = 274896; learningRatePerSample = 0.0070000002; epochTime=1653s
Finished Epoch[ 1 of 10]: [Training] ce = 7.37712057 * 274896; errs = 94.697% * 274896; totalSamplesSeen = 274896; learningRatePerSample = 0.0070000002; epochTime=1653s
ce = 7.37712057 * 274896; errs = 94.697% * 274896; totalSamplesSeen = 274896; learningRatePerSample = 0.0070000002; epochTime=1653s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.35-seconds latency this time; accumulated time on sync point = 1.35 seconds , average latency = 1.35 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 847.64-seconds latency this time; accumulated time on sync point = 847.64 seconds , average latency = 847.64 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 160.30-seconds latency this time; accumulated time on sync point = 160.30 seconds , average latency = 160.30 seconds
Finished Epoch[ 2 of 10]: [Training] ce = 6.53209968 * 275470; errs = 91.501% * 275470; totalSamplesSeen = 550366; learningRatePerSample = 0.0070000002; epochTime=1550.05s
Finished Epoch[ 2 of 10]: [Training] Finished Epoch[ 2 of 10]: [Training] ce = 6.53209968 * 275470; errs = 91.501% * 275470; totalSamplesSeen = 550366; learningRatePerSample = 0.0070000002; epochTime=1550.05s
Finished Epoch[ 2 of 10]: [Training] ce = 6.53209968 * 275470; errs = 91.501% * 275470; totalSamplesSeen = 550366; learningRatePerSample = 0.0070000002; epochTime=1550.05s
ce = 6.53209968 * 275470; errs = 91.501% * 275470; totalSamplesSeen = 550366; learningRatePerSample = 0.0070000002; epochTime=1550.05s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.50-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.50 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 707.76-seconds latency this time; accumulated time on sync point = 707.76 seconds , average latency = 707.76 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 32.78-seconds latency this time; accumulated time on sync point = 32.78 seconds , average latency = 32.78 seconds
Finished Epoch[ 3 of 10]: [Training] ce = 5.95758985 * 274750; errs = 85.987% * 274750; totalSamplesSeen = 825116; learningRatePerSample = 0.0070000002; epochTime=1466.4s
Finished Epoch[ 3 of 10]: [Training] ce = 5.95758985 * 274750; errs = 85.987% * 274750; totalSamplesSeen = 825116; learningRatePerSample = 0.0070000002; epochTime=1466.4s
Finished Epoch[ 3 of 10]: [Training] Finished Epoch[ 3 of 10]: [Training] ce = 5.95758985 * 274750; errs = 85.987% * 274750; totalSamplesSeen = 825116; learningRatePerSample = 0.0070000002; epochTime=1466.4s
ce = 5.95758985 * 274750; errs = 85.987% * 274750; totalSamplesSeen = 825116; learningRatePerSample = 0.0070000002; epochTime=1466.4s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.11-seconds latency this time; accumulated time on sync point = 1.11 seconds , average latency = 1.11 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 747.59-seconds latency this time; accumulated time on sync point = 747.59 seconds , average latency = 747.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 216.88-seconds latency this time; accumulated time on sync point = 216.88 seconds , average latency = 216.88 seconds
Finished Epoch[ 4 of 10]: [Training] Finished Epoch[ 4 of 10]: [Training] ce = 5.75065559 * 275086; errs = 84.501% * 275086; totalSamplesSeen = 1100202; learningRatePerSample = 0.0070000002; epochTime=1537.14s
Finished Epoch[ 4 of 10]: [Training] ce = 5.75065559 * 275086; errs = 84.501% * 275086; totalSamplesSeen = 1100202; learningRatePerSample = 0.0070000002; epochTime=1537.14s
Finished Epoch[ 4 of 10]: [Training] ce = 5.75065559 * 275086; errs = 84.501% * 275086; totalSamplesSeen = 1100202; learningRatePerSample = 0.0070000002; epochTime=1537.14s
ce = 5.75065559 * 275086; errs = 84.501% * 275086; totalSamplesSeen = 1100202; learningRatePerSample = 0.0070000002; epochTime=1537.14s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.99-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.99 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 192.62-seconds latency this time; accumulated time on sync point = 192.62 seconds , average latency = 192.62 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 652.08-seconds latency this time; accumulated time on sync point = 652.08 seconds , average latency = 652.08 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 5 of 10]: [Training] ce = 5.58821614 * 274842; errs = 83.061% * 274842; totalSamplesSeen = 1375044; learningRatePerSample = 0.0070000002; epochTime=1503.46s
Finished Epoch[ 5 of 10]: [Training] Finished Epoch[ 5 of 10]: [Training] ce = 5.58821614 * 274842; errs = 83.061% * 274842; totalSamplesSeen = 1375044; learningRatePerSample = 0.0070000002; epochTime=1503.46s
Finished Epoch[ 5 of 10]: [Training] ce = 5.58821614 * 274842; errs = 83.061% * 274842; totalSamplesSeen = 1375044; learningRatePerSample = 0.0070000002; epochTime=1503.46s
ce = 5.58821614 * 274842; errs = 83.061% * 274842; totalSamplesSeen = 1375044; learningRatePerSample = 0.0070000002; epochTime=1503.46s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.49-seconds latency this time; accumulated time on sync point = 0.49 seconds , average latency = 0.49 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 74.51-seconds latency this time; accumulated time on sync point = 74.51 seconds , average latency = 74.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 516.47-seconds latency this time; accumulated time on sync point = 516.47 seconds , average latency = 516.47 seconds
Finished Epoch[ 6 of 10]: [Training] Finished Epoch[ 6 of 10]: [Training] ce = 5.46744756 * 275012; errs = 81.850% * 275012; totalSamplesSeen = 1650056; learningRatePerSample = 0.0070000002; epochTime=1413.2s
Finished Epoch[ 6 of 10]: [Training] ce = 5.46744756 * 275012; errs = 81.850% * 275012; totalSamplesSeen = 1650056; learningRatePerSample = 0.0070000002; epochTime=1413.2s
Finished Epoch[ 6 of 10]: [Training] ce = 5.46744756 * 275012; errs = 81.850% * 275012; totalSamplesSeen = 1650056; learningRatePerSample = 0.0070000002; epochTime=1413.2s
ce = 5.46744756 * 275012; errs = 81.850% * 275012; totalSamplesSeen = 1650056; learningRatePerSample = 0.0070000002; epochTime=1413.2s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.88-seconds latency this time; accumulated time on sync point = 0.88 seconds , average latency = 0.88 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 12.14-seconds latency this time; accumulated time on sync point = 12.14 seconds , average latency = 12.14 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 419.27-seconds latency this time; accumulated time on sync point = 419.27 seconds , average latency = 419.27 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 7 of 10]: [Training] ce = 5.34373454 * 274889; errs = 79.732% * 274889; totalSamplesSeen = 1924945; learningRatePerSample = 0.0070000002; epochTime=1359.39s
Finished Epoch[ 7 of 10]: [Training] ce = 5.34373454 * 274889; errs = 79.732% * 274889; totalSamplesSeen = 1924945; learningRatePerSample = 0.0070000002; epochTime=1359.39s
Finished Epoch[ 7 of 10]: [Training] Finished Epoch[ 7 of 10]: [Training] ce = 5.34373454 * 274889; errs = 79.732% * 274889; totalSamplesSeen = 1924945; learningRatePerSample = 0.0070000002; epochTime=1359.39s
ce = 5.34373454 * 274889; errs = 79.732% * 274889; totalSamplesSeen = 1924945; learningRatePerSample = 0.0070000002; epochTime=1359.39s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.14-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 1.14 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 353.46-seconds latency this time; accumulated time on sync point = 353.46 seconds , average latency = 353.46 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 82.24-seconds latency this time; accumulated time on sync point = 82.24 seconds , average latency = 82.24 seconds
Finished Epoch[ 8 of 10]: [Training] ce = 5.24075825 * 275019; errs = 78.425% * 275019; totalSamplesSeen = 2199964; learningRatePerSample = 0.0070000002; epochTime=1365.31s
Finished Epoch[ 8 of 10]: [Training] ce = 5.24075825 * 275019; errs = 78.425% * 275019; totalSamplesSeen = 2199964; learningRatePerSample = 0.0070000002; epochTime=1365.31s
Finished Epoch[ 8 of 10]: [Training] Finished Epoch[ 8 of 10]: [Training] ce = 5.24075825 * 275019; errs = 78.425% * 275019; totalSamplesSeen = 2199964; learningRatePerSample = 0.0070000002; epochTime=1365.31s
ce = 5.24075825 * 275019; errs = 78.425% * 275019; totalSamplesSeen = 2199964; learningRatePerSample = 0.0070000002; epochTime=1365.31s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.99-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.99 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 76.91-seconds latency this time; accumulated time on sync point = 76.91 seconds , average latency = 76.91 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 472.79-seconds latency this time; accumulated time on sync point = 472.79 seconds , average latency = 472.79 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 9 of 10]: [Training] ce = 5.16169816 * 275093; errs = 77.811% * 275093; totalSamplesSeen = 2475057; learningRatePerSample = 0.0070000002; epochTime=1400.03s
Finished Epoch[ 9 of 10]: [Training] ce = 5.16169816 * 275093; errs = 77.811% * 275093; totalSamplesSeen = 2475057; learningRatePerSample = 0.0070000002; epochTime=1400.03s
Finished Epoch[ 9 of 10]: [Training] ce = 5.16169816 * 275093; errs = 77.811% * 275093; totalSamplesSeen = 2475057; learningRatePerSample = 0.0070000002; epochTime=1400.03s
Finished Epoch[ 9 of 10]: [Training] ce = 5.16169816 * 275093; errs = 77.811% * 275093; totalSamplesSeen = 2475057; learningRatePerSample = 0.0070000002; epochTime=1400.03s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.12-seconds latency this time; accumulated time on sync point = 1.12 seconds , average latency = 1.12 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 538.00-seconds latency this time; accumulated time on sync point = 538.00 seconds , average latency = 538.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 260.45-seconds latency this time; accumulated time on sync point = 260.45 seconds , average latency = 260.45 seconds
Finished Epoch[10 of 10]: [Training] ce = 5.08513638 * 275171; errs = 77.195% * 275171; totalSamplesSeen = 2750228; learningRatePerSample = 0.0070000002; epochTime=1483.38s
Finished Epoch[10 of 10]: [Training] Finished Epoch[10 of 10]: [Training] ce = 5.08513638 * 275171; errs = 77.195% * 275171; totalSamplesSeen = 2750228; learningRatePerSample = 0.0070000002; epochTime=1483.38s
Finished Epoch[10 of 10]: [Training] ce = 5.08513638 * 275171; errs = 77.195% * 275171; totalSamplesSeen = 2750228; learningRatePerSample = 0.0070000002; epochTime=1483.38s
ce = 5.08513638 * 275171; errs = 77.195% * 275171; totalSamplesSeen = 2750228; learningRatePerSample = 0.0070000002; epochTime=1483.38s


[CALL STACK]
[0x83772c]                                                            
[0x7f78597de0d5]    unlinkOrDie  (std::string const&)                  + 0x35
[0x7f78597de234]    renameOrDie  (std::string const&,  std::string const&) + 0x14
[0x7f78597de3b9]    renameOrDie  (std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>> const&,  std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>> const&) + 0x149
[0xc97116]          Microsoft::MSR::CNTK::ComputationNetwork::  Save  (std::basic_string<wchar_t,std::char_traits<wchar_t>,std::allocator<wchar_t>> const&,  Microsoft::MSR::CNTK::FileOptions) const + 0x66
[0xac5a37]          Microsoft::MSR::CNTK::SGD<float>::  TrainOrAdaptModel  (int,  std::shared_ptr<Microsoft::MSR::CNTK::ComputationNetwork>,  bool,  std::shared_ptr<Microsoft::MSR::CNTK::ComputationNetwork>,  std::shared_ptr<Microsoft::MSR::CNTK::ComputationNodeBase>,  Microsoft::MSR::CNTK::IDataReader*,  Microsoft::MSR::CNTK::IDataReader*) + 0x3757
[0xac7332]          Microsoft::MSR::CNTK::SGD<float>::  Train  (std::shared_ptr<Microsoft::MSR::CNTK::ComputationNetwork>,  int,  Microsoft::MSR::CNTK::IDataReader*,  Microsoft::MSR::CNTK::IDataReader*,  int,  bool) + 0x142
[0x98858d]          void  DoTrain  <Microsoft::MSR::CNTK::ConfigParameters,float>(Microsoft::MSR::CNTK::ConfigParameters const&) + 0x3bd
[0x9165c0]          void  DoCommands  <float>(Microsoft::MSR::CNTK::ConfigParameters const&,  std::shared_ptr<Microsoft::MSR::CNTK::MPIWrapper> const&) + 0x9e0
[0x8759d5]          wmainOldCNTKConfig  (int,  wchar_t**)              + 0x9a5
[0x8760ef]          wmain1  (int,  wchar_t**)                          + 0x7f
[0x823758]          main                                               + 0xd8
[0x7f78576a1830]    __libc_start_main                                  + 0xf0
[0x8349de]                                                            
EXCEPTION occurred: error deleting file 'Models': Is a directory
-------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code.. Per user-direction, the job has been aborted.
-------------------------------------------------------
--------------------------------------------------------------------------
mpiexec detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[61909,1],0]
  Exit code:    1
--------------------------------------------------------------------------
