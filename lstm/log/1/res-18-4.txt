CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/20 15:31:36

cntk  configFile=translate.cntk  makeMode=false  syncPeriod=18
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/20 15:31:36

cntk  configFile=translate.cntk  makeMode=false  syncPeriod=18
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/20 15:31:36

cntk  configFile=translate.cntk  makeMode=false  syncPeriod=18
CNTK 2.0 (HEAD ade8bc, May 31 2017 17:15:49) on Multiversohk at 2017/07/20 15:31:36

cntk  configFile=translate.cntk  makeMode=false  syncPeriod=18
--------------------------------------------------------------------------
[[46661,1],0]: A high-performance Open MPI point-to-point messaging module
was unable to find any relevant network interfaces:

Module: OpenFabrics (openib)
  Host: Multiversohk

Another transport will be used instead, although this may result in
lower performance.
--------------------------------------------------------------------------
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (before change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
ping [requestnodes (after change)]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (0) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (1) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (3) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
requestnodes [MPIWrapperMpi]: using 4 out of 4 MPI nodes on a single host (4 requested); we (2) are in (participating)
ping [mpihelper]: 4 nodes pinging each other
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
Build info: 

		Built time: May 31 2017 17:14:11
		Last modified date: Fri May 26 22:32:46 2017
		Build type: release
		Build target: GPU
		With 1bit-SGD: yes
		With ASGD: yes
		Math lib: mkl
		CUDA_PATH: /usr/local/cuda-8.0
		CUB_PATH: /usr/local/cub-1.4.1
		CUDNN_PATH: /usr/local/cudnn-5.1
		Build Branch: HEAD
		Build SHA1: ade8bc05d30e61160da729aee078e22f8bd4fced
		Built by Source/CNTK/buildinfo.h$$0 on 8df6191122a5
		Build Path: /home/philly/jenkins/workspace/CNTK-Build-Linux
		MPI distribution: Open MPI
		MPI version: 1.10.3
-------------------------------------------------------------------
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# train command (train action)                                               #
#                                                                            #
##############################################################################

-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# train command (train action)                                               #
#                                                                            #
##############################################################################

WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11437 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# train command (train action)                                               #
#                                                                            #
##############################################################################

-------------------------------------------------------------------
GPU info:

		Device[0]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 11436 MB
		Device[1]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[2]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
		Device[3]: cores = 2496; computeCapability = 3.7; type = "Tesla K80"; total memory = 11439 MB; free memory = 0 MB
-------------------------------------------------------------------

##############################################################################
#                                                                            #
# train command (train action)                                               #
#                                                                            #
##############################################################################

WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
WARNING: option syncPeroid in BlockMomentumSGD is going to be deprecated. Please use blockSizePerWorker instead in the future.
[Multiversohk:24374] 3 more processes have sent help message help-mpi-btl-base.txt / btl:no-nics
[Multiversohk:24374] Set MCA parameter "orte_base_help_aggregate" to 0 to see all help / error messages
Node 'decoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].

Model has 625 nodes. Using GPU 1.

Training criterion:   ce = Pass
Evaluation criterion: errs = Pass

Training 138016746 parameters in 120 parameter tensors.

Node 'decoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[0].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'encoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'encoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[0].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[0].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 200].
Node 'decoder.layers[1].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[1].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ot._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.ft._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.it._.PlusArgs[0].PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].
Node 'decoder.layers[2].lstmState._.bit.ElementTimesArgs[1].z.PlusArgs[0].PlusArgs[1].TimesArgs[0]' (LearnableParameter operation) operation: Tensor shape was inferred as [256 x 256].

Model has 625 nodes. Using GPU 0.

Training criterion:   ce = Pass
Evaluation criterion: errs = Pass

Training 138016746 parameters in 120 parameter tensors.


Model has 625 nodes. Using GPU 2.

Training criterion:   ce = Pass
Evaluation criterion: errs = Pass

Training 138016746 parameters in 120 parameter tensors.


Model has 625 nodes. Using GPU 3.

Training criterion:   ce = Pass
Evaluation criterion: errs = Pass

Training 138016746 parameters in 120 parameter tensors.

Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.90-seconds latency this time; accumulated time on sync point = 1.90 seconds , average latency = 1.90 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 331.23-seconds latency this time; accumulated time on sync point = 331.23 seconds , average latency = 331.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 969.60-seconds latency this time; accumulated time on sync point = 969.60 seconds , average latency = 969.60 seconds
Finished Epoch[ 1 of 50]: [Training] ce = 7.04782843 * 274896; errs = 93.415% * 274896; totalSamplesSeen = 274896; learningRatePerSample = 0.0049999999; epochTime=1641.48s
Finished Epoch[ 1 of 50]: [Training] ce = 7.04782843 * 274896; errs = 93.415% * 274896; totalSamplesSeen = 274896; learningRatePerSample = 0.0049999999; epochTime=1641.48s
Finished Epoch[ 1 of 50]: [Training] ce = 7.04782843 * 274896; errs = 93.415% * 274896; totalSamplesSeen = 274896; learningRatePerSample = 0.0049999999; epochTime=1641.48s
Finished Epoch[ 1 of 50]: [Training] ce = 7.04782843 * 274896; errs = 93.415% * 274896; totalSamplesSeen = 274896; learningRatePerSample = 0.0049999999; epochTime=1641.48s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.14-seconds latency this time; accumulated time on sync point = 1.14 seconds , average latency = 1.14 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 163.03-seconds latency this time; accumulated time on sync point = 163.03 seconds , average latency = 163.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 846.51-seconds latency this time; accumulated time on sync point = 846.51 seconds , average latency = 846.51 seconds
Finished Epoch[ 2 of 50]: [Training] ce = 6.19893682 * 275470; errs = 87.856% * 275470; totalSamplesSeen = 550366; learningRatePerSample = 0.0049999999; epochTime=1544.89s
Finished Epoch[ 2 of 50]: [Training] ce = 6.19893682 * 275470; errs = 87.856% * 275470; totalSamplesSeen = 550366; learningRatePerSample = 0.0049999999; epochTime=1544.89s
Finished Epoch[ 2 of 50]: [Training] ce = 6.19893682 * 275470; errs = 87.856% * 275470; totalSamplesSeen = 550366; learningRatePerSample = 0.0049999999; epochTime=1544.89s
Finished Epoch[ 2 of 50]: [Training] ce = 6.19893682 * 275470; errs = 87.856% * 275470; totalSamplesSeen = 550366; learningRatePerSample = 0.0049999999; epochTime=1544.89s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.51-seconds latency this time; accumulated time on sync point = 0.51 seconds , average latency = 0.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 33.75-seconds latency this time; accumulated time on sync point = 33.75 seconds , average latency = 33.75 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 701.22-seconds latency this time; accumulated time on sync point = 701.22 seconds , average latency = 701.22 seconds
Finished Epoch[ 3 of 50]: [Training] ce = 5.84416890 * 274750; errs = 85.326% * 274750; totalSamplesSeen = 825116; learningRatePerSample = 0.0049999999; epochTime=1457.61s
Finished Epoch[ 3 of 50]: [Training] ce = 5.84416890 * 274750; errs = 85.326% * 274750; totalSamplesSeen = 825116; learningRatePerSample = 0.0049999999; epochTime=1457.61s
Finished Epoch[ 3 of 50]: [Training] ce = 5.84416890 * 274750; errs = 85.326% * 274750; totalSamplesSeen = 825116; learningRatePerSample = 0.0049999999; epochTime=1457.61s
Finished Epoch[ 3 of 50]: [Training] ce = 5.84416890 * 274750; errs = 85.326% * 274750; totalSamplesSeen = 825116; learningRatePerSample = 0.0049999999; epochTime=1457.61s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.16-seconds latency this time; accumulated time on sync point = 1.16 seconds , average latency = 1.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 213.51-seconds latency this time; accumulated time on sync point = 213.51 seconds , average latency = 213.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 740.38-seconds latency this time; accumulated time on sync point = 740.38 seconds , average latency = 740.38 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 4 of 50]: [Training] ce = 5.71241641 * 275086; errs = 84.285% * 275086; totalSamplesSeen = 1100202; learningRatePerSample = 0.0049999999; epochTime=1526.51s
Finished Epoch[ 4 of 50]: [Training] ce = 5.71241641 * 275086; errs = 84.285% * 275086; totalSamplesSeen = 1100202; learningRatePerSample = 0.0049999999; epochTime=1526.51s
Finished Epoch[ 4 of 50]: [Training] Finished Epoch[ 4 of 50]: [Training] ce = 5.71241641 * 275086; errs = 84.285% * 275086; totalSamplesSeen = 1100202; learningRatePerSample = 0.0049999999; epochTime=1526.51s
ce = 5.71241641 * 275086; errs = 84.285% * 275086; totalSamplesSeen = 1100202; learningRatePerSample = 0.0049999999; epochTime=1526.51s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.82-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.82 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 192.69-seconds latency this time; accumulated time on sync point = 192.69 seconds , average latency = 192.69 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 647.28-seconds latency this time; accumulated time on sync point = 647.28 seconds , average latency = 647.28 seconds
Finished Epoch[ 5 of 50]: [Training] ce = 5.57614136 * 274842; errs = 83.010% * 274842; totalSamplesSeen = 1375044; learningRatePerSample = 0.0049999999; epochTime=1494.29s
Finished Epoch[ 5 of 50]: [Training] ce = 5.57614136 * 274842; errs = 83.010% * 274842; totalSamplesSeen = 1375044; learningRatePerSample = 0.0049999999; epochTime=1494.29s
Finished Epoch[ 5 of 50]: [Training] Finished Epoch[ 5 of 50]: [Training] ce = 5.57614136 * 274842; errs = 83.010% * 274842; totalSamplesSeen = 1375044; learningRatePerSample = 0.0049999999; epochTime=1494.29s
ce = 5.57614136 * 274842; errs = 83.010% * 274842; totalSamplesSeen = 1375044; learningRatePerSample = 0.0049999999; epochTime=1494.29s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.43-seconds latency this time; accumulated time on sync point = 0.43 seconds , average latency = 0.43 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 74.99-seconds latency this time; accumulated time on sync point = 74.99 seconds , average latency = 74.99 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 509.97-seconds latency this time; accumulated time on sync point = 509.97 seconds , average latency = 509.97 seconds
Finished Epoch[ 6 of 50]: [Training] Finished Epoch[ 6 of 50]: [Training] ce = 6.21760994 * 275012; errs = 89.783% * 275012; totalSamplesSeen = 1650056; learningRatePerSample = 0.0049999999; epochTime=1402.94s
Finished Epoch[ 6 of 50]: [Training] ce = 6.21760994 * 275012; errs = 89.783% * 275012; totalSamplesSeen = 1650056; learningRatePerSample = 0.0049999999; epochTime=1402.94s
Finished Epoch[ 6 of 50]: [Training] ce = 6.21760994 * 275012; errs = 89.783% * 275012; totalSamplesSeen = 1650056; learningRatePerSample = 0.0049999999; epochTime=1402.94s
ce = 6.21760994 * 275012; errs = 89.783% * 275012; totalSamplesSeen = 1650056; learningRatePerSample = 0.0049999999; epochTime=1402.94s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.47-seconds latency this time; accumulated time on sync point = 0.47 seconds , average latency = 0.47 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 414.23-seconds latency this time; accumulated time on sync point = 414.23 seconds , average latency = 414.23 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 17.59-seconds latency this time; accumulated time on sync point = 17.59 seconds , average latency = 17.59 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[ 7 of 50]: [Training] Finished Epoch[ 7 of 50]: [Training] ce = 6.58563411 * 274889; errs = 93.660% * 274889; totalSamplesSeen = 1924945; learningRatePerSample = 0.0049999999; epochTime=1351.52s
Finished Epoch[ 7 of 50]: [Training] ce = 6.58563411 * 274889; errs = 93.660% * 274889; totalSamplesSeen = 1924945; learningRatePerSample = 0.0049999999; epochTime=1351.52s
Finished Epoch[ 7 of 50]: [Training] ce = 6.58563411 * 274889; errs = 93.660% * 274889; totalSamplesSeen = 1924945; learningRatePerSample = 0.0049999999; epochTime=1351.52s
ce = 6.58563411 * 274889; errs = 93.660% * 274889; totalSamplesSeen = 1924945; learningRatePerSample = 0.0049999999; epochTime=1351.52s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.50-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.50 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 348.07-seconds latency this time; accumulated time on sync point = 348.07 seconds , average latency = 348.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 80.25-seconds latency this time; accumulated time on sync point = 80.25 seconds , average latency = 80.25 seconds
Finished Epoch[ 8 of 50]: [Training] Finished Epoch[ 8 of 50]: [Training] ce = 6.55692584 * 275019; errs = 93.642% * 275019; totalSamplesSeen = 2199964; learningRatePerSample = 0.0049999999; epochTime=1355.07s
Finished Epoch[ 8 of 50]: [Training] ce = 6.55692584 * 275019; errs = 93.642% * 275019; totalSamplesSeen = 2199964; learningRatePerSample = 0.0049999999; epochTime=1355.07s
Finished Epoch[ 8 of 50]: [Training] ce = 6.55692584 * 275019; errs = 93.642% * 275019; totalSamplesSeen = 2199964; learningRatePerSample = 0.0049999999; epochTime=1355.07s
ce = 6.55692584 * 275019; errs = 93.642% * 275019; totalSamplesSeen = 2199964; learningRatePerSample = 0.0049999999; epochTime=1355.07s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.83-seconds latency this time; accumulated time on sync point = 0.83 seconds , average latency = 0.83 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 75.03-seconds latency this time; accumulated time on sync point = 75.03 seconds , average latency = 75.03 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 467.44-seconds latency this time; accumulated time on sync point = 467.44 seconds , average latency = 467.44 seconds
Finished Epoch[ 9 of 50]: [Training] Finished Epoch[ 9 of 50]: [Training] ce = 6.55026698 * 275093; errs = 93.776% * 275093; totalSamplesSeen = 2475057; learningRatePerSample = 0.0049999999; epochTime=1389.55s
Finished Epoch[ 9 of 50]: [Training] ce = 6.55026698 * 275093; errs = 93.776% * 275093; totalSamplesSeen = 2475057; learningRatePerSample = 0.0049999999; epochTime=1389.55s
Finished Epoch[ 9 of 50]: [Training] ce = 6.55026698 * 275093; errs = 93.776% * 275093; totalSamplesSeen = 2475057; learningRatePerSample = 0.0049999999; epochTime=1389.55s
ce = 6.55026698 * 275093; errs = 93.776% * 275093; totalSamplesSeen = 2475057; learningRatePerSample = 0.0049999999; epochTime=1389.55s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.25-seconds latency this time; accumulated time on sync point = 1.25 seconds , average latency = 1.25 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 530.16-seconds latency this time; accumulated time on sync point = 530.16 seconds , average latency = 530.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 257.29-seconds latency this time; accumulated time on sync point = 257.29 seconds , average latency = 257.29 seconds
Finished Epoch[10 of 50]: [Training] ce = 6.53136765 * 275171; errs = 93.623% * 275171; totalSamplesSeen = 2750228; learningRatePerSample = 0.0049999999; epochTime=1470.88s
Finished Epoch[10 of 50]: [Training] ce = 6.53136765 * 275171; errs = 93.623% * 275171; totalSamplesSeen = 2750228; learningRatePerSample = 0.0049999999; epochTime=1470.88s
Finished Epoch[10 of 50]: [Training] ce = 6.53136765 * 275171; errs = 93.623% * 275171; totalSamplesSeen = 2750228; learningRatePerSample = 0.0049999999; epochTime=1470.88s
Finished Epoch[10 of 50]: [Training] ce = 6.53136765 * 275171; errs = 93.623% * 275171; totalSamplesSeen = 2750228; learningRatePerSample = 0.0049999999; epochTime=1470.88s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.50-seconds latency this time; accumulated time on sync point = 0.50 seconds , average latency = 0.50 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 770.51-seconds latency this time; accumulated time on sync point = 770.51 seconds , average latency = 770.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 445.05-seconds latency this time; accumulated time on sync point = 445.05 seconds , average latency = 445.05 seconds
Finished Epoch[11 of 50]: [Training] ce = 6.49523827 * 274992; errs = 93.330% * 274992; totalSamplesSeen = 3025220; learningRatePerSample = 0.0020000001; epochTime=1619.02s
Finished Epoch[11 of 50]: [Training] ce = 6.49523827 * 274992; errs = 93.330% * 274992; totalSamplesSeen = 3025220; learningRatePerSample = 0.0020000001; epochTime=1619.02s
Finished Epoch[11 of 50]: [Training] ce = 6.49523827 * 274992; errs = 93.330% * 274992; totalSamplesSeen = 3025220; learningRatePerSample = 0.0020000001; epochTime=1619.02s
Finished Epoch[11 of 50]: [Training] ce = 6.49523827 * 274992; errs = 93.330% * 274992; totalSamplesSeen = 3025220; learningRatePerSample = 0.0020000001; epochTime=1619.02s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.84-seconds latency this time; accumulated time on sync point = 0.84 seconds , average latency = 0.84 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 309.11-seconds latency this time; accumulated time on sync point = 309.11 seconds , average latency = 309.11 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 629.57-seconds latency this time; accumulated time on sync point = 629.57 seconds , average latency = 629.57 seconds
Finished Epoch[12 of 50]: [Training] Finished Epoch[12 of 50]: [Training] ce = 6.49395319 * 274654; errs = 93.470% * 274654; totalSamplesSeen = 3299874; learningRatePerSample = 0.0020000001; epochTime=1524.81s
Finished Epoch[12 of 50]: [Training] ce = 6.49395319 * 274654; errs = 93.470% * 274654; totalSamplesSeen = 3299874; learningRatePerSample = 0.0020000001; epochTime=1524.81s
Finished Epoch[12 of 50]: [Training] ce = 6.49395319 * 274654; errs = 93.470% * 274654; totalSamplesSeen = 3299874; learningRatePerSample = 0.0020000001; epochTime=1524.81s
ce = 6.49395319 * 274654; errs = 93.470% * 274654; totalSamplesSeen = 3299874; learningRatePerSample = 0.0020000001; epochTime=1524.81s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.07-seconds latency this time; accumulated time on sync point = 1.07 seconds , average latency = 1.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 139.01-seconds latency this time; accumulated time on sync point = 139.01 seconds , average latency = 139.01 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 527.84-seconds latency this time; accumulated time on sync point = 527.84 seconds , average latency = 527.84 seconds
Finished Epoch[13 of 50]: [Training] ce = 6.47623001 * 274820; errs = 93.316% * 274820; totalSamplesSeen = 3574694; learningRatePerSample = 0.0020000001; epochTime=1437.39s
Finished Epoch[13 of 50]: [Training] ce = 6.47623001 * 274820; errs = 93.316% * 274820; totalSamplesSeen = 3574694; learningRatePerSample = 0.0020000001; epochTime=1437.39s
Finished Epoch[13 of 50]: [Training] Finished Epoch[13 of 50]: [Training] ce = 6.47623001 * 274820; errs = 93.316% * 274820; totalSamplesSeen = 3574694; learningRatePerSample = 0.0020000001; epochTime=1437.39s
ce = 6.47623001 * 274820; errs = 93.316% * 274820; totalSamplesSeen = 3574694; learningRatePerSample = 0.0020000001; epochTime=1437.39s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.17-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 1.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 25.83-seconds latency this time; accumulated time on sync point = 25.83 seconds , average latency = 25.83 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 410.16-seconds latency this time; accumulated time on sync point = 410.16 seconds , average latency = 410.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[14 of 50]: [Training] ce = 6.48547422 * 274912; errs = 93.368% * 274912; totalSamplesSeen = 3849606; learningRatePerSample = 0.0020000001; epochTime=1355.89s
Finished Epoch[14 of 50]: [Training] ce = 6.48547422 * 274912; errs = 93.368% * 274912; totalSamplesSeen = 3849606; learningRatePerSample = 0.0020000001; epochTime=1355.89s
Finished Epoch[14 of 50]: [Training] Finished Epoch[14 of 50]: [Training] ce = 6.48547422 * 274912; errs = 93.368% * 274912; totalSamplesSeen = 3849606; learningRatePerSample = 0.0020000001; epochTime=1355.89s
ce = 6.48547422 * 274912; errs = 93.368% * 274912; totalSamplesSeen = 3849606; learningRatePerSample = 0.0020000001; epochTime=1355.89s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.85-seconds latency this time; accumulated time on sync point = 0.85 seconds , average latency = 0.85 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 128.76-seconds latency this time; accumulated time on sync point = 128.76 seconds , average latency = 128.76 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 593.42-seconds latency this time; accumulated time on sync point = 593.42 seconds , average latency = 593.42 seconds
Finished Epoch[15 of 50]: [Training] ce = 6.48199639 * 275167; errs = 93.373% * 275167; totalSamplesSeen = 4124773; learningRatePerSample = 0.0020000001; epochTime=1450.78s
Finished Epoch[15 of 50]: [Training] Finished Epoch[15 of 50]: [Training] ce = 6.48199639 * 275167; errs = 93.373% * 275167; totalSamplesSeen = 4124773; learningRatePerSample = 0.0020000001; epochTime=1450.78s
Finished Epoch[15 of 50]: [Training] ce = 6.48199639 * 275167; errs = 93.373% * 275167; totalSamplesSeen = 4124773; learningRatePerSample = 0.0020000001; epochTime=1450.78s
ce = 6.48199639 * 275167; errs = 93.373% * 275167; totalSamplesSeen = 4124773; learningRatePerSample = 0.0020000001; epochTime=1450.78s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.95-seconds latency this time; accumulated time on sync point = 0.95 seconds , average latency = 0.95 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 761.82-seconds latency this time; accumulated time on sync point = 761.82 seconds , average latency = 761.82 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 237.16-seconds latency this time; accumulated time on sync point = 237.16 seconds , average latency = 237.16 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[16 of 50]: [Training] ce = 6.47985155 * 275229; errs = 93.439% * 275229; totalSamplesSeen = 4400002; learningRatePerSample = 0.0020000001; epochTime=1542.7s
Finished Epoch[16 of 50]: [Training] ce = 6.47985155 * 275229; errs = 93.439% * 275229; totalSamplesSeen = 4400002; learningRatePerSample = 0.0020000001; epochTime=1542.7s
Finished Epoch[16 of 50]: [Training] Finished Epoch[16 of 50]: [Training] ce = 6.47985155 * 275229; errs = 93.439% * 275229; totalSamplesSeen = 4400002; learningRatePerSample = 0.0020000001; epochTime=1542.7s
ce = 6.47985155 * 275229; errs = 93.439% * 275229; totalSamplesSeen = 4400002; learningRatePerSample = 0.0020000001; epochTime=1542.7s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.26-seconds latency this time; accumulated time on sync point = 1.26 seconds , average latency = 1.26 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 786.51-seconds latency this time; accumulated time on sync point = 786.51 seconds , average latency = 786.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 158.36-seconds latency this time; accumulated time on sync point = 158.36 seconds , average latency = 158.36 seconds
Finished Epoch[17 of 50]: [Training] ce = 6.47313460 * 275246; errs = 93.418% * 275246; totalSamplesSeen = 4675248; learningRatePerSample = 0.0020000001; epochTime=1527.7s
Finished Epoch[17 of 50]: [Training] ce = 6.47313460 * 275246; errs = 93.418% * 275246; totalSamplesSeen = 4675248; learningRatePerSample = 0.0020000001; epochTime=1527.7s
Finished Epoch[17 of 50]: [Training] ce = 6.47313460 * 275246; errs = 93.418% * 275246; totalSamplesSeen = 4675248; learningRatePerSample = 0.0020000001; epochTime=1527.7s
Finished Epoch[17 of 50]: [Training] ce = 6.47313460 * 275246; errs = 93.418% * 275246; totalSamplesSeen = 4675248; learningRatePerSample = 0.0020000001; epochTime=1527.7s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.99-seconds latency this time; accumulated time on sync point = 0.99 seconds , average latency = 0.99 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 702.99-seconds latency this time; accumulated time on sync point = 702.99 seconds , average latency = 702.99 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 116.64-seconds latency this time; accumulated time on sync point = 116.64 seconds , average latency = 116.64 seconds
Finished Epoch[18 of 50]: [Training] ce = 6.47605213 * 274800; errs = 93.364% * 274800; totalSamplesSeen = 4950048; learningRatePerSample = 0.0020000001; epochTime=1487.52s
Finished Epoch[18 of 50]: [Training] ce = 6.47605213 * 274800; errs = 93.364% * 274800; totalSamplesSeen = 4950048; learningRatePerSample = 0.0020000001; epochTime=1487.52s
Finished Epoch[18 of 50]: [Training] ce = 6.47605213 * 274800; errs = 93.364% * 274800; totalSamplesSeen = 4950048; learningRatePerSample = 0.0020000001; epochTime=1487.52s
Finished Epoch[18 of 50]: [Training] ce = 6.47605213 * 274800; errs = 93.364% * 274800; totalSamplesSeen = 4950048; learningRatePerSample = 0.0020000001; epochTime=1487.52s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.86-seconds latency this time; accumulated time on sync point = 0.86 seconds , average latency = 0.86 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 647.91-seconds latency this time; accumulated time on sync point = 647.91 seconds , average latency = 647.91 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 68.51-seconds latency this time; accumulated time on sync point = 68.51 seconds , average latency = 68.51 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[19 of 50]: [Training] ce = 6.46834685 * 275175; errs = 93.337% * 275175; totalSamplesSeen = 5225223; learningRatePerSample = 0.0020000001; epochTime=1447.8s
Finished Epoch[19 of 50]: [Training] Finished Epoch[19 of 50]: [Training] ce = 6.46834685 * 275175; errs = 93.337% * 275175; totalSamplesSeen = 5225223; learningRatePerSample = 0.0020000001; epochTime=1447.8s
Finished Epoch[19 of 50]: [Training] ce = 6.46834685 * 275175; errs = 93.337% * 275175; totalSamplesSeen = 5225223; learningRatePerSample = 0.0020000001; epochTime=1447.8s
ce = 6.46834685 * 275175; errs = 93.337% * 275175; totalSamplesSeen = 5225223; learningRatePerSample = 0.0020000001; epochTime=1447.8s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.96-seconds latency this time; accumulated time on sync point = 0.96 seconds , average latency = 0.96 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 708.04-seconds latency this time; accumulated time on sync point = 708.04 seconds , average latency = 708.04 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 248.08-seconds latency this time; accumulated time on sync point = 248.08 seconds , average latency = 248.08 seconds
Finished Epoch[20 of 50]: [Training] ce = 6.47233565 * 275116; errs = 93.473% * 275116; totalSamplesSeen = 5500339; learningRatePerSample = 0.0020000001; epochTime=1533.04s
Finished Epoch[20 of 50]: [Training] ce = 6.47233565 * 275116; errs = 93.473% * 275116; totalSamplesSeen = 5500339; learningRatePerSample = 0.0020000001; epochTime=1533.04s
Finished Epoch[20 of 50]: [Training] ce = 6.47233565 * 275116; errs = 93.473% * 275116; totalSamplesSeen = 5500339; learningRatePerSample = 0.0020000001; epochTime=1533.04s
Finished Epoch[20 of 50]: [Training] ce = 6.47233565 * 275116; errs = 93.473% * 275116; totalSamplesSeen = 5500339; learningRatePerSample = 0.0020000001; epochTime=1533.04s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.17-seconds latency this time; accumulated time on sync point = 1.17 seconds , average latency = 1.17 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 369.22-seconds latency this time; accumulated time on sync point = 369.22 seconds , average latency = 369.22 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 710.37-seconds latency this time; accumulated time on sync point = 710.37 seconds , average latency = 710.37 seconds
Finished Epoch[21 of 50]: [Training] ce = 6.45397771 * 275078; errs = 93.456% * 275078; totalSamplesSeen = 5775417; learningRatePerSample = 0.00079999998; epochTime=1570.19s
Finished Epoch[21 of 50]: [Training] ce = 6.45397771 * 275078; errs = 93.456% * 275078; totalSamplesSeen = 5775417; learningRatePerSample = 0.00079999998; epochTime=1570.19s
Finished Epoch[21 of 50]: [Training] ce = 6.45397771 * 275078; errs = 93.456% * 275078; totalSamplesSeen = 5775417; learningRatePerSample = 0.00079999998; epochTime=1570.19s
Finished Epoch[21 of 50]: [Training] ce = 6.45397771 * 275078; errs = 93.456% * 275078; totalSamplesSeen = 5775417; learningRatePerSample = 0.00079999998; epochTime=1570.19s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.82-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.82 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 212.07-seconds latency this time; accumulated time on sync point = 212.07 seconds , average latency = 212.07 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 603.02-seconds latency this time; accumulated time on sync point = 603.02 seconds , average latency = 603.02 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
Finished Epoch[22 of 50]: [Training] ce = 6.44955089 * 274990; errs = 93.332% * 274990; totalSamplesSeen = 6050407; learningRatePerSample = 0.00079999998; epochTime=1481.57s
Finished Epoch[22 of 50]: [Training] Finished Epoch[22 of 50]: [Training] ce = 6.44955089 * 274990; errs = 93.332% * 274990; totalSamplesSeen = 6050407; learningRatePerSample = 0.00079999998; epochTime=1481.57s
Finished Epoch[22 of 50]: [Training] ce = 6.44955089 * 274990; errs = 93.332% * 274990; totalSamplesSeen = 6050407; learningRatePerSample = 0.00079999998; epochTime=1481.57s
ce = 6.44955089 * 274990; errs = 93.332% * 274990; totalSamplesSeen = 6050407; learningRatePerSample = 0.00079999998; epochTime=1481.57s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 0.82-seconds latency this time; accumulated time on sync point = 0.82 seconds , average latency = 0.82 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 0.00-seconds latency this time; accumulated time on sync point = 0.00 seconds , average latency = 0.00 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 26.58-seconds latency this time; accumulated time on sync point = 26.58 seconds , average latency = 26.58 seconds
		(model aggregation stats): 1-th sync point was hit, introducing a 424.49-seconds latency this time; accumulated time on sync point = 424.49 seconds , average latency = 424.49 seconds
Finished Epoch[23 of 50]: [Training] ce = 6.45312500 * 274734; errs = 93.349% * 274734; totalSamplesSeen = 6325141; learningRatePerSample = 0.00079999998; epochTime=1361.72s
Finished Epoch[23 of 50]: [Training] ce = 6.45312500 * 274734; errs = 93.349% * 274734; totalSamplesSeen = 6325141; learningRatePerSample = 0.00079999998; epochTime=1361.72s
Finished Epoch[23 of 50]: [Training] ce = 6.45312500 * 274734; errs = 93.349% * 274734; totalSamplesSeen = 6325141; learningRatePerSample = 0.00079999998; epochTime=1361.72s
Finished Epoch[23 of 50]: [Training] ce = 6.45312500 * 274734; errs = 93.349% * 274734; totalSamplesSeen = 6325141; learningRatePerSample = 0.00079999998; epochTime=1361.72s
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
Parallel training (4 workers) using BlockMomentumSGD with block momentum = 0.7500, block momentum time constant (per worker) = 62.5691, block learning rate = 1.0000, block size per worker = 18 samples, resetting SGD momentum after sync.
		(model aggregation stats): 1-th sync point was hit, introducing a 1.09-seconds latency this time; accumulated time on sync point = 1.09 seconds , average latency = 1.09 seconds
